{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import simplejson as json\n",
    "\n",
    "# get GOOG data\n",
    "with open('small_data/goog.json') as raw_f:\n",
    "    raw_data = raw_f.read()\n",
    "    json_data = json.loads(raw_data)\n",
    "\n",
    "goog = pd.DataFrame(json_data['data'], columns=json_data['column_names'])\n",
    "goog['Day'] = goog.index.values\n",
    "goog.set_index(pd.DatetimeIndex(goog['Date']), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization Theory\n",
    "<!-- requirement: small_data/goog.json -->\n",
    "<!-- requirement: images/visual_variables.jpg -->\n",
    "<!-- requirement: images/visual_precision.jpg -->\n",
    "<!-- requirement: images/color_blindness_illustration.jpg -->\n",
    "<!-- requirement: images/size_illusion.png -->\n",
    "<!-- requirement: images/mach_bands.png -->\n",
    "<!-- requirement: images/adelson_checkerboard.jpg -->\n",
    "<!-- requirement: images/us_evapo_map.jpg -->\n",
    "<!-- requirement: images/colormap_comparison.png -->\n",
    "<!-- requirement: images/colormap_nonlinearity.png -->\n",
    "<!-- requirement: images/colormap_linear.webp -->\n",
    "<!-- requirement: images/preattentive_1.jpg -->\n",
    "<!-- requirement: images/preattentive_2.jpg -->\n",
    "<!-- requirement: images/preattentive_3.jpg -->\n",
    "<!-- requirement: images/preattentive_4.jpg -->\n",
    "<!-- requirement: images/attentive_5.png -->\n",
    "<!-- requirement: images/preattentive_5.png-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different types of data for visualization purposes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization purposes, there are four types of data: **Nominal**, **Ordered**, **Interval**, and **Ratio**.  We give each one below and the associated operations which can be applied to each data type: **Equality**, **Comparison**, **Difference**, **Ratio**:\n",
    "  \n",
    "<table>\n",
    "\n",
    "<tr>\n",
    "<th>Data Type</th>\n",
    "<th>Example</th>\n",
    "<th>Equality</th>\n",
    "<th>Comparison</th>\n",
    "<th>Difference</th>\n",
    "<th>Ratio</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>**Nominal**</td>\n",
    "<td>e.g. countries of the world</td>\n",
    "<td>`=`,`!=`</td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>**Ordered**</td>\n",
    "<td>e.g. bond ratings: A, AA</td>\n",
    "<td>`=`,`!=`</td>\n",
    "<td>`>`, `<=`</td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>**Interval** (location of 0 is arbitrary)</td>\n",
    "<td>e.g. dates and time, lat-long</td>\n",
    "<td>`=`,`!=`</td>\n",
    "<td>`>`, `<=`</td>\n",
    "<td>`-`</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>**Ratio**</td>\n",
    "<td>e.g. physical measures or values</td>\n",
    "<td>`=`,`!=`</td>\n",
    "<td>`>`, `<=`</td>\n",
    "<td>`-`</td>\n",
    "<td>`/`</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seven categories of visual cues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are seven well-recognized ways of visually encoding data, **Position**, **Size**, **Value** (Saturation or density of a color), **Texture**, **Color** (Hue), **Orientation**, **Shape**.\n",
    "\n",
    "| Label      | Nominal  | Ordinal | Quantitative (Interval or Ratio) |\n",
    "| ---------- |:-:|:-:|:-:|\n",
    "|Position    | N | O | Q |\n",
    "|Size        |   | O | Q |\n",
    "|Value       | N | O | Q?|\n",
    "|Texture     | N | O |   |\n",
    "|Color       | N | O |   |\n",
    "|Orientation | N |   |   |\n",
    "|Shape       | N |   |&nbsp;|\n",
    "\n",
    "\n",
    "![Categories of Visual Cues](images/visual_variables.jpg)\n",
    "\n",
    "As indicated, certain cues are not well-suited to representing certain types of variables. Similarly our eye has greater precision in discriminating between certain visual cues.\n",
    "\n",
    "![Illustration of visual precision](images/visual_precision.jpg)\n",
    "\n",
    "Below, we plot some of Google's stock prices using various visual cues.  Which are easiest for you to read?  Which give you the most precision in determining the numeric values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = goog[-50:]\n",
    "d['Ones'] = np.ones(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [10, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = d[(500 < d['Close']) & (d['Close'] <= 520)].plot(kind='scatter', x='Day', y='Ones', marker='o')\n",
    "d[(520 < d['Close']) & (d['Close'] <= 540)].plot(kind='scatter', x='Day', y='Ones', marker='s', ax=ax)\n",
    "d[(540 < d['Close']) & (d['Close'] <= 560)].plot(kind='scatter', x='Day', y='Ones', marker='d', ax=ax)\n",
    "d[(560 < d['Close']) & (d['Close'] <= 580)].plot(kind='scatter', x='Day', y='Ones', marker='*', ax=ax)\n",
    "\n",
    "x = np.linspace(370,380,4)\n",
    "s = [10,30,50,70]\n",
    "shapes = 'osd*'\n",
    "for xx, ss, sh in zip(x, s, shapes):\n",
    "    plt.scatter(xx, [1.01], marker=sh, c=sns.color_palette()[0])\n",
    "    plt.text(xx, 1.008, 500 + ss, horizontalalignment='center',\n",
    "             verticalalignment='top')\n",
    "plt.yticks([])\n",
    "plt.ylabel('')\n",
    "plt.text(375, 1.005, 'Close', horizontalalignment='center', verticalalignment='top')\n",
    "plt.gcf().set_figheight(2)\n",
    "plt.title('Shape');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle(close):\n",
    "    return (close - 500) / 80 * 360\n",
    "\n",
    "for row in d.iterrows():\n",
    "    plt.scatter(row[1].Day, [0], marker=(2, 0, -angle(row[1].Close)/3.), s=200, c=sns.color_palette()[0])\n",
    "\n",
    "x = np.linspace(370,380,4)\n",
    "s = [510,530,550,570]\n",
    "for xx, ss, sh in zip(x, s, shapes):\n",
    "    plt.scatter(xx, [0.01], marker=(2, 0, -angle(1.0*ss)/3.), s=200, c=sns.color_palette()[0])\n",
    "    plt.text(xx, 0.008, ss, horizontalalignment='center',\n",
    "             verticalalignment='top')\n",
    "plt.yticks([])\n",
    "plt.ylabel('')\n",
    "plt.text(375, 0.005, 'Close', horizontalalignment='center', verticalalignment='top')\n",
    "plt.gcf().set_figheight(2)\n",
    "plt.title('Orientation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.plot(kind='scatter', x='Day', y='Ones', c='Close', cmap=plt.cm.rainbow,\n",
    "       ax=plt.gca())  # Hack for vanishing x-axis\n",
    "plt.yticks([])\n",
    "plt.ylabel('')\n",
    "plt.gcf().set_figheight(2)\n",
    "plt.title('Color');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.plot(kind='scatter', x='Day', y='Ones', c='Close', cmap=plt.cm.Blues,\n",
    "       ax=plt.gca())  # Hack for vanishing x-axis\n",
    "plt.yticks([])\n",
    "plt.ylabel('')\n",
    "plt.gcf().set_figheight(2)\n",
    "plt.title('Value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.plot(kind='scatter', x='Day', y='Ones', s=d['Close']-500)\n",
    "x = np.linspace(370,380,4)\n",
    "s = [10,30,50,70]\n",
    "plt.scatter(x, [1.01]*4, s=s)\n",
    "for xx, ss in zip(x, s):\n",
    "    plt.text(xx, 1.008, 500 + ss, horizontalalignment='center',\n",
    "             verticalalignment='top')\n",
    "plt.yticks([])\n",
    "plt.ylabel('')\n",
    "plt.text(375, 1.005, 'Close', horizontalalignment='center', verticalalignment='top')\n",
    "plt.gcf().set_figheight(2)\n",
    "plt.title('Size');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.plot(kind='scatter', x='Day', y='Close')\n",
    "plt.title('Position');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic algorithm for creating a visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the rough algorithm for how to draw a visualization.\n",
    "\n",
    "1. Express your message in terms of a few quantitative relationships to be expressed (probably no more than 2 and definitely not more than 3).\n",
    "1. Rank those quantitative relationships.\n",
    "1. Use the \"accuracy of visual perception\" and the table above to think about how to encode the data.\n",
    "\n",
    "In reality, you probably can't follow this algorithm literally but hopefully this can help you better understand what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portability & Accessibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colors are not produced the same way in all media. For instance, electronic devices use an RGB additive color system since on screens the colors are produced by mixing wavelengths of light. Meanwhile, printers use a CMYK subtractive color system since printed materials produce color by mixing pigments which absorb wavelengths of light. Even different electronic devices may render certain colors robustly and other only weakly. These factors can strongly limit or distort contrast between colors.\n",
    "\n",
    "Similarly, many men and some women have some form of color blindness, limiting their ability to distinguish between certain colors. When designing graphics to be accessible for color blind individuals, you may want mark categorical differences with differing texture or shape in addition to or instead of color.\n",
    "\n",
    "![Color blindness illustration](images/color_blindness_illustration.jpg)\n",
    "\n",
    "It is a good idea to use the intensity of a monochrome palette as a visual cue instead of hue/color where possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perception and Visual Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our eyes and brain don't respond linearly to changes in color, intensity, or size. Many of these factors interact in ways that can mislead us concerning the underlying data, perceiving larger or smaller contrasts than are accurate.\n",
    "\n",
    "![Relative size distortions](images/size_illusion.png)\n",
    "\n",
    "Relative sizes can lead us to believe that two identical objects differ in size.\n",
    "\n",
    "![Mach bands](images/mach_bands.png)\n",
    "\n",
    "Contrast in color or intensity is often dependent on what colors surround the objects. Our mind infers shadows or difference in lighting conditions when interpreting colors, [which can dramatically skew our perception](https://en.wikipedia.org/wiki/The_dress).\n",
    "\n",
    "![Adelson checkerboard](images/adelson_checkerboard.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptually uniform colormaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One pitfall many visualizations suffer from is from choosing colors that have high contrast even where there are no significant differences in the data.\n",
    "\n",
    "![US evapotranspiration map](images/us_evapo_map.jpg)\n",
    "\n",
    "In this example there is a perceived sharp divide in values between the eastern and western half of the United States. However, inspecting the legend reveals that there is very little difference along this apparent divide. Instead, the perceived difference is entirely due to the difference in brightness as the dark green colors shift towards bright yellow in the scale.\n",
    "\n",
    "![colormap comparison](images/colormap_comparison.png)\n",
    "\n",
    "Our eye is more sensitive to changes in brightness than changes in hue. Many commonly used colormaps (such as the previous default in `matplotlib`, \"jet\") are not \"perceptually uniform,\" meaning their perceived brightness shows nonlinearity or even reversals as the scale advances from low to high values.\n",
    "\n",
    "![colormap nonlinearity](images/colormap_nonlinearity.png)\n",
    "\n",
    "![linear colormap](images/colormap_linear.webp)\n",
    "\n",
    "Fortunately `matplotlib` has replaced their default colormap with the visually uniform `viridis` colormap. However, when representing categorical data, periodic data, or data where we want to highlight a strong divergence (for example differentiating values below and above an average), the `viridis` colormap may not be a good choice. [There are a number of excellent choices for these purposes available in `matplotlib`](https://matplotlib.org/tutorials/colors/colormaps.html#lightness-of-matplotlib-colormaps), and they provide plots of the perceived brightness for each colormap in the library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizing markers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using size as a cue, pay attention to whether you are scaling linear dimension or area.  Usually you want area, but if you are using an elongated shape you may want a linear dimension.  In `matplotlib` the size argument is in units of pixel$^2$.  Not all plotting libraries use the same convention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention and Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sense organs (such as our eyes) collect information in tremendous volume at tremendous speed. Much of this information either leads the eye in a [*pre-attentive process*](https://en.wikipedia.org/wiki/Pre-attentive_processing) or is discarded as irrelevant. The visual cues listed at the beginning of the notebook are targets for pre-attentive processing. We can think of these cues as organizing the viewer's conscious processing. Compare how quickly you can complete the following tasks.\n",
    "\n",
    "**Find the red circle.**\n",
    "\n",
    "![pre-attentive task 1](images/preattentive_1.jpg)\n",
    "![pre-attentive task 2](images/preattentive_2.jpg)\n",
    "\n",
    "**Find the filled circle.**\n",
    "\n",
    "![pre-attentive task 3](images/preattentive_3.jpg)\n",
    "\n",
    "**Find the boundary between circles and squares.**\n",
    "\n",
    "![pre-attentive task 4](images/preattentive_4.jpg)\n",
    "\n",
    "**Count the number of 5s in the image below.**\n",
    "\n",
    "![attentive task 5](images/attentive_5.png)\n",
    "\n",
    "![pre-attentive task 5](images/preattentive_5.png)\n",
    "\n",
    "Notice how pre-attentive processing dramatically improves the speed at which we can identify the desired information. Therefore we should use visual cues to highlight the most important data feature, and avoid encoding any other data features using visual cues that may interfere with the primary pre-attentive processing.\n",
    "\n",
    "Memory is another limited resource when processing information. In analogy with computers, we can think of our mind as having short-term, ready-access memory (like RAM) and long-term, slower-recall memory (disk storage). However, just like in a computer, our short-term/working memory [is limited in size (quotes range from 3-10 \"items\" or \"chunks\")](https://en.wikipedia.org/wiki/Short-term_memory). This means we can only compare a few groups and/or values at a time. The implication for visualization is that we should also structure our visual cues in a way that aggregates multiple data sources or implies a possible aggregation the user could make by eye.\n",
    "\n",
    "The following graph requires the viewer to frequently reference the legend because there are too many categories to remember which colors represent them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "x = np.arange(100)\n",
    "trials = np.empty((8, 100))\n",
    "for trial in range(8):\n",
    "    trend = np.random.rand() * (x / x.max() * np.random.rand() * np.random.choice([1, -1]) + 1)\n",
    "    noise = .05 * np.random.randn(x.size) * trend\n",
    "    trials[trial, :] = trend + noise\n",
    "    plt.plot(x, trend + noise, label='trial {}'.format(trial))\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, if we are representing a sensor reading from subsequent experiments, we may wish to aggregate these trials into a single signal with some uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = np.median(trials, axis=0)\n",
    "errors = np.vstack([np.percentile(trials, 25, axis=0), np.percentile(trials, 75, axis=0)])\n",
    "\n",
    "plt.plot(x, median, 'k', label='median among trials')\n",
    "plt.fill_between(x, errors[0, :], errors[1, :], alpha=.5, label='IQR')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual storytelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Narrative is about establishing a baseline, then disrupting it\n",
    "* The \"actionable\" is what is needed to restore balance\n",
    "* Why does the audience care?\n",
    "* Make everyone a stakeholder - \"the conflict between what is and what could be\" -Nancy Duarte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
