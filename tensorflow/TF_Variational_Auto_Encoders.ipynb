{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pylib.tensorboardcmd import tensorboard_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: increase this to at least 10 for a useful run, 20 or more produces better results\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- requirement: images/VAE.png -->\n",
    "<!-- requirement: images/VAEscheme.png -->\n",
    "<!-- requirement: pylib/__init__.py -->\n",
    "<!-- requirement: pylib/tensorboardcmd.py -->\n",
    "<!-- requirement: pylib/tf_utils.py -->\n",
    "<!-- requirement: pylib/mnist_dataset.py -->\n",
    "\n",
    "# Variational Autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders are neural networks where the number of input and output neurons are the same. If our input neurons represent pixels in an image, the output of our autoencoder will ideally be the input image. Why would we want to create a model that simply reproduces our data? If we restrict the number of neurons in our hidden layers to be less than the number of input or output neurons, we force our model to learn sparse representations of the data. Therefore, autoencoders can be used for image compression and removing noise from images. \n",
    "\n",
    "![VAE](images/VAE.png)\n",
    "\n",
    "An autoencoder consists of two neural networks -- an **encoder** and **decoder**. The encoder takes in high dimensional data and generates low dimensional representations of that data. Then, the decoder takes the low dimensional representations and translates them back into the high dimensional input space. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders (VAEs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Autoencoders (VAEs) differ from regular autoencoders, because they not only learn sparse representations of data but also generate new data. Consequently, VAEs are used to [create new images](https://openai.com/blog/generative-models/). For example, we can train a VAE on the MNIST data set and have it create an image of a handwritten \"5.\" \n",
    "\n",
    "How do VAEs generate new data? They do so by making smart assumptions about the distributions of these sparse data representations, or **latent vectors**. More generally, they belong to a class of models called generative models, which learn the joint probability distribution between the input ($x$) and output (or latent vectors, $z$). We can then use this information to come up with likely $(x,z)$ pairs. For example, once we learn the distribution corresponding to the sparse representation of a handwritten \"5,\" we can sample from this distribution to form new latent vectors for \"5.\" \n",
    "\n",
    "In this notebook, we will first build a simple autoencoder to recreate images in the MNIST data set and then will we build a VAE to generate new images of handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Autoencoder\n",
    "\n",
    "We will start off as usual by loading our data and setting our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylib.tf_utils import mnist_test, mnist_train\n",
    "train_images, train_label = mnist_train()\n",
    "test_images, test_label = mnist_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "img_size = 28\n",
    "img_size_flat = img_size * img_size\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, our encoder and decoder are neural networks (of two layers each) that are mirror images of each other. We feed the output of the encoder directly into the decoder to make the full autoencoder.\n",
    "\n",
    "We want to allow the flexibility to change the number of layers and their size without much work, so we'll build the layers up with a for loop through a list of sizes.\n",
    "\n",
    "Note that we are using the functional interface for this.  This allows us to build two distinct models, the encoder and decoder, then build our full model by using the former as the input to the latter.  This way, when we train the big model, we train both the decoder and encoder without extra work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll build a network that goes 784 -> 256 -> 128 -> 256 -> 784\n",
    "layer_sizes = [256, 128]\n",
    "\n",
    "original = keras.layers.Input(shape=(img_size_flat,))\n",
    "\n",
    "#Build the forward part, 784 -> 256 -> 128\n",
    "layer = original\n",
    "for size in layer_sizes:\n",
    "    layer = keras.layers.Dense(size, activation='sigmoid')(layer)\n",
    "#This is our encoder  \n",
    "encoder_out = layer\n",
    "\n",
    "encoder = keras.models.Model(original, encoder_out)\n",
    "#encoder.summary()\n",
    "\n",
    "#The decoder will be the reverse, 128 -> 256 -> 784\n",
    "#We'll need to reverse our layers, \n",
    "#drop the first one (it's input now), and add the final shape\n",
    "encoded_input = keras.layers.Input(shape=layer_sizes[-1:])\n",
    "layer = encoded_input\n",
    "for size in layer_sizes[-2::-1] + [img_size_flat]:\n",
    "    layer = keras.layers.Dense(size, activation='sigmoid')(layer)\n",
    "decoder_out = layer\n",
    "\n",
    "decoder = keras.models.Model(encoded_input, decoder_out)\n",
    "#decoder.summary()\n",
    "    \n",
    "autoencoder_out = decoder(encoder(original))\n",
    "autoencoder = keras.models.Model(original, autoencoder_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize the loss due to inaccurate pixel values, so the loss function that we will use that penalizes these errors is mean-square-error.\n",
    "\n",
    "Unlike last time, we will use the Adam Optimizer instead of the Gradient Descent Optimizer. This optimizer uses a decaying learning rate. \n",
    "\n",
    "As a reminder, we can interpret the learning rate as the size of the step we take down a gradient of our loss function. If the step size is too large, we may never get to the minimum. A large learning rate will manifest itself as noise in our loss curve that never converges to a minimum point. However, if we have a very small step size, our model may take a long time to run. Ideally, we want to take large steps at the start of the training process and small steps towards the end. The [Adam Optimizer](https://arxiv.org/pdf/1412.6980v8.pdf) changes the learning rate for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will train our model.  Again, we note that our input and output are the same - we're training a model that reproduces its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(train_images, train_images, epochs=num_epochs, \n",
    "                batch_size=batch_size, validation_data=(test_images,test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will regenerate 10 of the MNIST images. We will display the original test images above the regenerated ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 10\n",
    "\n",
    "# Applying encode and decode over test set\n",
    "encode_decode = autoencoder.predict(test_images[:n_examples])\n",
    "\n",
    "# Compare original images with their reconstructions\n",
    "f, a = plt.subplots(2, n_examples, figsize=(20, 4))\n",
    "for i in range(n_examples):\n",
    "    a[0][i].imshow(np.reshape(test_images[i], img_shape))\n",
    "    a[1][i].imshow(np.reshape(encode_decode[i], img_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Noise removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our system now has the capacity to reproduce the original images, and thus has some sort of internal model of what they should look like.  We can take advantage of this by feeding it images that _almost_ look like what it's seen before - it will return its best guess as to what they should be, given what it's seen.  Since we've trained it on clean images, we expect it will return clean images.  So, if we feed it a noisy image, it should do its best to return something like it's seen before, often very successfully removing the noise.\n",
    "\n",
    "Here we'll take an image, add a uniform random number to a percentage of the pixels, then run it through the encode-decode pathway.  We'll also run the clean image through the pathway to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def uniform_noise(img, percent):\n",
    "    start = test_images[img].copy()\n",
    "    noisy = start.copy()\n",
    "    for i in range(len(test_images[img])):\n",
    "        if random.random() < percent:\n",
    "            noisy[i] += random.random()\n",
    "            noisy[i] = min(noisy[i],1)\n",
    "    f, a = plt.subplots(1, 4, figsize=(20, 4))\n",
    "    encode_decode = autoencoder.predict(np.array([start, noisy]))\n",
    "    a[0].imshow(np.reshape(start, img_shape))\n",
    "    a[0].set_title(\"Input\")\n",
    "    a[0].grid(False)\n",
    "    a[1].imshow(np.reshape(encode_decode[0], img_shape))\n",
    "    a[1].set_title(\"Encode-decode on input\")\n",
    "    a[1].grid(False)\n",
    "    a[2].imshow(np.reshape(noisy, img_shape))\n",
    "    a[2].set_title(\"Noisy\")\n",
    "    a[2].grid(False)\n",
    "    a[3].imshow(np.reshape(encode_decode[1], img_shape))\n",
    "    a[3].set_title(\"Encode-decode on noisy\")\n",
    "    a[3].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've made a little function to do just that, just give it which test image (there are $10000$ of them) and what percentage of the pixels to add noise to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "interact(uniform_noise, img=(0,100), percent=(0,0.4,0.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate new images we need to construct a **Variational Autoencoder**. With a simple autoencoder that we just constructed, we can not create new images since we don't know how to create latent vectors other than obtain them by encoding training images. In other words, we want to build a model that can generate images and not just memorize those from the training data set.  \n",
    "\n",
    "As mentioned above, during training VAEs learn the **distribution** of latent variables so that we can then sample latent vectors from this distribution and decode them into new examples of handwritten digits. Therefore, we will now consider VAEs from a probability framework. \n",
    "\n",
    "Recall **Bayes' Theorem** that tells us:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "P(z \\mid x) &=& \\frac{P(x \\mid z) \\cdot P(z)}{P(x)} \\\\\n",
    "\\\\\n",
    "\\text{posterior distribution} &=& \\frac{\\text{likelihood} \\times \\text{prior}}{\\text{marginal likelihood}}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "\n",
    "Say we have data $x$ and latent variables $z$. The **encoder** tries to approximate the posterior distribution $P(z \\mid x)$, or generate latent variables conditioned on the data. On the other hand, the **decoder** takes $z$ [sampled from $P(z \\mid x)$] and outputs parameters to the likelihood distribution $P(x \\mid z)$. These parameters are the weights and biases of the neural networks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL-Divergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior distribution $P(z \\mid x)$, which is what we are essentially looking for is often intractable. A way around that is to approximate it with a multivariate Gaussian $\\mathcal{N}(\\mu, \\sigma)$ that approximates the true posterior distribution as best as possible. The amount of information lost when approximating $P(z \\mid x)$ is called the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullbackâ€“Leibler_divergence), and we will use it to construct our loss function. A choice commonly made to make our lives simple, is to have the true posterior distribution be a unit normal distribution and [calculate](http://allisons.org/ll/MML/KL/Normal/) the divergence accordingly:\n",
    "\n",
    "$$\n",
    "D_{\\rm{KL}}(\\mathcal{N}(0, 1) || \\mathcal{N}(\\mu, \\sigma)) = 0.5*(\\mu^2 + \\sigma^2 - log(\\sigma^2) - 1)\n",
    "$$\n",
    "\n",
    "This constraint on latent variables to have to approximate a certain distribution can also be thought of as a form of   regularization where we lose some fidelity to ensure we are capturing only important features. A nice explanation of choosing latent variables can be found [here](http://kvfrans.com/variational-autoencoders-explained/).\n",
    "\n",
    "We also want to minimize the loss due to inaccurate pixel values when decoding images from latent space and must therefore create a component of the loss function that penalizes these errors. This component is often called generation loss, and it is the mean squared difference between the true and regenerated pixel values in our images. Total loss is then the sum of the mean squared error and Kullback-Leibler divergence:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = MSE + D_{\\rm{KL}}(\\mathcal{N}(0, 1) || \\mathcal{N}(\\mu, \\sigma))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following scheme represents the architecture of a variational autoencoder. Same as before, we will implement it using the functional interface.\n",
    "\n",
    "![VAE scheme](images/VAEscheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of working with the standard deviation of the approximate posterior distribution, we will be computing the $log(\\sigma^2)$ because this is not only more convenient to work with, but also helps with numerical stability. A sample latent vector $z$ is then obtained as\n",
    "\n",
    "$$\n",
    "z = \\mu + exp(0.5*log(\\sigma^2)) * \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is a random normal tensor.\n",
    "We choose both the encoder and decoder networks to have a single layer of 512 neurons and the size of the latent space to be 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_dim = 512\n",
    "latent_dim = 2\n",
    "\n",
    "# input layer\n",
    "x = keras.layers.Input(shape=(img_size_flat,))\n",
    "\n",
    "# encoder network setup\n",
    "h = keras.layers.Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "# mean and std vector\n",
    "z_mu = keras.layers.Dense(latent_dim)(h)\n",
    "z_log_var = keras.layers.Dense(latent_dim)(h)\n",
    "\n",
    "# sampling layer\n",
    "def sampling(args):\n",
    "    z_mu, z_log_var = args\n",
    "    batch = keras.backend.shape(z_mu)[0]\n",
    "    epsilon = keras.backend.random_normal(shape=(batch, latent_dim),\n",
    "                              mean=0., stddev=1.)\n",
    "    return z_mu + keras.backend.exp(0.5*z_log_var) * epsilon\n",
    "\n",
    "z = keras.layers.Lambda(sampling)([z_mu, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = keras.Model(x, [z_mu, z_log_var, z])\n",
    "\n",
    "# decoder network setup\n",
    "latent_inputs = keras.layers.Input(shape=(latent_dim,))\n",
    "h = keras.layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = keras.layers.Dense(img_size_flat, activation='sigmoid')(h)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = keras.Model(latent_inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct the full variational autoencoder and train it using the custom loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variational autoencoder\n",
    "outputs = decoder(encoder(x)[2])\n",
    "vae = keras.Model(x, outputs)\n",
    "\n",
    "def vae_loss(x, x_decoded):\n",
    "    mse = tf.reduce_sum(tf.square(x - x_decoded))\n",
    "    kl_loss = - 0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mu) - tf.exp(z_log_var), axis=-1)\n",
    "    return mse + kl_loss\n",
    "\n",
    "vae.compile(optimizer='adam', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.fit(train_images, train_images,\n",
    "        shuffle=True,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(test_images, test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate new images of handwritten digits. We chose our latent space to have 2 dimensions so let's now scan this latent plane by sampling points at regular intervals. From each latent vector obtained this way we will generate the corresponding digit by passing it through the decoder/generator network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8  # compute n x n digits\n",
    "figure = np.zeros((img_size * n, img_size * n))\n",
    "# sample points within [-10, 10] standard deviations\n",
    "grid_x = np.linspace(-10, 10, n)\n",
    "grid_y = np.linspace(-10, 10, n)\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(img_size, img_size)\n",
    "        figure[i * img_size: (i + 1) * img_size,\n",
    "               j * img_size: (j + 1) * img_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: New numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a different approach to generating new numbers: try creating a few new numbers as combinations of the old ones (they don't have to be linear combinations of two of them, you could do any size set, but remember that each pixel must be between 0 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Different compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen a hidden size of 2 and it does very well.  Try playing with this size and see what happens if you change it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
