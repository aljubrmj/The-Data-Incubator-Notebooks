{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators and Datasets\n",
    "<!-- requirement: pylib/mnist_dataset.py -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been dealing with a fairly low level API tools starting with building neural networks from scratch and then using the layers API.  TensorFlow offers other API tools to interact with its functionality.  One is an `Estimator` and associated classes which implement a high level API reminiscent of `scikit-learn`.  The other is the `Dataset` API which gives a way of defining data transformations before a neural network.  Here we will explore `Estimator`s and `Dataset`s.  These are probably the types of tools you should be using in production purposes, although for pedagogical reasons, we have saved them until now. \n",
    "\n",
    "We will first import TensorFlow as well as the `mnist` data set generator from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pylib import mnist_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow `Dataset`\n",
    "\n",
    "Often in any machine learning task, we make use of data pipelines to bring our data from its source, perform some transformations on it, and then fit a model.  While Neural Networks do not necessarily require the same feature engineering as standard models, they still require data pipelines to ensure the data is amenable to being fed into the network.  This can be especially important when the data is coming from multiple sources and must be coerced into the particular network architecture the model has been fit with.  TensorFlow offers the [`Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) class as a solution to a few of these data problems.\n",
    "\n",
    "A `Dataset` represents some data as well as transformations that will be performed on the data.  Let's create a `Dataset` from just a list of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.range(10)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at what type of outputs this `Dataset` contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.output_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at what shapes this `Dataset` contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.output_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see its just a list of numbers, which in TensorFlow language is a `TensorShape([])`.  We can confirm this by looking at the output types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.output_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is all well and good to examine the properties of the `Dataset`, but we do need some way of actually acquiring values from it.  Like TensorFlow tensors, the `Dataset` will not have its values on demand.  We must tell it to evaluate the operations it needs to acquire these values.  In this case, that will be create a list of numbers, but in general could be something like reading from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = ds.make_one_shot_iterator()\n",
    "next_ele = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(next_ele))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_elements(ds):\n",
    "    next_ele = ds.make_one_shot_iterator().get_next()\n",
    "    eles = []\n",
    "    with tf.Session() as sess:\n",
    "        try:\n",
    "            while True:\n",
    "                eles.append(sess.run(next_ele))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "    return eles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_elements(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared = ds.map(lambda x: x**2)\n",
    "get_all_elements(squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "even = ds.filter(lambda x: x % 2 == 0)\n",
    "get_all_elements(even)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget that we are dealing with TensorFlow, so the equality comparison is identity and not equivalence.  We will need to use `tf.equal` to get what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "even = ds.filter(lambda x: tf.equal(x % 2, 0))\n",
    "get_all_elements(even)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise*: Get all numbers less than $n$ which have perfect squares within 5 numbers either positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def near_n(ds, n):\n",
    "    near = ds.filter(lambda x: tf.less_equal(x**2, n+5) & tf.greater_equal(x**2, n-5))\n",
    "    return get_all_elements(near)\n",
    "\n",
    "near_n(ds, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For debugging purposes, it might be nice to simulate some of these operations in plain Python. Luckily, we can perform `map`, `filter`, *etc.*, in Python!  Its always a good idea to test out the types of operations we want to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in range(10)]\n",
    "print(list(map(lambda x: x**2, x)))\n",
    "print(list(filter(lambda x: x % 2 ==0, x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we will work with rather large data set and we might want to take only some number of the elements.  We can do this with the `take` operation.  (If at this point you are noticing some real inspiration from `Spark RDD`s then you and the author have the same feelings.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = ds.take(5)\n",
    "get_all_elements(first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from just taking the elements, we can perform a batching operation which will take the `Dataset` and turn it into groups of some number (in this case we will use 2).  This comes in handy when training neural networks.  Note that it is not guaranteed all the batches will be the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched = ds.batch(2)\n",
    "get_all_elements(batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also shuffle the data.  This is useful to ensure that your training steps are not biased towards a particular area in the data set.  The argument to the shuffle is the buffer size which specifies the number of elements to be shuffled at a time.  It is a compromise between randomness and memory usage. Here we will choose all of the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled = ds.shuffle(10)\n",
    "get_all_elements(shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaining these two together gives us a data set which is shuffled and batched!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_and_shuffled = ds.shuffle(10).batch(3)\n",
    "get_all_elements(batched_and_shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question:* Does order matter here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how we might actually use this in a training example?  Let's make just a simple graph to take these numbers and perform some of these operations.  The idea is we can feed the `get_next` operation into the start of our computation graph which in general can be a neural network.  Here we will subtract 19 and add 20, which is more commonly known as adding one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_batch = ds.repeat().batch(3)\n",
    "next_ele = repeat_batch.make_one_shot_iterator().get_next()\n",
    "x = tf.subtract(next_ele, 19)\n",
    "y = tf.add(x, 20)\n",
    "with tf.Session() as sess:\n",
    "    for i in range(4):\n",
    "        print(sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we have used the `repeat` method which repeats the tensor. In this case we have not specified an argument, so it will repeat indefinitely, but in general one can specify the number of times to be repeated as an argument to the `repeat` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an image pipeline with TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another key consideration in machine learning production environments is efficient file storage. When dealing with large amounts of data, this becomes especially important. \n",
    "\n",
    "To this end, TensorFlow provides the `.tfrecords` file format, which stores data in binary strings that can be sequentially read from disk. This provides significant increases in reading speed, especially when working on standard hard disk drives as opposed to solid state drives. Moreover, the TFRecords format makes it easy to combine multiple files, and integrates well with TensorFlow `Datasets.` \n",
    "\n",
    "Let's build a TFRecords file for an image classification problem, where images are currently stored as `.jpg` files in multiple directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the flower images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "data_root = tf.keras.utils.get_file('flower_photos','https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz', untar=True)\n",
    "data_root = pathlib.Path(data_root)\n",
    "print(data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the directory structure. We see there are five classes in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data_root.iterdir():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
    "label_to_index = dict((name, index) for index,name in enumerate(label_names))\n",
    "print(label_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the file path and label for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_paths = list(data_root.glob('*/*'))\n",
    "all_image_paths = [str(path) for path in all_image_paths]\n",
    "all_image_labels = [label_to_index[pathlib.Path(path).parent.name]\n",
    "                    for path in all_image_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the `.tfrecords` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get to the bulk of this tutorial: formatting data correctly to write to TFRecords. Perhaps surprisingly, we don't turn data into tensors before creating the TFRecords file. Instead, each *unit* (in our case, image and label) will be represented as a `tf.train.Example`, which contains multiple `tf.train.Feature` attributes. \n",
    "\n",
    "```\n",
    "TFRecords\n",
    "    tf.train.Example\n",
    "        'image':tf.train.Feature\n",
    "        'label':tf.train.Feature\n",
    "```\n",
    "\n",
    "A `tf.train.Feature` will contain either an `Int64List`, a `FloatList`, or `BytesList`, in accordance with TensorFlow data types. We encode each image with a `FloatList` and each label with an `Int64List`.\n",
    "\n",
    "In this example, we decode and preprocess images before writing them to the TFRecords file. We load and resize the image to a NumPy array of size 32x32, divide by 255, then flatten the array and convert it to a standard list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_filename = 'all_images.tfrecords'\n",
    "\n",
    "writer = tf.python_io.TFRecordWriter(tfrecord_filename)\n",
    "for image, label in zip(all_image_paths, all_image_labels):\n",
    "    img = Image.open(image)\n",
    "    img = np.array(img.resize((32,32)))/255\n",
    "    img = img.reshape(-1).tolist()\n",
    "    \n",
    "    feature = {'image': tf.train.Feature(float_list=tf.train.FloatList(value=(img))),\n",
    "              'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label])) }\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "    # Writing the serialized example.\n",
    "\n",
    "    writer.write(example.SerializeToString())\n",
    "        \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the `.tfrecords` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from a TFRecords file still requires a bit of work. We first read the file to a `tf.data.TFRecordDataset`, then map a parsing function, which will tell TensorFlow what features to expect from each `tf.train.Example`, and reshape the image array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    features = {\"image\": tf.FixedLenFeature([3072,], tf.float32, default_value=[0]*3072),\n",
    "                \"label\": tf.FixedLenFeature((), tf.int64, default_value=0)}\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    return tf.reshape(parsed_features[\"image\"], [32,32,3]), parsed_features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "dataset = tf.data.TFRecordDataset('all_images.tfrecords')\n",
    "dataset = dataset.map(_parse_function)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.shuffle(5000) \n",
    "dataset = dataset.batch(50)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "image_batch, label_batch = sess.run(next_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow `Estimator`\n",
    "Now that we have learned about how to work with `Dataset`s, lets dive into the `Estimator` object.  This object allows us to use both predefined estimators (really just graphs) or easily define our own graphs in a simple way that is oriented towards machine learning.  They don't require managing all of the objects necessary when using the lower `API`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _one_hot(x, y):\n",
    "    return x, tf.one_hot(y, 10)\n",
    "\n",
    "def _input_fn(type_, batch_size, name, one_hot):\n",
    "    data = getattr(mnist_dataset, type_)('/tmp/data')\n",
    "    ds = data.map(lambda x, y : ({name: x}, y)).shuffle(500)\n",
    "    if one_hot:\n",
    "        ds = ds.map(_one_hot)\n",
    "    if type_ == 'train':\n",
    "        ds = ds.repeat()\n",
    "    return ds.batch(batch_size)\n",
    "\n",
    "def test_input_fn(batch_size, name='pixels', one_hot=False):\n",
    "    return _input_fn('test', batch_size, name, one_hot)\n",
    "\n",
    "def train_input_fn(batch_size, name='pixels', one_hot=False):\n",
    "    return _input_fn('train', batch_size, name, one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature_columns = [tf.feature_column.numeric_column(key='pixels', shape=(28*28))]\n",
    "estimator = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    dropout=.5,\n",
    "    hidden_units=[100, 100],\n",
    "    n_classes=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(\n",
    "    input_fn=lambda:train_input_fn(50),\n",
    "    steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.evaluate(input_fn=lambda:test_input_fn(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is great that we can use the baked in estimators, and often they will be suitable for the tasks at hand.  Yet, we will also need to write our own custom estimators.  Remember `Estimator`s break up the data pipeline step and the model step, so we can use our estimator with varying data pipelines.  This might come in handy if you train a model on the `MNIST` data set, but want to perform inference or further training on a different data set which will need to be transformed in some way to be coerced into the proper shape.\n",
    "\n",
    "The main part of an `Estimator` is the model function, think of this as the function which generates a particular computation you want to perform.  In TensorFlow's estimation, there are three things you want your model function to do, *predict*, *train*, and *evaluate*.  The model function should handle the logic behind each one of these steps. Generally speaking the first part of a model function will set up the calculation (graph) and then branch into different logical steps depending on which one of these three actions the user requests the model to perform.  \n",
    "\n",
    "The function signature of the model function will take a few arguments\n",
    "\n",
    "* `features` - the input features to the model\n",
    "* `label` - the input labels (truth values)\n",
    "* `mode` - the mode, usually one of `tf.estimator.ModeKeys.PREDICT, tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL`\n",
    "* `params` - extra parameters needed by the function.\n",
    "\n",
    "The return value of the function will be a `tf.estimator.EstimatorSpec` of which we will use a variety of arguments depending on the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    # Create an input layer from the input features\n",
    "    in_layer = tf.feature_column.input_layer(features, \n",
    "                                             params['feature_columns'])\n",
    "    # Create dense layers and an output layer with 10 classes\n",
    "    \n",
    "    out = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Dense(100, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(100, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(10)\n",
    "        ]\n",
    "    )(in_layer)\n",
    "    \n",
    "    \n",
    "\n",
    "    # If PREDICT mode, return predictions\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions= {\n",
    "            \"class_ids\" : tf.argmax(out, 1)[:, tf.newaxis],\n",
    "            \"probabilities\": tf.nn.softmax(out),\n",
    "            \"logits\": out\n",
    "        }\n",
    "        class_out = tf.estimator.export.ClassificationOutput\n",
    "        return tf.estimator.EstimatorSpec(mode, \n",
    "                                          predictions=predictions,\n",
    "                                          export_outputs={\n",
    "                                              \"predict\":class_out(\n",
    "                                                  scores=predictions[\"probabilities\"],\n",
    "                                                  classes=tf.cast(predictions[\"class_ids\"], tf.string)\n",
    "                                              )\n",
    "                                          })\n",
    "    \n",
    "    # Compute the loss and accuracy\n",
    "    # we will need these for TRAIN and EVAL\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels,\n",
    "                                                  logits=out)\n",
    "    acc = tf.metrics.accuracy(labels=labels,\n",
    "                                 predictions=tf.argmax(out, 1)[:, tf.newaxis],\n",
    "                                 name='accuracy')\n",
    "    # create a summary scalar the Estimator will handle FileWriters\n",
    "    tf.summary.scalar(\"accuracy\", acc[1])\n",
    "    \n",
    "    # If EVAL mode, return the loss and the metrics we want to evaluate\n",
    "    # here we will choose accuracy\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode,\n",
    "                                         loss=loss,\n",
    "                                         eval_metric_ops={'accuracy': acc})\n",
    "    \n",
    "    # If TRAIN mode, create an optimizer and a train operator\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdagradOptimizer(.01)\n",
    "        train = optimizer.minimize(loss, global_step = tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an `Estimator` we can of course use it in exactly the same way as before.  Lets pass in a `model_dir` so we can use `Tensorboard` to view the results of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "custom_est = tf.estimator.Estimator(model_fn=model_fn, \n",
    "                                    model_dir='tb/{}'.format(time.time()),\n",
    "                                    params={\n",
    "    \"feature_columns\":my_feature_columns\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can train our estimator with the same training input function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_est.train(\n",
    "    input_fn=lambda:train_input_fn(50),\n",
    "    steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate using the test input function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_est.evaluate(input_fn=lambda:test_input_fn(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice that our model checkpoints are saved into the directory.  We can check this with a quick Bash command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls {custom_est.model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a model made with `Keras` and turn it directly into an estimator, this is probably how you would want to do things in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Dense(100, activation='relu', \n",
    "                                  name='pixels', \n",
    "                                  input_shape=(28*28,)),\n",
    "            tf.keras.layers.Dense(100, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "model.compile(optimizer='adagrad',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "keras_est = tf.keras.estimator.model_to_estimator(\n",
    "    keras_model=model,\n",
    "    model_dir='tb/{}'.format(time.time())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we look at the model inputs, `Keras` adds some extra text to the input layer name (`pixels_input` instead of `pixels`). To account for this we can use the `name` parameter of the `train_input_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_est.train(input_fn=lambda:train_input_fn(100, \n",
    "                                               name='pixels_input', \n",
    "                                               one_hot=True),\n",
    "               steps=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate it as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_est.evaluate(input_fn=lambda:test_input_fn(100,\n",
    "                                                 name='pixels_input', \n",
    "                                                 one_hot=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a nice model, potentially with checkpoints, but what if we want to deploy into production, how can we save this model in a format useful for `tf.Serving`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving an `Estimator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest thing we will need to write a function to take the data we want to perform inference upon and process it into input proper for our model.  This can be very useful when the inference data is not the same as the training data.  This function is usually called the `serving_input_receiver_fn`, lets define one there.  Often the estimator will receive something like a serialized string which needs to be parsed, but for now we will have it receive data in the correct input format.  We can use the handy `build_raw_serving_input_receiver_fun` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    return tf.estimator.export.build_raw_serving_input_receiver_fn(\n",
    "          {\"pixels\": tf.placeholder(dtype=tf.string, shape=[None])}\n",
    "    )()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_est.export_savedmodel(\"tb/saved_model\", \n",
    "                             serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this model with `tf.Serving` to make some real predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
