{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn API\n",
    "\n",
    "## Classes vs Objects\n",
    "\n",
    "In Scikit-Learn, machine learning algorithms are implemented as classes.\n",
    "\n",
    "For example, the class `LinearRegression`:\n",
    "\n",
    "* Is a recipe for creating linear models, but is not a model itself\n",
    "* Cannot learn or store model parameters\n",
    "* Cannot be used to make predictions\n",
    "\n",
    "By contrast, the object `LinearRegression()`:\n",
    "\n",
    "* Is an instance of the `LinearRegression` class and represents a concrete model\n",
    "* Can learn and store parameters (e.g. regression coefficients)\n",
    "* Can be applied to test data to make predictions\n",
    "\n",
    "Note that hyperparameters are often specified as arguments when we create an instance of a class.  For example, if we want to specify the regularization parameter used in a Ridge model, we can do so as follows:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_model = Ridge(alpha=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn has two main types of classes: estimators and transformers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators\n",
    "\n",
    "Estimators are objects that can make predictions.  All estimators share the following core methods: \n",
    "\n",
    "* **`.fit(X,y)`** learns and stores model parameters based on a training set $(X,y)$ and then returns the trained model.\n",
    "* **`.predict(X)`** returns a list of predicted labels for a test set $X$ using parameters learned from previous training.\n",
    "\n",
    "This common syntax is what we're referring to when we use the phrase Scikit-Learn API.  It makes it very easy to pick up and use new models.  For example, we can very easily fit a linear model to some toy data and make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Training Data\n",
    "X_ = np.linspace(1,10,10)\n",
    "X = X_.reshape(-1,1)\n",
    "y = 2*X_ + 0.3*np.random.randn(10) \n",
    "\n",
    "# Test Data\n",
    "X_test = np.linspace(11,15,5).reshape(-1,1)\n",
    "\n",
    "# Model and Prediction\n",
    "lin_est = LinearRegression() # Create an instance of the LinearRegression class\n",
    "lin_est.fit(X,y)             # Learn model parameters by fitting to the test data\n",
    "lin_est.predict(X_test)      # Make a prediction using the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can implement a more complicated model just as easily (although in this case we'll get worse results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_est = RandomForestRegressor()\n",
    "forest_est.fit(X,y)\n",
    "forest_est.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Scikit-Learn is a bit fussy about how our data is formatted.  $X$ needs to be $2$-dimensional and $y$ needs to be $1$-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised and probabilistic models have additional methods in common:\n",
    "\n",
    "* **`.score(X,y)`** returns either the $R^2$ score or accuracy of the estimator on a test set $(X,y)$ \n",
    "* **`.predict_proba(X)`** returns an array with the raw probabilities predicted by the model for a test set $X$. The output will have one column for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [22, 24, 26, 28, 30]\n",
    "\n",
    "lin_est.score(X_test,y_test), forest_est.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual estimators may have additional methods and attributes, many of which are model specific.  In general, Scikit-Learn's documentation is quite good.  For example, take a look at the page for [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_est.coef_, lin_est.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Transformers are used to do preprocessing on data before feeding it into a model.  All transformers share the following core methods:\n",
    "\n",
    "* **`.fit(X)`** learns and stores parameters based on the set $X$ and then returns the transformer \n",
    "* **`.transform(X)`** transforms the set $X$, relying on stored parameters if appropriate, and then returns the result\n",
    "* **`.fit_transform(X)`** applies fit and transform in sequence\n",
    "\n",
    "For example, the `.transform` method of Scikit-Learn's [`StandardScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) rescales data by applying the formula\n",
    "\n",
    "$$ x \\mapsto \\frac{x- \\mu}{\\sigma} $$\n",
    "\n",
    "to each feature.  The `.fit` method calculates and stores the mean $\\mu$ and the standard deviation $\\sigma$ of whatever data set it is given.  This means that `.fit_transform` rescales data to have mean $0$ and standard deviation $1$.  \n",
    "\n",
    "In general, we do not want to apply `.fit` every time we apply `.transform`.  In machine learning setups, we want to transform the training and test sets the same way, so we usually call the `.fit` method only once (on the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_trans      = scaler.fit_transform(X)         # We should fit our transformer when applying it to the training set\n",
    "X_test_trans = scaler.transform(X_test)        # But not when applying it to the test set\n",
    "\n",
    "# mean and standard deviation\n",
    "print(X.mean(), X.std())                       # of X                  \n",
    "print(X_trans.mean(), X_trans.std())           # of X_trans\n",
    "print(X_test_trans.mean(), X_test_trans.std()) # of X_test_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "Pipelines are a way to combine a sequence of transformers (and optionally an estimator) into a single object.  This saves us the work of computing and storing each transformer output separately and can help us avoid errors.  Pipelines act either as transformers or estimators, depending on whether they contain an estimator as a component.  \n",
    "\n",
    "* **`.fit(X,y)`** calls the `.fit_transform` method of each transformer component and then calls the `.fit` method of the terminal estimator if one is present\n",
    "* **`.predict(X)`** calls the `.transform` method of each transformer component and then calls the `.predict` method of the terminal estimator \n",
    "* **`.transform(X)`** calls the `.transform` method of each component (if none of the components is an estimator)\n",
    "\n",
    "Syntactically, a pipeline in Scikit-Learn takes a list of tuples as arguments.  The first element of each tuple is a string that acts as a label for the corresponding step, and the second element is the transformer or estimator that we want to include.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([(\"feature scaling\", scaler),\n",
    "                 (\"linear regression\", LinearRegression())])\n",
    "\n",
    "pipe.fit(X,y)\n",
    "pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual components of a pipeline can be accessed using the `.named_steps` attribute, which returns a dictionary of steps.  The label strings act as keys and the components themselves are returned as values.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"feature scaling\", scaler),\n",
    "                 (\"ridge\", ridge_model)])\n",
    "\n",
    "print(type(pipe.named_steps))\n",
    "print(pipe.named_steps['ridge'])\n",
    "print(pipe.named_steps['ridge'].alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
