{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- requirement: images/trans_ex1.png -->\n",
    "<!-- requirement: images/trans_ex2.png -->\n",
    "<!-- requirement: images/trans_ex3.png -->\n",
    "<!-- requirement: images/projection.png -->\n",
    "\n",
    "# Transformers and Preprocessing\n",
    "\n",
    "Raw data is often unsuitable for direct use by Machine Learning algorithms. This is most obvious when data is not in the format that the algorithm expects. For example, suppose we are trying to use Linear Regression on the following:\n",
    "\n",
    "$$\n",
    "X_{raw} = \\begin{bmatrix} \\mbox{left}  & 3 \\\\ \n",
    "                          \\mbox{right} & 1 \\\\\n",
    "                          \\mbox{left}  & 2 \\\\\n",
    "                          \\mbox{right} & 5 \\\\\n",
    "                          \\mbox{right} & 4\n",
    "          \\end{bmatrix}\n",
    "\\qquad\\qquad\n",
    "y = \\begin{bmatrix} 5 \\\\ 7 \\\\ 4 \\\\ 11 \\\\ 10\n",
    "    \\end{bmatrix}   \n",
    "$$\n",
    "\n",
    "We can't do numerical computations with strings, so it's impossible to apply the algorithm directly, but we can use an encoding (`left=0`, `right=1`) to represent each string as a number. This yields a transformed data set which can be fit by the algorithm. \n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix} 0  & 3 \\\\ \n",
    "                    1 & 1 \\\\\n",
    "                    0  & 2 \\\\\n",
    "                    1 & 5 \\\\\n",
    "                    1 & 4\n",
    "    \\end{bmatrix}\n",
    "\\qquad\\qquad\n",
    "y = \\begin{bmatrix} 5 \\\\ 7 \\\\ 4 \\\\ 11 \\\\ 10\n",
    "    \\end{bmatrix}   \n",
    "$$\n",
    " \n",
    "In this case, the model $y = 4x_1 + x_2 +2$ is a perfect fit for the data. We just need to remember that `left` and `right` mean $x_1=0$ and $x_1=1$ respectively when the time comes to make predictions for new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing** is a general term for transformation steps like this and **transformers** are objects that we use to learn and apply transformations.\n",
    "\n",
    "* Preprocessing can improve the performance of algorithms even when data begins in a valid format.\n",
    "* Usually a transformer is more than just a single function.\n",
    "\n",
    "For example, suppose that our data has two continuous features. The first varies from $0$ to $10$ and the second varies from $0$ to $10,000$. Distance based algorithms (like K-Nearest Neighbors) will neglect the first feature because of its smaller scale and make predictions that are almost exclusively based on the latter. If we want to remove this bias, then we should normalize the data so that both features are on a comparable scale.\n",
    "\n",
    "![Example 1](images/trans_ex1.png)\n",
    "\n",
    "A simple recipe for normalization is to divide each feature by its standard deviation. It seems natural to express this as a function:\n",
    "\n",
    "$$ \\mbox{norm}(X) = \\frac{X}{\\sigma_X} $$\n",
    "\n",
    "But what happens when we apply this function to a second data set?\n",
    "\n",
    "![Example 2](images/trans_ex2.png)\n",
    "\n",
    "The second set is scaled differently because its standard deviations are different from the first set. If you imagine that the blue points represent our training set and the red points represent our test set, then its clear we've done something wrong.\n",
    "\n",
    "The solution is to break the process into two steps:\n",
    "\n",
    "* **Fit:** Compute and store the parameter $\\sigma = \\sigma_X$ based only on the training set.\n",
    "* **Transform:** Apply the function $X \\mapsto X/\\sigma$, using the same stored value of $\\sigma$ for both training and test sets. \n",
    "\n",
    "![Example 3](images/trans_ex3.png)\n",
    "\n",
    "Now both data sets are transformed in a consistent way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a transformer is an object in Scikit-learn with the following methods:\n",
    "\n",
    "* **`.fit(X)`:** Learn and store parameters based on $X$ (if any). Returns the transformer itself (to allow for method chaining).\n",
    "* **`.transform(X):`** Applies a transformation function to $X$ (using stored parameters, if any) and returns the result. \n",
    "* **`.fit_transform(X):`** Shorthand for `.fit(X).transform(X)`\n",
    "\n",
    "As you might expect, Scikit-learn implements transformation algorithms using classes. For example, `MaxAbsScaler` is a recipe for a specific type of scaling transformer, and `MaxAbsScaler()` is an object that implements that recipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "trans = MaxAbsScaler()\n",
    "\n",
    "X = np.arange(6).reshape(-1,1)\n",
    "print(X)\n",
    "\n",
    "trans.fit(X)\n",
    "trans.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.arange(6).reshape(-1,1) + 5\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What will each of the following cells output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans.transform(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans.fit_transform(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans.fit(X).transform(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "There are many situations in which it is beneficial to normalize data so that all features have comparable scales:\n",
    "\n",
    "* To avoid biasing distance based algorithms (like K-Nearest Neighbors)\n",
    "* When we plan to interpret coefficients in a linear model as a measure of feature importance\n",
    "* When using regularization (a penalty based on coefficient size used to combat overfitting)\n",
    "\n",
    "There are also many different ways to normalize data (also referred to as performing feature scaling):\n",
    "\n",
    "* We can divide by the standard deviation or the range of the data in each dimension.\n",
    "* We may want to center the data by shifting it so that the mean is zero in each dimension.\n",
    "* We may want to transform data so that each feature is mapped to the same range (e.g. the interval $[0,1]$).\n",
    "\n",
    "One of the most popular options is the `StandardScaler` transformer which scales data to have a mean of $0$ and a standard deviation of $1$. More precisely, its `.transform` method applies the linear transformation\n",
    "\n",
    "$$ X \\mapsto \\frac{X - \\mu}{\\sigma} $$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are the mean and standard deviation learned from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = np.arange(6, dtype=np.float32).reshape(-1,1)\n",
    "X_transformed = StandardScaler().fit_transform(X)\n",
    "\n",
    "print(\"X:\\n\", X, \"\\n\")\n",
    "print(\"X_transformed:\\n\",X_transformed, \"\\n\")\n",
    "print(\"mean:\",X_transformed.mean())\n",
    "print(\"standard deviation:\",X_transformed.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.arange(6, dtype=np.float32).reshape(-1,1)*2 + 5\n",
    "X_new_transformed = StandardScaler().fit(X).transform(X_new)\n",
    "\n",
    "print(\"X_new:\\n\", X_new, \"\\n\")\n",
    "print(\"X_new_transformed:\\n\",X_new_transformed, \"\\n\")\n",
    "print(\"mean:\",X_new_transformed.mean())\n",
    "print(\"standard deviation:\",X_new_transformed.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's fine to use `StandardScaler` for most applications, but you should consider [other options](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling) if you want to control the range of the output or if you're worried about outliers distorting the transformation. \n",
    "\n",
    "The most common edge case to worry about is scaling sparse data. A data matrix is called sparse when most of its values are $0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matr = np.eye(8,3,0) * 5\n",
    "matr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's most efficient to store sparse data in a compressed format where we only record the location and value of the non-zero entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sparse_matr = sparse.csr_matrix(matr)\n",
    "print(sparse_matr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StandardScaler` is a poor choice for sparse data:\n",
    "\n",
    "* Subtracting the mean from each column _breaks_ the sparsity of the matrix, forcing us to use a less efficient representation. If the data set is large, this can cause memory errors. \n",
    "* The standard deviations of sparse features are abnormally small (since most values of each feature are 0). Dividing by these standard deviations can distort the data instead of normalizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StandardScaler().fit_transform(sparse_matr.todense()) # What happens if we remove .todense()? Why?               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better alternative is `MaxAbsScaler`, which divides each feature by its maximum absolute value and does not perform any shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "print(MaxAbsScaler().fit_transform(sparse_matr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#) let's us scale features to a range whose minimum and maximum we choose explicitly. Can you think of any disadvantages of scaling to a fixed range (compared to the flexible scaling done by `StandardScaler`)? \n",
    "1. Can you think of any situations in which feature scaling might _hurt_ the performance of a Machine Learning algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables\n",
    "\n",
    "Strings and other categorical values need to be given a numerical representation before most Machine Learning algorithms can be applied. The simplest solution, called **label encoding**, is to assign each unique value a different number. For example, if we have a color feature, then we might encode its values as follows.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mbox{blue} \\mapsto 0 \\\\\n",
    "\\mbox{green} \\mapsto 1 \\\\\n",
    "\\mbox{red} \\mapsto 2 \\\\\n",
    "\\mbox{yellow} \\mapsto 3 \n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "ex = [\"red\", \"green\", \"red\", \"yellow\", \"blue\", \"red\"]\n",
    "print(ex)\n",
    "print(LabelEncoder().fit_transform(ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encoding is easy, but the values it produces aren't particularly meaningful. Is \"yellow\" three times as much as \"green\"? Is \"red\" more than \"green\"? Not really. \n",
    "\n",
    "This can be a big problem for Machine Learning algorithms. For example, suppose we are trying to use Linear Regression to predict car prices using features like the model and mileage. With label encoding, our data matrix will look something like this:\n",
    "\n",
    "$$\n",
    "X_{raw} = \\begin{array}{c}\n",
    "\\begin{matrix}\n",
    "\\mbox{model} & \\mbox{mileage}\\!\n",
    "\\end{matrix} \\\\\n",
    "\\left[\\ \\begin{matrix}\n",
    "\\mbox{Accord} & 50,000  \\\\\n",
    "\\mbox{Accord} & 40,000  \\\\\n",
    "\\mbox{Camry} & 110,000  \\\\\n",
    "\\mbox{Camry} & 65,000  \\\\\n",
    "\\mbox{Civic} & 10,000  \n",
    "\\end{matrix}\\ \\right]\n",
    "\\end{array}\n",
    "\\quad \\quad\n",
    "X = \\begin{array}{c}\n",
    "\\begin{matrix}\n",
    "\\mbox{model} & \\mbox{mileage}\\!\n",
    "\\end{matrix} \\\\\n",
    "\\left[\\ \\begin{matrix}\n",
    "0 & 50,000  \\\\\n",
    "0 & 40,000  \\\\\n",
    "1 & 110,000  \\\\\n",
    "1 & 65,000  \\\\\n",
    "2 & 10,000  \n",
    "\\end{matrix}\\ \\right]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Using a Linear Regression algorithm means finding an approximation of the form\n",
    "\n",
    "$$ \\mbox{price} \\approx c_0 + c_1 \\cdot \\mbox{model} + c_2 \\cdot \\mbox{mileage} $$\n",
    "\n",
    "The algorithm will probably learn some negative coefficient for the `mileage` variable. It makes intuitive sense that the price of each car will decrease as it is driven more. But what can the algorithm do with the `model` variable? More than you might expect, but less than we'd like. On the one hand, increases in the value of `model` indirectly convey information about what model the car is, so we should expect some correlation between `model` and `price`. On the other hand, our setup forces us to conflate different scenarios (does an increase of $1$ mean that the model changed from Accord to Camry or from Camry to Civic?), which essentially means that we're throwing away useful information. \n",
    "\n",
    "A better approach is to disentangle different category values by creating a new feature (or indicator variable) for each. This approach, called **one-hot encoding**, yields the following for our car example:\n",
    "\n",
    "$$\n",
    "X_{raw} = \\begin{array}{c}\n",
    "\\begin{matrix}\n",
    "\\mbox{model} & \\mbox{mileage}\\!\n",
    "\\end{matrix} \\\\\n",
    "\\left[\\ \\begin{matrix}\n",
    "\\mbox{Accord} & 50,000  \\\\\n",
    "\\mbox{Accord} & 40,000  \\\\\n",
    "\\mbox{Camry} & 110,000  \\\\\n",
    "\\mbox{Camry} & 65,000  \\\\\n",
    "\\mbox{Civic} & 10,000  \n",
    "\\end{matrix}\\ \\right]\n",
    "\\end{array}\n",
    "\\quad \\quad\n",
    "X = \\begin{array}{c}\n",
    "\\begin{matrix}\n",
    "\\mbox{is_accord} & \\mbox{is_camry} & \\mbox{is_civic} & \\mbox{mileage}\\!\n",
    "\\end{matrix} \\\\\n",
    "\\left[\\ \\begin{matrix}\n",
    "\\qquad 1 & \\qquad 0 & \\qquad 0 & \\qquad 50,000  \\\\\n",
    "\\qquad 1 & \\qquad 0 & \\qquad 0 & \\qquad 40,000  \\\\\n",
    "\\qquad 0 & \\qquad 1 & \\qquad 0 & \\qquad 110,000  \\\\\n",
    "\\qquad 0 & \\qquad 1 & \\qquad 0 & \\qquad 65,000  \\\\\n",
    "\\qquad 0 & \\qquad 0 & \\qquad 1 & \\qquad 10,000  \n",
    "\\end{matrix}\\ \\right]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Now our Linear Regression is trying to find an approximation of the form\n",
    "\n",
    "$$ \\mbox{price} \\approx c_0 + c_1 \\cdot \\mbox{is_accord} + c_2 \\cdot \\mbox{is_camry} + c_3 \\cdot \\mbox{is_civic} + c_4 \\cdot \\mbox{mileage} $$\n",
    "\n",
    "and we're getting more use out of the `model` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we implement one-hot encoding in practice? The `pandas.get_dummies` function is an excellent option, especially if we only need to encode a single data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "models  = [\"Accord\", \"Accord\", \"Camry\", \"Camry\", \"Civic\"]\n",
    "mileages = [50000, 40000, 110000, 65000, 10000]\n",
    "\n",
    "X = pd.DataFrame({\"model\": models, \"mileage\": mileages})\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why wasn't the `mileage` column encoded?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drawback of using `get_dummies` is that it is not a transformer. If we want to encode a second data set in a consistent way, then we need to keep track of what labels we saw the first time and be careful about what columns represent which labels. We can't simply apply `get_dummies` a second time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pd.DataFrame({\"model\": [\"Civic\", \"Civic\", \"Camry\"], \"mileage\": [30000, 40000, 50000]})\n",
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we've lost the `model_Accord` column. This simple issue could block us completely if we lose access to the original data set, but it's easily handled if we've done the proper bookkeeping.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns of the original output\n",
    "X_trans = pd.get_dummies(X)\n",
    "old_cols = set(X_trans) \n",
    "\n",
    "# Get the columns of the new output\n",
    "X_new_trans = pd.get_dummies(X_new)\n",
    "new_cols = set(X_new_trans)\n",
    "\n",
    "# Add the missing columns to the new output\n",
    "for col in old_cols.difference(new_cols):\n",
    "    X_new_trans[col] = 0\n",
    "\n",
    "# Put new columns in order (they were added on the right by default)    \n",
    "X_new_trans.sort_index(axis=1, inplace=True)\n",
    "X_new_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an ideal world, we would have a single transformer handle all of this for us. If you have the very latest version of Scikit-learn (`0.20.1` or greater), then [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) does the trick. Unfortunately, older versions of this transformer assume that data is label encoded to start with and cannot handle string values. `LabelEncoder` isn't built for 2-D arrays, so trying to encode data using the built in tools has traditionally been a pain point.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import __version__ as version\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using an older version and want a self contained solution using the Scikit-learn API, you may want to write a custom transformer to do the job. (For example, see the Custom Transformers section below) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other encoding methods, but one-hot encoding is the most common and is usually a fine choice. If you're interested in alternatives, you may want to look at the [category encoder package](http://contrib.scikit-learn.org/categorical-encoding/).\n",
    "\n",
    "**Question:** One alternate encoding method, called **binary encoding**, uses $N$ binary features to represent a categorical feature with $\\leq 2^N$ unique values. For example, a binary encoding of the colors from before would look something like this:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mbox{blue} \\mapsto 0 \\mapsto (0,0) \\\\\n",
    "\\mbox{green} \\mapsto 1 \\mapsto (0,1) \\\\\n",
    "\\mbox{red} \\mapsto 2 \\mapsto (1,0) \\\\\n",
    "\\mbox{yellow} \\mapsto 3 \\mapsto (1,1) \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "What do you think the advantages and disadvantages of this method are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "When data is missing, it's often preferable to impute or artificially assign values to empty fields rather than disregarding incomplete observations entirely. This is especially important when we expect the model we are training to be applied in situations with incomplete information.\n",
    "\n",
    "Scikit-learn offers the `Imputer` transformer, which replaces missing values (instances of `np.nan`) with the average value of the appropriate feature (choose your preferred definition of 'average' using the `strategy` argument). More sophisticated imputation is better preformed in NumPy or Pandas, but `Imputer` has the advantage of convenience. Being a transformer means that it is easy to reuse, behaves consistently, and can be incorporated into pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "X_train = np.arange(8).reshape(4,2)*1.\n",
    "X_train[0][1] = np.nan\n",
    "X_train[3][0] = np.nan\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "trans = Imputer(strategy=\"mean\")\n",
    "\n",
    "print(\"\\n\", trans.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The means learned by .fit\n",
    "trans.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([[8, np.nan], [np.nan, 12]])\n",
    "print(X_test)\n",
    "\n",
    "print(\"\\n\", trans.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, transformers like `Imputer` are easily incorporated into Scikit-learn pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# This pipeline makes LinearRegression 'robust' by \n",
    "# adding imputation as an automatic preprocessing step\n",
    "RobustRegressor = Pipeline([(\"Imputer\",   Imputer(strategy=\"mean\")),\n",
    "                            (\"Regressor\", LinearRegression())])\n",
    "\n",
    "# Toy data so we can demo .fit/.predict syntax\n",
    "y_train = np.arange(4)\n",
    "\n",
    "RobustRegressor.fit(X_train, y_train)\n",
    "RobustRegressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **`RobustRegressor.fit`** is equivalent to **`Imputer().fit_transform`** followed by **`LinearRegression().fit`**.\n",
    "\n",
    "Similarly, **`RobustRegressor.predict`** is equivalent to **`Imputer().transform`** followed by **`LinearRegression().predict`**.\n",
    "\n",
    "**Question:** While they correctly convey the behavior of the pipeline, the preceding two statements are technically inaccurate. Why should we avoid running code like `Imputer().fit(X_train)` or `Imputer().fit_transform(X_train)` followed by `Imputer().transform(X_test)`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction\n",
    "\n",
    "The term **dimension reduction** refers to a variety of techniques for reducing the number of features in a data set. There are many reasons why this may be desirable:\n",
    "\n",
    "* To save memory or reduce computation\n",
    "* For visualization (since it's very hard to plot data with more than 2 or 3 dimensions)\n",
    "* To combat overfitting\n",
    "\n",
    "Caution is advised on this final point as premature dimension reduction is one of the easiest ways to _underfit_. After all, we are essentially throwing away information.\n",
    "\n",
    "In general, the goal is to keep as much information as possible while using as few features as we can get away with. Rather than using a subset of the original features, we create new artificial features which are linear combinations of the originals. Mathematically, if we think of our data set as a collection of points in $N$ dimensional space, then dimension reduction is simply projection onto a lower dimensional subspace.\n",
    "\n",
    "![Dimension Reduction Example](images/projection.png)\n",
    "\n",
    "How should we choose this subspace? There are several methods to choose from, but the most popular is **principal component analysis**, which maximizes the proportion of the data's variance that is conserved when the projection is applied. This is implemented in Scikit-learn as the [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array([2*np.arange(10), -np.arange(10)]).T + np.round(np.random.random((10,2))/10,3)\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "PCA(n_components=1).fit_transform(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's recommended to center data (subtract the mean in each dimension) before applying `PCA`, but you should be cautious about normalization (dividing by the standard deviation in each dimension) as this has the potential to inflate features that `PCA` would otherwise ignore.  \n",
    "\n",
    "For more information about dimension reduction, please refer to `ML_Unsupervised_Learning.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "One of the fundamental challenges of natural language processing is how to encode text data in a numerical format. The most common method, called the **bag of words** approach, is simply to count how many times each word occurs in each document. For example, if our 'documents' are short phrases, the bag of words encoding would look something like this:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\underline{\\mbox{Phrase}}      &   & \\underline{\\mbox{and}} & \\underline{\\mbox{brown}} & \\underline{\\mbox{fox}} &                                                    \\underline{\\mbox{happy}} & \\underline{\\mbox{moose}} & \\underline{\\mbox{proud}} &                                                \\underline{\\mbox{quick}} & \\underline{\\mbox{the}} \\\\\n",
    "\\mbox{\"the quick brown fox\"}   &             & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 1 \\\\\n",
    "\\mbox{\"the proud happy moose\"} & \\rightarrow & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 1 \\\\\n",
    "\\mbox{\"happy and proud\"}       &             & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We lose information about word order, but this easy and robust approach is sufficient for a wide range of applications.\n",
    "\n",
    "In Scikit-learn, bag of words is implemented by the `CountVectorizer` transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_encoder = CountVectorizer()\n",
    "\n",
    "bag_of_words_array = bag_encoder.fit_transform([\"Let's feed the duck.\", \"Let's play duck duck goose.\"])\n",
    "bag_of_words_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use a large vocabulary, then most of the counts will be $0$, so the output is stored in sparse format by default. Converting to dense format is fine for small outputs, but not recommended when working with large amounts of data. \n",
    "\n",
    "Note that `CountVectorizer` builds its vocabulary from the words it encounters in the training set. The key-value pairs stored in the `.vocabulary_` attribute are essentially column labels for the transformed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bag_of_words_array.todense())\n",
    "print()\n",
    "print(bag_encoder.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question: ** What are the separate functions of `CountVectorizer`'s `.fit` and `.transform` methods?\n",
    "\n",
    "Scikit-learn also includes some basic variants of `CountVectorizer`:\n",
    "\n",
    "* [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) applies some NLP-specific feature scaling (TF-IDF normalization) on top of `CountVectorizer`'s normal behavior.\n",
    "* [`HashingVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) removes the costs of needing to `.fit` a training set by using a hash function in place of an explicit vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformers\n",
    "\n",
    "If no built-in transformer suits your needs, you can always write your own. Using the template below guarantees that your custom transformer class is compatible with the rest of Scikit-learn, which is really nice if you need to include custom transformers in a pipeline. \n",
    "\n",
    "``` python\n",
    "class Transformer(base.BaseEstimator, base.TransformerMixin):\n",
    "  def __init__(self, ...):\n",
    "    # initialization code\n",
    "    \n",
    "  def fit(self, X, y=None):\n",
    "    # fit the transformation\n",
    "    return self\n",
    "  \n",
    "  def transform(self, X):\n",
    "    return ... # transformation\n",
    "```\n",
    "\n",
    "Note that the `.fit` method _must_ return `self` in order for `.fit_transform` to work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a demonstration of syntax, here's a simple example which centers data by learning and subtracting the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import base\n",
    "\n",
    "class CenteringTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "  \n",
    "    def __init__(self):\n",
    "        self.mean = 0 \n",
    "\n",
    "    def fit(self, X):\n",
    "        self.mean = X.mean()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X - self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.arange(5)\n",
    "X_test  = np.arange(5) + 5\n",
    "trans = CenteringTransformer()\n",
    "\n",
    "trans.fit(X_train)\n",
    "\n",
    "print(trans.mean)\n",
    "print()\n",
    "print(\"X_train:\")\n",
    "print(X_train, trans.transform(X_train))\n",
    "print()\n",
    "print(\"X_test:\")\n",
    "print(X_test, trans.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a more practical example, here's the one-hot encoding procedure from before written as a transformer. This version assumes that the inputs are dataframes, but you can easily add branches to handle NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEncoder(base.BaseEstimator, base.TransformerMixin):\n",
    "  \n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns # The columns to be encoded, if we want to specify manually.\n",
    "        self.output_cols = []  # The columns that are expected in the output.\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.output_cols = set(pd.get_dummies(X))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_trans = pd.get_dummies(X)\n",
    "        new_cols = set(X_trans)\n",
    "        for col in self.output_cols.difference(new_cols):\n",
    "            X_trans[col] = 0\n",
    "        for col in new_cols.difference(self.output_cols): # New feature values are ignored to ensure consistent representation\n",
    "            X_trans.drop([col], axis=1, inplace=True)\n",
    "        return X_trans.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = DummyEncoder()\n",
    "\n",
    "models  = [\"Accord\", \"Accord\", \"Camry\", \"Camry\", \"Civic\"]\n",
    "mileages = [50000, 40000, 110000, 65000, 10000]\n",
    "\n",
    "X = pd.DataFrame({\"model\": models, \"mileage\": mileages})\n",
    "trans.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pd.DataFrame({\"model\": [\"Ferrari\", \"Civic\", \"Camry\"], \"mileage\": [30000, 40000, 50000]})\n",
    "X_new\n",
    "\n",
    "trans.transform(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We're actually calling `pd.get_dummies` _twice_ every time we `.fit_transform`. The `.fit` method could be more efficient since we only need to determine the unique labels, but the inefficiency shouldn't bother us unless we're working with a large amount of data. This is an example of prioritizing developer time over computation time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Questions\n",
    "\n",
    "#### Feature Scaling\n",
    "\n",
    "1. This method is much more sensitive to outliers. The maximum and minimum values in the data set (for each feature) completely determine the transformation used by `MinMaxScaler` while the standard deviations used by `StandardScaler` are determined in a more balanced way.\n",
    "1. Feature scaling sometimes destroys information when features are already comparable and measured in the same units (for example, the lengths and widths in the [Iris data set](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)). Feature scaling may also be inappropriate when used prior to dimension reduction (if the relative variances of the features are meaningful) although it is still correct to center data in these settings by subtracting the mean.\n",
    "\n",
    "#### Encoding Categorical Variables\n",
    "\n",
    "* The `mileage` column is automatically skipped because it has a numerical data type. Numbers sometimes represent categorical labels, so we can override this behavior using the `columns` argument of `pd.get_dummies` if we choose. Conversely, we should take care that columns with continuous values don't accidentally get the `object` data type (e.g. if numbers represented as strings).\n",
    "* Binary encoding is a compromise between label encoding and one-hot encoding. The main downside of one-hot encoding is that it can create an unwieldy number of features, especially when the original categorical features take many different values. Binary encoding essentially prevents this explosion of features, but the features it generates are more abstract and less interpretable (similar to label encoding).   \n",
    "\n",
    "#### Imputation\n",
    "\n",
    "* Every instance of `Imputer()` is creating a new instance of the `Imputer` class, so the means learned by `.fit` won't be stored. If we want information to persist across steps, then we need to create a persistent object with something like `trans=Imputer()`. This happens automatically inside of the pipeline object. To be pedantic:\n",
    "\n",
    "`RobustRegressor.fit` is \n",
    "\n",
    "`RobustRegressor.named_steps['Imputer'].fit_transform` followed by\n",
    "\n",
    "`RobustRegressor.named_steps['Regressor'].fit`.\n",
    "\n",
    "And `RobustRegressor.predict` is \n",
    "\n",
    "`RobustRegressor.named_steps['Imputer'].transform` followed by \n",
    "\n",
    "`RobustRegressor.named_steps['Regressor'].predict`.\n",
    "\n",
    "#### Natural Language Processing\n",
    "\n",
    "* `.fit` learns a vocabulary consisting of all of the unique words present in the training documents. It also chooses an order for these words (alphabetical by default) and stores that order as a dictionary (essentially a pairing of words to column numbers). `.transform` uses the stored vocabulary and order to convert the given list of documents to an array of word counts. Any new words in the documents that aren't in the stored vocabulary are simply ignored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
