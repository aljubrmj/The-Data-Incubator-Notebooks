{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- requirement: images/pipe_arch.svg -->\n",
    "<!-- requirement: small_data/cal_house.json.gz -->\n",
    "\n",
    "# Scikit-learn Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Understand how to write custom classes in Scikit-learn\n",
    " - Pipelines\n",
    " - Feature Unions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing custom estimators and transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Scikit-learn library has a wealth of functionality available in its [classes](http://scikit-learn.org/stable/modules/classes.html). Occasionally you might want to customize the behavior of these classes, for example to add in functionality or for engineering reasons.\n",
    "\n",
    "Scikit-learn has some detailed documentation on customizing these classes - start [here](http://scikit-learn.org/stable/developers/contributing.html#apis-of-scikit-learn-objects) and make sure to get to the \"Rolling your own Estimator\" section. \n",
    "\n",
    "This notebook contains a bit more exposition and some examples for implementing custom classes. Think of it as a field guide to writing your own pluggable functionality.\n",
    "\n",
    "All estimators (e.g. Linear Regression, KMeans, etc ...) support `fit` and `predict` methods.  In fact, you can build your own by inheriting from classes in `sklearn.base` by using this template:                                                                                                 \n",
    "``` python                                                                                                                                        \n",
    "class Estimator(base.BaseEstimator, base.RegressorMixin):\n",
    "  def __init__(self, ...):\n",
    "    # initialization code\n",
    "  \n",
    "  def fit(self, X, y):\n",
    "    # fit the model ...\n",
    "    return self\n",
    "    \n",
    "  def predict(self, X):\n",
    "    return # prediction\n",
    "    \n",
    "  def score(self, X, y):\n",
    "    return # custom score implementation\n",
    "```\n",
    "\n",
    "Conforming to this convention has the benefit that many tools (e.g. cross-validation, grid search) rely on this interface so you can use your new estimators with the existing `sklearn` infrastructure.                                                                 \n",
    "                                                                                \n",
    "For example `model_selection.GridSearchCV` takes an estimator and some hyperparameters as arguments, and returns another estimator.  Upon fitting, it fits the best model (based on the inputted hyperparameters) and uses that for prediction.                                                                    \n",
    "                                                                                \n",
    "Of course, we sometimes need to process or transform the data before we can do machine learning on it.  `sklearn` has Transformers to help with this.  They implement this interface:\n",
    "``` python\n",
    "class Transformer(base.BaseEstimator, base.TransformerMixin):\n",
    "  def __init__(self, ...):\n",
    "    # initialization code\n",
    "    \n",
    "  def fit(self, X, y=None):\n",
    "    # fit the transformation\n",
    "    return self\n",
    "  \n",
    "  def transform(self, X):\n",
    "    return ... # transformation\n",
    "```\n",
    "\n",
    "A comprehensive `.fit_transform` is implemented based on the `.fit` and `.transform` methods in `base.TransformerMixin` ([docs](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html)). However, especially for transformers, `.fit` is often empty and only `.transform` actually does something.\n",
    "\n",
    "The following is some example code to demonstrate the usage of custom classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "cali_data = json.load(gzip.open('small_data/cal_house.json.gz'))\n",
    "\n",
    "cali_data['data'] = np.array(cali_data['data'])\n",
    "cali_data['target'] = np.array(cali_data['target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cali_data['data'], cali_data['target'], test_size=0.2, random_state=42)\n",
    "cali_data['data'].shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenteringTransformer(sk.base.BaseEstimator, sk.base.TransformerMixin):\n",
    "    \"\"\"\n",
    "    Centers the features about 0\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        self.means = X.mean(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X - self.means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CenteringTransformer()\n",
    "ct.fit(X_train)\n",
    "# Note: trained on train data, testing on test data\n",
    "X_test.mean(), ct.transform(X_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some estimators and transformers take arguments.  These can be specified in the `__init__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncateTransformer(sk.base.BaseEstimator, sk.base.TransformerMixin):\n",
    "    \"\"\"\n",
    "    Returns the first k columns of a feature array\n",
    "    \"\"\"\n",
    "    def __init__(self, k=None):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.k is None:\n",
    "            return X\n",
    "        else:\n",
    "            return X[:,:self.k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncator = TruncateTransformer(2)\n",
    "truncator.transform(X_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a silly estimator that returns a values based on whether there are more positive or negative features for an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosNegEstimator(sk.base.BaseEstimator, sk.base.RegressorMixin):\n",
    "    \"\"\"\n",
    "    Predict mean values based on whether there are more positive or negative features.\n",
    "    \n",
    "    Yes, this will do a terrible job.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.pos_mean = 0\n",
    "        self.neg_mean = 0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pos_rows = (X > 0).sum(axis=1) > X.shape[1]/2\n",
    "        self.pos_mean = y[pos_rows].mean() if sum(pos_rows) > 0 else 0\n",
    "        self.neg_mean = y[~pos_rows].mean() if sum(~pos_rows) > 0 else 0\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pos_rows = (X > 0).sum(axis=1) > X.shape[1]/2\n",
    "        y = np.zeros(X.shape[0])\n",
    "        y[pos_rows] = self.pos_mean\n",
    "        y[~pos_rows] = self.neg_mean\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pne = PosNegEstimator()\n",
    "pne.fit(X_train, y_train)\n",
    "pne.predict(X_test)\n",
    "pne.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RegressorMixin` provides a `.score` method that calculates the coefficient of determination.  In this case, all rows are positive, so this is just a mean model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CenteringTransformer()\n",
    "pne = PosNegEstimator()\n",
    "\n",
    "ct.fit(X_train, y_train)\n",
    "X_train_center = ct.transform(X_train)\n",
    "pne.fit(X_train_center, y_train)\n",
    "X_test_center = ct.transform(X_test)\n",
    "pne.score(X_test_center, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit method returns the `self` object, so that method calls can be chained.  The `.fit_transform` method is equivalent to chaining the `.transform` method to the `.fit` method.  Together, these can turn the above into a one-liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pne.fit(ct.fit_transform(X_train), y_train).score(ct.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting transformers to estimators can become tedious.  One way around this is to build an estimator that combines them into one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShellEstimator(sk.base.BaseEstimator, sk.base.RegressorMixin):\n",
    "    \"\"\"\n",
    "    A shell estimator that combines a transformer and regressor into a single object.\n",
    "    \"\"\"\n",
    "    def __init__(self, transformer, model):\n",
    "        self.transformer = transformer\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_trans = self.transformer.fit(X, y).transform(X)\n",
    "        self.model.fit(X_trans, y)\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        X_test = self.transformer.transform(X)\n",
    "        return self.model.score(X_test, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_test = self.transformer.transform(X)\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncator = TruncateTransformer(2)\n",
    "X_train_k = truncator.transform(X_train)\n",
    "X_test_k = truncator.transform(X_test)\n",
    "\n",
    "linreg = LinearRegression()\n",
    "k_model = linreg.fit(X_train_k, y_train)\n",
    "y_pred_k = k_model.predict(X_test_k)\n",
    "print(k_model.score(X_test_k, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_shell = ShellEstimator(TruncateTransformer(2), LinearRegression(fit_intercept=True))\n",
    "k_shell.fit(X_train, y_train)\n",
    "assert((k_shell.predict(X_test) == y_pred_k).all())\n",
    "print(k_shell.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out there's a built-in tool to chain together our transformers and estimators into one unit, and it scales much easier than custom estimators. They're called pipelines. The following code would replace all the fitting and scoring code above.  That is, the pipeline itself is an estimator (and implements the `.fit` and `.predict` methods).  Note that a pipeline can have multiple transformers chained up but at most one (optional) terminal estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "\n",
    "k_pipe = pipeline.Pipeline([\n",
    "  ('truncate', TruncateTransformer(2)),\n",
    "  ('linreg', LinearRegression(fit_intercept=True))\n",
    "  ])\n",
    "k_pipe.fit(X_train, y_train)\n",
    "print(k_pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyper-parameters of the component elements are available as parameters in the pipeline with the `element__param_name` format.  This means the pipelines can be used in cross-validation routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a Pipeline, `pipe`, defined as \n",
    "```\n",
    "pipe = Pipe([\n",
    "    ('mod_a', model_a),\n",
    "    ('mod_b', model_b),\n",
    "    ('mod_c', model_c)\n",
    "])\n",
    "```\n",
    "When `pipe.fit(X, y)` is called, the following chain of operations will take place (note this is pseudocode):\n",
    "```\n",
    "Xt = model_a.fit_transform(X, y)\n",
    "Xt = model_b.fit_transform(Xt, y)\n",
    "model_c.fit(Xt, y)\n",
    "```\n",
    "Thus the fit method of a pipeline will fit the last model in the pipeline with a feature matrix obtained by successive transformations of the original feature matrix by all of the previous models.\n",
    "\n",
    "When `pipe.predict(X)` is called, the following chain of operations will take place (again in pseudocode, Scikit-learn checks a few things under the hood we will ignore here):\n",
    "```\n",
    "Xt = model_a.transform(X)\n",
    "Xt = model_b.transform(Xt)\n",
    "model_c.predict(Xt)\n",
    "```\n",
    "Thus the predict method will use the predict method of the last model with a feature matrix obtained by successive transformations of the original feature matrix by all of the previous models.\n",
    "\n",
    "A good place to delve into the details is the source code located [here](https://github.com/scikit-learn/scikit-learn/blob/0.19.X/sklearn/pipeline.py#L29)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Unions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Feature unions](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) are designed to get around the problem that you might not be able to prepare your desired feature matrix with just one series of transformations. For example, you might have text features, categorical features, and a couple different kinds of numerical features and want to feed them all into the same estimator or pipeline. Each feature type would require a different kind of transformer.\n",
    "\n",
    "What the feature union does is a kind of *parallel* transformation operation using multiple transformers and consolidating them into one output matrix â€” which can then be fed into an estimator or pipeline. You can imagine that between the serial behavior of pipelines and the parallel behavior of feature unions you can create complex multi-stage workflows.\n",
    "\n",
    "This example code applies several different transformations to X before throwing the features into a Linear Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseTruncateTransformer(sk.base.BaseEstimator, sk.base.TransformerMixin):\n",
    "    \"\"\"\n",
    "    Returns the last k columns of a feature array\n",
    "    \"\"\"\n",
    "    def __init__(self, k=None):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.k is None:\n",
    "            return X\n",
    "        else:\n",
    "            return X[:,-self.k:]\n",
    "     \n",
    "all_features = pipeline.FeatureUnion([\n",
    "  ('first two cols', TruncateTransformer(2)),\n",
    "  ('last two cols', ReverseTruncateTransformer(2))\n",
    "  ])\n",
    "k_union = pipeline.Pipeline([\n",
    "  (\"features\", all_features),\n",
    "  (\"linreg\", LinearRegression(fit_intercept=True))\n",
    "  ])\n",
    "k_union.fit(X_train, y_train)\n",
    "print(k_union.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining feature unions and pipelines, it is possible to build arbitrarily complex flows from raw data to final predictions, all contained within a single Scikit-learn object. For complex combinations, it can be helpful to diagram the architecture.\n",
    "\n",
    "![Pipeline Architecture](images/pipe_arch.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use feature unions to combine the predictions of multiple estimators. If you rewrite your estimators as transformers and feed them into a feature union which has, for example, a linear regressor as an estimator, you'll be able to automatically weight and combine each individual prediction into an ensemble model.\n",
    "\n",
    "To do this, you'll need to write custom transformers where the `.transform` method carries out the `.predict` implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What types of data can be fed in as the `X` and `y` arguments to transformers and estimators?  There are two considerations:\n",
    "1. If you are writing your own transformers and estimators, you can make them take whatever file data format you like, as long as output and input types agree for sequential elements in a chain.  Scikit-learn will happily pass these objects through pipelines, feature union, grid searches, etc.\n",
    "2. If you want to interact with transforms and estimators provided by Scikit-learn, you need to provide data in a compatible type.  Scikit-learn, like much of the Python ecosystem, uses duck typing.  This means that instead of testing that the input is a particular type, it tests that the input has particular capabilities.  For most elements, Scikit-learn will require that the inputs behave like NumPy arrays.  If you're not sure if a particular input will work, just try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Your Implementations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've implemented a custom estimator/transformer, you can validate that it correctly works using `sklearn.utils.estimator_checks.check_estimator` [docs](http://scikit-learn.org/stable/modules/generated/sklearn.utils.estimator_checks.check_estimator.html). Here's an example, validating our above implementations.\n",
    "\n",
    "**Warning**: This is a bit of code inside of sklearn that's still under active development and discussion. It should help you get the basics down for implementing sklearn's interface, but you might run into some weird issues, or this validation might be \"too much\" for the pipeline you're trying to build or even not general enough for something that might be compatible. You might actually have something that's compatible with `GridSearchCV` for example, and would still fail validation here (e.g. input validation below).\n",
    "\n",
    "For example, let's try validating the transformer we wrote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import estimator_checks as ec\n",
    "try:\n",
    "    ec.check_estimator(CenteringTransformer)\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we still have an error around input validation. This exception is expected... \n",
    "http://scikit-learn.org/stable/developers/contributing.html#input-validation\n",
    "And is even something worth worrying about during development. Here's the validation Scikit-learn is looking for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_array\n",
    "\n",
    "def validate_args(arg):\n",
    "    check_array(arg)\n",
    "    return np.asarray(arg)\n",
    "\n",
    "class ValidatingCenteringTransformer(sk.base.BaseEstimator, sk.base.TransformerMixin):\n",
    "    \"\"\"\n",
    "    Centers the features about 0\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        v_X = validate_args(X) \n",
    "        v_y = np.asarray(y) if y is not None else None\n",
    "        self._means = v_X.mean(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        v_X = validate_args(X)\n",
    "        return v_X - self._means\n",
    "\n",
    "\n",
    "ec.check_estimator(ValidatingCenteringTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this time, we pass!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
