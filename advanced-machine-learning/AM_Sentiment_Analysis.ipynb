{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "The objective of sentiment analysis is to quantify the attitudes expressed in a body of text.  In the simplest case, we want to determine if the sentiment of text is positive or negative.  This is a text classification problem, so the methods that we discussed before all apply.  However, there are some nuances that we should keep in mind:\n",
    "\n",
    "* **Number of Classes:** We'll stick to binary classification in this notebook for simplicity, but it's common to add a third class for neutral statements that don't express positive or negative sentiment.  More ambitious classifiers might include additional classes to encode intensity of sentiment or target specific emotions.   \n",
    "* **Context Dependence:** Use of language varies significantly based on context.  For example, the word *cheesy* is more likely to be negative in the context of a movie review, and more likely to be positive if the topic is pizza.  So it's best to use training data that closely resembles your intended use case.   \n",
    "* **Imperfect Classification:** Sentiment is subjective, so even human coders will not label sentences with 100% consistency.  In other words, this is a hard problem, and we should expect that any classifier we train will make mistakes some of the time.  We can identify trends in sentiment across large data sets, but we cannot reliably classify the sentiment of individual statements.            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Model\n",
    "\n",
    "In this notebook we'll work with a movie review data set consisting of 1000 positive reviews and 1000 negative reviews.  We'll start by downloading the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                            # nltk is another nlp library\n",
    "nltk.download(\"movie_reviews\")         # try nltk.download() with no arguments to see additional options\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = movie_reviews.fileids()        \n",
    "files[:3]+files[-3:]  # Example filenames                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movie_reviews.raw(files[-2])[:400]+\"...\")  # Example of a positive review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data for machine learning, we shuffle the positive and negative reviews together, perform a train-test split, and extrapolate training labels from the file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(200) # For consistency \n",
    "files = movie_reviews.fileids() # For consistency if the cell is run again \n",
    "\n",
    "from random import shuffle\n",
    "shuffle(files)  # Randomizes the order of the files, changing the list in place\n",
    "\n",
    "train = files[:1600] \n",
    "test = files[1600:]\n",
    "\n",
    "X_train = [movie_reviews.raw(movie) for movie in train]\n",
    "X_test = [movie_reviews.raw(movie) for movie in test]\n",
    "\n",
    "def label(movie):\n",
    "    return \"Positive\" if movie[:3]==\"pos\" else \"Negative\"\n",
    "\n",
    "y_train = [label(movie) for movie in train]\n",
    "y_test = [label(movie) for movie in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a simple and effective pipeline just by combining a vectorizer with a Naive Bayes classifier. More details on this type of classifier can be found in the Naive Bayes notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "est = Pipeline([('vectorizer', TfidfVectorizer()),\n",
    "                ('classifier', MultinomialNB())])\n",
    "\n",
    "est.fit(X_train,y_train)\n",
    "est.score(X_test,y_test) # Returns the accuracy of the model on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better model, let's use some of the tools we discussed before in the __[Natural Language Processing notebook](AM_Natural_Language_Processing.ipynb)__.  We'll include bigrams, exclude stopwords, and use lemmatization.  Note that we've disabled a number of steps in the `spaCy` language processing pipeline.  By default, `spaCy` does a substantial amount of parsing, part-of-speech tagging, and other processing that is not necessary if all we want to do is get word lemmas.  Disabling these components of the `spaCy` pipeline results in substantially faster tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('en', disable=['parser','tagger','ner','textcat'])\n",
    "\n",
    "def tokenize_lemma(text):\n",
    "    return [w.lemma_ for w in nlp(text)]\n",
    "\n",
    "stop_words_lemma = set(w.lemma_ for w in nlp(' '.join(STOP_WORDS)))\n",
    "\n",
    "est = Pipeline([('vectorizer', TfidfVectorizer(ngram_range=(1,2),\n",
    "                                               stop_words=stop_words_lemma,\n",
    "                                               tokenizer=tokenize_lemma)),\n",
    "                ('classifier', MultinomialNB())])\n",
    "\n",
    "est.fit(X_train,y_train) # This may take 2 or 3 minutes to run \n",
    "est.score(X_test,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a more sophisticated machine learning algorithm in place of Naive Bayes.  Two good candidates are Logistic Regression and Linear SVM.\n",
    "\n",
    "However, these models may train slowly on NLP data sets because of the large number of features.  One solution is to train them using stochastic gradient descent (which only considers one sample at each training step) instead of gradient descent (which considers the entire sample at each step).  Sklearn's `SGDClassifier` can be used to implement a variety of linear models using stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_pipe = Pipeline([('vectorizer', TfidfVectorizer(ngram_range=(1,2),\n",
    "                                               stop_words=STOP_WORDS)), # lemmatization removed to improve runtime\n",
    "                     ('classifier', SGDClassifier(max_iter=5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may not be obvious, but we've implicitly made some choices in defining the pipeline above:\n",
    "\n",
    "* `SGDClassifier` implements Linear SVM by default, but we want to try Logistic Regression as well.\n",
    "* `SGDClassifier` tries to combat overfitting using regularization, a penalty which increases with model complexity.  The strength of this penalty is controlled by a hyperparameter, alpha.  alpha defaults to 0.0001, but this isn't necessarily the optimal value.\n",
    "\n",
    "Instead of guessing at the best choices, we can systematically search over a grid of hyperparameter values and select the model with the best cross-validation score.  We can use the `GridSearchCV` function in Scikit-Learn to do this.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV          \n",
    "\n",
    "parameters = {'vectorizer__ngram_range': [(1, 1), (1, 2)], \n",
    "              'classifier__alpha': (0.001, 0.0001, 0.00001),\n",
    "              'classifier__loss': ('log', 'hinge'), # log = Logistic Regression, hinge = Linear SVM\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(sgd_pipe, parameters, cv=5)\n",
    "grid.fit(X_train,y_train) # This may take 2 or 3 minutes to run \n",
    "\n",
    "est = grid.best_estimator_  #  Let's have a closer look at the best parameters...\n",
    "est.score(X_test,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the best parameters that `GridSearchCV` found using the `named_steps` attribute, which allows us to\n",
    "see those parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.named_steps['classifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Model\n",
    "\n",
    "The models that we trained in this notebook have a high degree of interpretability.  In contrast to other machine learning models, which are effectively black boxes, we can say a lot about how predictions are being made:\n",
    "\n",
    "* We can infer feature importance from the coefficients of the model.  In other words, we can identify the words that most strongly signal positive and negative sentiment.\n",
    "* We can look at the probability values associated with model predictions to judge the model's confidence in those predictions. \n",
    "\n",
    "For example, let's look at the more basic Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Pipeline([('vectorizer', TfidfVectorizer(min_df=50)), # min_df=50 because we're not interested in uncommon words\n",
    "                ('classifier', MultinomialNB())])\n",
    "est.fit(X_train,y_train)\n",
    "\n",
    "vocab = est.get_params()['vectorizer'].vocabulary_ # A dictionary of (word, value) pairs\n",
    "                                                   # vocab['word'] is the index in the coefficient array corresponding to 'word'\n",
    "    \n",
    "coeff_pos = est.get_params()['classifier'].feature_log_prob_[1] \n",
    "coeff_neg = est.get_params()['classifier'].feature_log_prob_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients from the model tell us how likely each word is to occur in reviews of each class:\n",
    "\n",
    "$$\\mbox{coeff_pos[vocab['word']]} = \\log \\bigl( P(\\mbox{ Review contains word }|\\mbox{ Review is positive })\\bigr)$$\n",
    "\n",
    "$$\\mbox{coeff_neg[vocab['word']]} = \\log \\bigl( P(\\mbox{ Review contains word }|\\mbox{ Review is negative })\\bigr)$$\n",
    "\n",
    "One way to measure the \"polarity\" of a word is just to look at the difference between these two coefficients.  Let's identify the most positively and negatively charged words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argsort\n",
    "\n",
    "polarity = coeff_pos - coeff_neg\n",
    "indices = argsort(polarity) # indices of the polarity list, sorted from least to greatest\n",
    "\n",
    "print(\"Positive Words \\n-----\")\n",
    "for word in vocab:\n",
    "    if vocab[word] in indices[-10:]:\n",
    "        print(word)\n",
    "        \n",
    "print(\"\\nNegative Words \\n-----\")\n",
    "for word in vocab:\n",
    "    if vocab[word] in indices[:10]:\n",
    "        print(word)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple way to visualize the sentiment of text is to color polarized words.  Below we'll write a function that both colorizes the text of reviews from the test set and summarizes the output from our model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 250\n",
    "\n",
    "best_N = indices[-cutoff:] # Indices of the best/worst 250 words\n",
    "worst_N = indices[:cutoff]\n",
    "\n",
    "best_N = [word for (word,index) in vocab.items() if index in best_N] # Lists of the words themselves\n",
    "worst_N = [word for (word,index) in vocab.items() if index in worst_N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore, Style\n",
    "import re\n",
    "\n",
    "def colorize(text): # Colors the most polarized words             \n",
    "    for word in best_N:\n",
    "        text = re.sub(r\"\\b\"+word+r\"\\b\",Fore.GREEN+Style.BRIGHT+word+Fore.RESET+Style.RESET_ALL,text)  \n",
    "    for word in worst_N:\n",
    "        text = re.sub(r\"\\b\"+word+r\"\\b\",Fore.RED+Style.BRIGHT+word+Fore.RESET+Style.RESET_ALL,text)\n",
    "    return text\n",
    "\n",
    "def summary(n, short=True): # Summarizes model performance for the specified review in the test set\n",
    "    text = X_test[n]\n",
    "    if short:\n",
    "        text = text[:500]+\"...\"\n",
    "    print(\"Review\", \"#\"+str(n))\n",
    "    print(\"Actual Class:\", y_test[n])\n",
    "    print(\"Model Prediction:\", est.predict([text])[0])\n",
    "    print(\"Confidence:\", max(est.predict_proba([text])[0][0], est.predict_proba([text])[0][1])) # Probability\n",
    "    print(\"\\n\" + colorize(text) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(14)  # Some examples\n",
    "summary(143)\n",
    "summary(192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar and Other Tools\n",
    "\n",
    "The bag of words approach has its limitations.  It's not hard to imagine sentences that will confuse our models.  For example:\n",
    "\n",
    "> \"I was expecting an uninspired remake, but this movie wasn't bad at all.  There was no lame dialogue, and the plot twist wasn't terrible, so I'd actually say that the original was worse.\"\n",
    "\n",
    "In order to parse sentences like these, we need some notion of grammar.  A simple grammar is something we can write by hand.  It's just a set of rules that say how the parts of a sentence can fit together.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "... S -> NP VP\n",
    "... NP -> DT NN | DT JJ NN\n",
    "... VP -> VBD PP\n",
    "... PP -> IN NP\n",
    "... DT -> 'The' | 'the'\n",
    "... NN -> 'fox' | 'dog'\n",
    "... VBD -> 'jumped'\n",
    "... IN -> 'over'\n",
    "... JJ -> 'lazy'\n",
    "... NN -> 'dog'\n",
    "... \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = nltk.ChartParser(simple_grammar)\n",
    "sentence = \"The fox jumped over the lazy dog\"\n",
    "trees = parser.parse(sentence.split(\" \")) # The input is an ordered list of words and the output is a generator\n",
    "                                          # Depending on the grammar, there may be different ways to parse the same sentence. \n",
    "for tree in trees:\n",
    "    tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and using grammars for practical text analysis is more challenging:\n",
    "\n",
    "* Parsing will fail whenever it encounters words or structures not included in the grammar, so we're obligated to include a large number of each.\n",
    "* We're likely to get ambiguous results (many alternative parse trees) when using grammars with many rules, especially when we try to parse long sentences.  \n",
    "* Ambiguity can be reduced using probabilistic parsing (based on a weighted grammar), but this further increases the complexity of the task. \n",
    "\n",
    "The simplest approach is to avoid these issues by using tools that already exist.  One such tool is Stanford's [CoreNLP](https://stanfordnlp.github.io/CoreNLP/).  CoreNLP is written in Java, so it requires a bit of setup, but it has Python wrappers like [`py-corenlp`](https://github.com/smilli/py-corenlp), and its parsing capabilities are more comprehensive than pure Python alternatives.  CoreNLP also includes a sentiment analysis tool that uses [deep learning](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf).\n",
    "\n",
    "Another option is `spaCy`, which has built in parsing capabilities.  However, `spaCy` only keeps track of dependencies between words, so we won't get all of the higher level information that we had before.  Since structures like NP and VP aren't included, we get a shallower tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Tree\n",
    "\n",
    "nlp = spacy.load('en', disable=['tagger','ner','textcat'])\n",
    "def to_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(\"-\".join([node.text, node.tag_]), [to_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return \"-\".join([node.text, node.tag_])\n",
    "\n",
    "sentence = next(nlp(u\"The fox jumped over the lazy dog\").sents)    \n",
    "    \n",
    "to_tree(sentence.root).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Note\n",
    "\n",
    "While the appeal of more sophisticated methods is obvious, we should not feel obligated to use them in every circumstance.  At the sentence level, even state-of-the-art methods have misclassification rates over 10%, so in many circumstances they don't offer a practical advantage over bag of words.  We should use grammars and parsing only if the complexity of the problem makes it necessary.  For the example, in this notebook it was sufficient to identify polarized words in a general way because the topics of the reviews are known.  However, if we were trying to do topic identification and sentiment analysis simultaneously, then we would need a way to link polarized words to their targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
