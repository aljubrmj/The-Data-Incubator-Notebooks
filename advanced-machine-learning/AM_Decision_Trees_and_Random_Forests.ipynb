{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a binary tree.  At each of the internal nodes, it chooses a feature $i$ and a threshold $t$.  Each leaf has a value.  Evaluation of the model is just traversal of the tree from the root.  At each node, we go down the left branch if $X_{ji} \\le t$ and the right branch otherwise.  The value of the model $f(X_{ji})$ is the value at the terminating leaf of this traversal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we show a picture of this on small decision tree trained on the iris data set.  Notice that each internal node has a decision criterion and each leaf has the breakdown of label classes left at this leaf of the tree.  For a geometric picture of a decision tree, take a look at this [blog post](https://shapeofdata.wordpress.com/2013/07/02/decision-trees/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and draw out a decision tree\n",
    "\n",
    "from IPython import display\n",
    "from sklearn import datasets, tree, utils\n",
    "from sklearn.externals.six import StringIO  \n",
    "import graphviz \n",
    "\n",
    "# target (y) has 3 classes, 50 observations each\n",
    "dataset = datasets.load_iris()\n",
    "# print(dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a small decision tree on the iris data set\n",
    "X_iris, y_iris = utils.shuffle(dataset.data, dataset.target, random_state=42)\n",
    "tree_clf = tree.DecisionTreeClassifier(max_depth=3).fit(X_iris, y_iris)\n",
    "\n",
    "# Generate a plot of the decision tree\n",
    "graphviz.Source(tree.export_graphviz(tree_clf, out_file=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to find a tree where with \"pure leaves.\" In other words, we want observations of a single class to end up in a given leaf. We optimize machine learning models by minimizing some error function with respect to our model parameters. For classification trees, we can formalize the purity criterion by using the **Gini index** or **Gini impurity**, defined as follows: \n",
    "\n",
    "$$ G_m = \\sum_k p_{mk} (1 - p_{mk})\\, , $$\n",
    "\n",
    "where $p_{mk}$ is the fraction of training observations in region $m$ from class $k$. As an example consider data consisting of 2 classes of 50 observations each. Now consider two classifiers -- the first is a decision tree that perfectly separates your observations by class into two leaves while the second places 25 observations of each class into two leaves. How do the Gini indices of (one leaf in) these two trees compare?\n",
    "\n",
    "1. Case 1: value = [50, 0]\n",
    "$$ G = \\frac{50}{50} \\left(1 - \\frac{50}{50}\\right) + \\frac{0}{50} \\left(1 - \\frac{0}{50}\\right) = 0 $$\n",
    "\n",
    "1. Case 2: value = [25, 25]\n",
    "$$ G = \\frac{25}{50} \\left(1 - \\frac{25}{50}\\right) + \\frac{25}{50} \\left(1 - \\frac{25}{50}\\right) = 0.5 $$\n",
    "\n",
    "The Gini index is quite similar to the entropy\n",
    "\n",
    "$$ H_m = - \\sum_k p_{mk} \\log_2 p_{mk} $$\n",
    "\n",
    "1. Case 1: value = [50, 0]\n",
    "$$ H = -\\left[\\frac{50}{50} \\log_2 \\left(\\frac{50}{50}\\right) + \\frac{0}{50} \\log_2 \\left(\\frac{0}{50}\\right)\\right] = 0 $$\n",
    "\n",
    "1. Case 2: value = [25, 25]\n",
    "$$ H = -\\left[\\frac{25}{50} \\log_2 \\left(\\frac{25}{50}\\right) + \\frac{25}{50} \\log_2 \\left(\\frac{25}{50}\\right)\\right] = 1 $$\n",
    "\n",
    "Both the Gini index and the entropy are minimized by *pure* nodes&mdash;those that contain only a single class. Sometimes the __Information Gain__ is used instead. Information gain is the difference in entropy of the unsplit data set to the entropy of the data after the split. \n",
    "\n",
    "$$ IG = H - \\sum_m \\rho_m H_m $$\n",
    "\n",
    "where $\\rho_m$ is the fraction of elements in region $m$ compared to the whole data set. If we use information gain as a metric, we wish to maximize it.\n",
    "\n",
    "In practice, we are searching for the optimal splitting thresholds that minimize these costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit the Boston housing data set and model home prices with a decision tree regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a basic decision tree.  Can you do better?\n",
    "from sklearn import model_selection\n",
    "import pandas as pd\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "# print(boston['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a small decision tree on the iris data set\n",
    "X_bos, y_bos = utils.shuffle(boston.data, boston.target, random_state=42)\n",
    "tree_reg = tree.DecisionTreeRegressor(max_depth=3).fit(X_bos, y_bos)\n",
    "\n",
    "# Generate a plot of the decision tree\n",
    "graphviz.Source(tree.export_graphviz(tree_reg, out_file=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression trees typically minimize the mean squared error. Notice in the figure above that the \"values\" associated with each node are no longer vectors containing information on number of observations associated with each class. Instead the values are scalars. The MSE of each node is computed by comparing the values of the observations (or samples) in that node to the value of the node itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Algorithm and Tree Building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we optimize these values to split on? There are a bunch of algorithms to [choose from](http://www.math.le.ac.uk/people/ag153/homepage/SofeikovTyuMirGorProchIJCNN2014.pdf), but we will consider the simple [Iterative Dichotomiser 3 (ID3)](https://en.wikipedia.org/wiki/ID3_algorithm) for classification. We will iterate through each feature and recursively find the split which maximizes our information gain (or minimizes some cost) for each subset of data. The procedure is as follows:\n",
    "\n",
    "1. For a set of values $x_j$ (for a given feature $j$), generate a set of evenly spaced threshold values {$t_{j1},...,t_{jK}$} that span the range of $x_j$. \n",
    "1. For each threshold value $t_{jk}$, compute the information gain associated with a split at this value.\n",
    "1. Find the threshold that results in the maximum information gain, $t_{jM}$\n",
    "1. Repeat the previous two steps for every unused attribute and select the attribute (and associated threshold) with the largest information gain. \n",
    "1. Split the data into $x_j < t_{jM}$ and $x_j \\ge t_{jM}$. \n",
    "1. Remove $t_{jM}$ from {$t_{j1},...,t_{jK}$}.\n",
    "1. Repeat the process on each subset until some condition is met. \n",
    "\n",
    "Stopping conditions include the following:\n",
    "1. The observations are grouped into pure subsets. \n",
    "1. There are no more attributes left.\n",
    "1. A condition set by the hyperparameters (below) is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of decision trees can be controlled by many hyperparameters (see Sklearn's [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for details). The decision tree classifier and regressor share all of the same hyperparameters except for one -- there is a \"class_weight\" option for the classifier in the event you have unbalanced classes. (See the Unbalanced Classes notebook for information.) We have listed four major ones with their descriptions below:\n",
    "\n",
    "<table>\n",
    "\t<tr>\n",
    "    <th>Feature</th>\n",
    "    <th>Value</th>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`max_features`</td>\n",
    "    <td>The number of features to consider when choosing a split for an internal node</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`max_depth`</td>\n",
    "    <td>The maximum depth of tree from the root</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`min_samples_split`</td>\n",
    "    <td>Minimum number of samples required for a split to be considered</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`min_samples_leaf`</td>\n",
    "    <td>Minimum number of samples required for each leaf</td>\n",
    "\t</tr>\n",
    "</table>\n",
    "\n",
    "Play around with different values for different hyperparameters in the cell below, to get a feel for what they do.\n",
    "\n",
    "**Question**: How do each of these features affect the Variance Bias Decomposition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = tree.DecisionTreeClassifier(min_samples_leaf=30).fit(X_iris, y_iris)\n",
    "\n",
    "# Generate a plot of the decision tree\n",
    "graphviz.Source(tree.export_graphviz(tree_clf, out_file=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is just an ensemble of decision trees.  The predicted value is just the average of the trees (for both regression and classification problems -- for classification problems, it is the probabilities that are averaged).  You can adjust `n_estimators` to change the number of trees in the forest.  Random forests demonstrate the idea of bootstrap aggregation, or **bagging**. Each individual model has an equal vote in the ensemble, and you have the freedom to skew the weak learners towards higher variance, knowing that the averaging will (ideally) wash out the randomness and prevent overfitting.\n",
    "\n",
    "If each tree is trained on the same subset of data, why aren't they identical?  Two reasons:\n",
    "1. **Subsampling**: each tree is actually trained on a random selected (with replacement) subset (i.e. bootstrap)\n",
    "1. **Maximum Features**: the optimal split comes from a randomly selected subset of the features.  In Scikit-learn, this feature is controlled by `max_features`.\n",
    "\n",
    "If you take a look at the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), you'll notice that while random forests share many hyperparameters in common with decision trees, random forests have have a few more:\n",
    "\n",
    "<table>\n",
    "\t<tr>\n",
    "    <th>Feature</th>\n",
    "    <th>Value</th>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`n_estimators`</td>\n",
    "    <td>The number of trees in the forest.</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`bootstrap`</td>\n",
    "    <td>Whether bootstrap samples are used when building trees.</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`oob_score`</td>\n",
    "    <td>Whether to use out-of-bag samples to estimate the generalization accuracy.</td>\n",
    "\t</tr>\n",
    "\n",
    "\t<tr>\n",
    "    <td>`n_jobs`</td>\n",
    "    <td>The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.</td>\n",
    "\t</tr>\n",
    "    \n",
    "    <tr>\n",
    "    <td>`warm_start`</td>\n",
    "    <td>When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest..</td>\n",
    "\t</tr>\n",
    "</table>\n",
    "\n",
    "Notice how their construction can be parallelized by setting the parameter `n_jobs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely Random Forests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described Sklearn's [documentation](http://scikit-learn.org/stable/modules/ensemble.html) for ensemble methods:\n",
    "\n",
    ">In extremely randomized trees (see `ExtraTreesClassifier` and `ExtraTreesRegressor` classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias.\n",
    "\n",
    "**Question**: What happens to bias and variance of the individual trees in the averaging process of Random Forests and Extremely Random Forests.  How would you change your parameters to compensate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train random forests with default parameters.  Can you do better?\n",
    "from sklearn import model_selection, ensemble\n",
    "import pandas as pd\n",
    "\n",
    "cv = model_selection.ShuffleSplit(n_splits=20, test_size=0.2, random_state=42)\n",
    "def compute_error(clf, X, y):\n",
    "    return - model_selection.cross_val_score(clf, X, y, cv=cv, scoring='neg_mean_squared_error').mean()\n",
    "\n",
    "tree_reg = tree.DecisionTreeRegressor()\n",
    "extra_reg = ensemble.ExtraTreesRegressor()\n",
    "forest_reg = ensemble.RandomForestRegressor()\n",
    "\n",
    "model_performance = pd.DataFrame([\n",
    "    (\"Mean Model\", y_bos.var()),\n",
    "    (\"Decision Tree\", compute_error(tree_reg, X_bos, y_bos)),\n",
    "    (\"Random Forest\", compute_error(forest_reg, X_bos, y_bos)),\n",
    "    (\"Extra Random Forest\", compute_error(extra_reg, X_bos, y_bos)),\n",
    "], columns=[\"Model\", \"MSE\"])\n",
    "model_performance.plot(x=\"Model\", y=\"MSE\", kind=\"Bar\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting algorithms train a sequence of models and combine them to make sophisticated (low bias) predictions. The key idea is that we can iteratively reduce any loss function using a strategy that imitates gradient descent.\n",
    "\n",
    "The combined model can be written as\n",
    "\n",
    "$$ f(X) = f_m(X) = \\sum_{k=1}^m \\gamma_k h_k(x) $$\n",
    "\n",
    "where $h_k$ are **weak learners**.  For Gradient Boosting Trees, the $h_k$ are stump trees (very short trees, often with a single split). At each iteration, we want to add a new term to the sum that reduces the loss function. First, we train a weak learner $h_m$ to fit the **pseudo-residuals**\n",
    "\n",
    "$$r_m = - \\left[\\frac{\\partial L(y_{true},y_{pred})}{\\partial y_{pred}} \\right]_{y_{pred} = f_{m-1}(X)}     $$\n",
    "\n",
    "and then we do a linear search to find the coefficient $\\gamma_m$ that minimizes\n",
    "\n",
    "$$ L(y_{true},f_{m-1}(X) + \\gamma_m h_m(X)) .$$\n",
    "\n",
    "The intuition is that $r_m$ represents the direction in which we would like to modify our predictions to decrease the loss function most quickly. $h_m(X)$ represents a step that's roughly in the same direction, although there will be some error because of how simple the weak learners are. (by design, otherwise the algorithm would overfit) It's not obvious how big of a step we should take in the $h_m(X)$ direction, so we do a search to find the best coefficient. Then we set\n",
    "\n",
    "$$ f_m(x) = f_{m-1}(x) + \\gamma_m h_m(x) $$\n",
    "\n",
    "as our new predictor and iterate. \n",
    "\n",
    "We can give a more concrete description when the loss function is mean squared error. In this case, the pseudo-residuals are residuals in the usual sense (times a constant), so the training target for each new term is just the prediction error of the previous stage of the model. As you can see in the following demo, we gradually build a complicated model by iteratively compensating for the error in the previous stage.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab as plt\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# Generate test data\n",
    "rng = np.random.RandomState(42)\n",
    "N_points = 400\n",
    "X = np.sort(5 * rng.rand(N_points, 1), axis=0)\n",
    "y = np.sin(X).ravel() + .4 * (0.5 - rng.rand(N_points))\n",
    "\n",
    "# Regular array of X values for plotting\n",
    "X_reg = np.linspace(0,5,50)    \n",
    "\n",
    "# Helper code to plot data and predictions\n",
    "def make_subplot(y,y_pred,color,title,label):\n",
    "    plt.plot(X, y, 'b.')\n",
    "    plt.plot(X_reg,y_pred,c=color,label=label)\n",
    "    plt.ylim(-1.3,1.3)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "\n",
    "# Fit/plot gradient boosting model and next term\n",
    "def make_plot(n_estimators=1):    \n",
    "    est = ensemble.GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=1, max_depth=1)\n",
    "    est.fit(X,y)\n",
    "    y_pred = est.predict(X_reg.reshape(-1,1))\n",
    "    \n",
    "    residuals = y - est.predict(X.reshape(-1,1))\n",
    "    res_est = tree.DecisionTreeRegressor(max_depth=1)\n",
    "    y_res = res_est.fit(X.reshape(-1,1),residuals).predict(X_reg.reshape(-1,1))  \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(20, 8)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    make_subplot(y,y_pred,\"red\",\"Original Data\",\"Boosting Model\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    make_subplot(residuals,y_res,\"green\",\"Residuals\",\"Next Term\")\n",
    "\n",
    "interact(make_plot, n_estimators=(1,25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few basic variants that are usually applied\n",
    "\n",
    "1.  **Learning Rate**: we usually reduce the learning rate $\\alpha$ by choosing a value $0 < \\alpha < 1$ such that\n",
    "\n",
    "    $$ f_m(x) = f_{m-1}(x) + \\alpha \\gamma_m h_m(x) $$\n",
    "\n",
    "    Setting $\\alpha = 1$ would yield the model present above.  This slows down the rate at which we learn.\n",
    "1.  **Subsampling**: we can learn on a fraction of the data which is a subsample of the full data set.\n",
    "1.  **Max Features**: we can learn from only a subset of the features (i.e. trees only select a subset of the features).\n",
    "1.  **Trying out different loss functions**: We can choose different functional forms $L$.\n",
    "\n",
    "Beware of overfitting here. You can learn more about gradient boosting [here](http://scikit-learn.org/stable/modules/ensemble.html) and [here](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/).\n",
    "\n",
    "**Question**: How do these techniques affect the bias and variance of the learned models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingRegressor\n",
    "gradient_reg = ensemble.GradientBoostingRegressor()\n",
    "\n",
    "gradient_performance = pd.DataFrame([\n",
    "    (\"Gradient Boosted Regressor\", compute_error(gradient_reg, X_bos, y_bos))\n",
    "], columns=[\"Model\", \"MSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance.append(gradient_performance).plot(x=\"Model\", y=\"MSE\", kind=\"Bar\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining feature importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a linear regression, we could inspect the coefficients to tell which terms were the most important.  With decision trees and random forests, the model structure is much more opaque.  Fortunately, all of these models (once trained) have an array called `feature_importances_` which gives the importances of all the features.  How do they compute feature importance?  According to the [Scikit documentation](http://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation):\n",
    "\n",
    "> The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree are used contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features.\n",
    "\n",
    "Let's show this in action.  For Scikit, larger values are more important and the values of `feature_importances_` sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1)\n",
    "columns = [\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSAT\"]\n",
    "pd.DataFrame([\n",
    "    tree_reg.fit(X_bos, y_bos).feature_importances_,\n",
    "    forest_reg.fit(X_bos, y_bos).feature_importances_,\n",
    "    extra_reg.fit(X_bos, y_bos).feature_importances_,\n",
    "    gradient_reg.fit(X_bos, y_bos).feature_importances_\n",
    "], columns=columns, index=[\"Tree\", \"Forest\", \"Extra Random\", \"Gradient\"]).T.plot(kind=\"bar\").legend(loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the tradeoff between Decision Trees, Random Forests, and Gradient Boosting Trees, in terms of explicability, computational time, memory, and accuracy?\n",
    "2. What are some pros and cons of using decision trees (as opposed to other ML algorithms)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What attribute(s) of decision trees enable them to be chained together into gradient boosting trees?\n",
    "1. Which is more computationally intensive? Training or testing?\n",
    "1. Can a random forest model be trained online?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spoilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing `max_features` and `max_depth` and decreasing `min_samples_split` and `min_samples_leaf` tend to build more complex models (increase Variance and reduce Bias).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extremely Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance between different trees tends to cancel each other while the biases reinforce each other. That is, because the trees are different, they tend to overfit in different ways but when they underfit, they underfit the same way. So you want to use higher variance, lower bias parameters than you would with a decision tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the learning rate increases variance. Decreasing the fraction subsampled and the max features will decrease the variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decision Trees are clearly the least accurate although possibly the most explicable and they train quickly.  (Extra) Random Forests are more expensive to train but can be trained in parallel -- both across processors on the same data in one machine and across machines by splitting up the sample data.  Gradient Boosting Trees are not as naturally parallelizable and (empirically) seem to be the slowest. Also see the [choosing ML algorithms notebook](AM_Choosing_ML_Algorithms.ipynb).\n",
    "2. Pros and cons can be found [here](http://scikit-learn.org/stable/modules/tree.html#decision-trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
