{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from static_grader import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Miniproject\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The city of New York does restaurant inspections and assigns a grade. Inspections data for the last 4 years are available on s3 as an SQLite database, which you can import in the next few cells. These were extracted from a set of csv files and an XLS file, as described in the <b>How we loaded the data</b> section\n",
    "\n",
    "\n",
    "The raw data can be found [here](https://s3.amazonaws.com/dataincubator-course/coursedata/nyc_inspection_data.zip) and can be useful to look at. The file `RI_Webextract_BigApps_Latest.xls` contains a description of each of the data files and what the columns mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://dataincubator-course/coursedata/ . --exclude '*' --include 'nyc_inspection.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will load the pre-existing tables\n",
    "%load_ext sql\n",
    "%sql sqlite:///nyc_inspection.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what tables are in the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM sqlite_master WHERE \"type\"='table';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to look at the format of an individual table (note that you may need to change types to get the answers in the right form):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "PRAGMA table_info(webextract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQLite3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project should be written in SQL. Between SQLite and PostgreSQL we recommend SQLite for this project.  You can use the SQLite command prompt by running this command in bash\n",
    "```bash\n",
    "sqlite3 cmd \"DROP TABLE IF EXISTS writer;\\\n",
    "CREATE TABLE IF NOT EXISTS writer (first_name, last_name, year);\\\n",
    "INSERT INTO writer VALUES ('William', 'Shakespeare', 1616);\\\n",
    "INSERT INTO writer VALUES ('Francis', 'Fitzgerald', 1896);\\\n",
    "\\\n",
    "SELECT * FROM writer;\\\n",
    "\"\n",
    "```\n",
    "Alternatively, you can run bash commands in a Jupyter notebook by prepending the `!` in a code cell (notice that we conveniently get the output displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sqlite3 cmd \"\"\"\\\n",
    "DROP TABLE IF EXISTS writer;\\\n",
    "CREATE TABLE IF NOT EXISTS writer (first_name, last_name, year);\\\n",
    "INSERT INTO writer VALUES ('William', 'Shakespeare', 1616);\\\n",
    "INSERT INTO writer VALUES ('Francis', 'Fitzgerald', 1896);\\\n",
    "\\\n",
    "SELECT * FROM writer;\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the [`ipython-sql` extension](https://github.com/catherinedevlin/ipython-sql#ipython-sql) by first loading this extension and then running our code with the \"magic\" command in the first line\n",
    "```python\n",
    "%%sql sqlite://\n",
    "```\n",
    "Notice that the output table is formatted nicely as a nice HTML table.\n",
    "\n",
    "This is our recommended technique.  However, the grader is expecting python objects and you may need to use list comprehensions to reformat this output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS writer;\n",
    "CREATE TABLE IF NOT EXISTS writer (first_name, last_name, year);\n",
    "INSERT INTO writer VALUES ('William', 'Shakespeare', 1616);\n",
    "INSERT INTO writer VALUES ('Francis', 'Fitzgerald', 1896);\n",
    "\n",
    "SELECT * FROM writer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = _\n",
    "#This captures the output of the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we loaded the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future reference, here is how you can load data in to SQL (with examples).  If you have a csv file you created with something like\n",
    "\n",
    "```\n",
    "!printf \"Name,Age\\nAlice,3\\nBob,10\" > sample.csv.nogit\n",
    "```\n",
    "\n",
    "\n",
    "Then SQLite has a convenient [`.import` function](https://sqlite.org/cli.html#csv_import) which can create tables from `.csv` files.\n",
    "\n",
    "```bash\n",
    "sqlite> .import sample.csv.nogit sample\n",
    "sqlite> SELECT * FROM sample;\n",
    "```\n",
    "\n",
    "The files may contain badly formatted text.  Unfortunately, this is all too common.  As a stop gap, remember that [`iconv`](https://linux.die.net/man/1/iconv) is a Unix utility that can convert files between different text encodings.\n",
    "\n",
    "Alternatively, you can also read csv files using pandas and convert that into SQL via some SQL magic (this is what we actually did).\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "sample = pd.read_csv('sample.csv.nogit')\n",
    "%sql DROP TABLE IF EXISTS sample\n",
    "%sql PERSIST sample\n",
    "%sql SELECT * FROM sample;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Null entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the number of inspections (`CAMIS`, `INSPDATE` pairs) that do not have a score - i.e. where none of the rows with those (`CAMIS`, `INSPDATE`) values has a score. Remove the corresponding rows from the data set for the rest of the questions in the assignment.\n",
    "\n",
    "**Question:** How else might we have handled this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_entries = 0\n",
    "\n",
    "grader.score('sql__null_entries', null_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Score by zip code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a list of tuples of the form:\n",
    "\n",
    "    (zip code, mean score, number of restaurants)\n",
    "\n",
    "for each of the 87 zip codes in the city with over 100 restaurants. Use the score from the latest inspection date for each restaurant. Sort the list in ascending order by mean score.\n",
    "\n",
    "**Note:** There is an interesting discussion here about what the mean score *means* in this data set. Think about what we're actually calculating - does it represent what we're trying to understand about these zip codes?\n",
    "\n",
    "What if we use the average of a restaurant's inspections instead of the latest?\n",
    "\n",
    "**Checkpoints:**\n",
    "- Total unique restaurants: 24,361;\n",
    "- Total restaurants in valid zip codes: 19,172\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_zipcode = [(\"11201\", 9.81739130434783, 345)] * 87\n",
    "\n",
    "grader.score('sql__score_by_zipcode', score_by_zipcode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Mapping scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are not terribly enlightening.  Use [CartoDB](http://cartodb.com/) to produce a map of average scores by zip code.  You can sign up for a free trial.\n",
    "\n",
    "You will have to use their wizard to plot the data by [zip code](https://carto.com/learn/guides/analysis/georeference). You will need to specify \"USA\" in the country field.  Then use the \"share\" button to return a link of the form [https://x.cartodb.com/](https://x.cartodb.com/).\n",
    "\n",
    "**For fun:** How do JFK, Brighton Beach, Liberty Island (home of the Statue of Liberty), Financial District, Chinatown, and Coney Island fare?\n",
    "\n",
    "**For more fun:** Plot restaurants as pins on the map, allowing the user to filter by \"low\", \"middling\", or \"high\"-scoring restaurants. You can use a CASE WHEN statement to create the different groups based on score thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must be url of the form https://x.cartodb.com/...\n",
    "score_by_map = \"https://cartodb.com\"\n",
    "\n",
    "grader.score('sql__score_by_map', score_by_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Score by borough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a list of tuples of the form:\n",
    "\n",
    "    (borough, mean score, number of restaurants)\n",
    "\n",
    "for each of the city's five boroughs. Use the latest score for each restaurant. Sort the list in ascending order by grade.\n",
    "\n",
    "**Hint:** You will have to perform a join with the `boroughs` table. The borough names should be reported in ALL CAPS.\n",
    "\n",
    "**Checkpoint:**\n",
    "- Total restaurants in valid boroughs: 24,350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_borough = [(\"MANHATTAN\", 10.7269875502402, 10201)] * 5\n",
    "\n",
    "grader.score('sql__score_by_borough', score_by_borough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Violations by cuisine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look at violations themselves now.  We'll need to think more carefully about what we're measuring, since most restaurants have many inspections with possibly multiple violations per inspection, or long stretches of inspections with no violations.\n",
    "\n",
    "There are many ways to deal with this normalization issue, but we'll go with a fairly straightforward one: dividing the number of violations by the length of time (in years) the restaurant has been open.  As a proxy for the length, we'll look at the difference between the oldest and newest inspection date, treating anything less than 30 days as 30 days (to account for those that were only inspected once, we'll assume everything was open for at least a month).\n",
    "\n",
    "Since there are so many restaurants, we'll group them by cuisine and do a weighted average by computing \n",
    "\n",
    "    (total violations for a cuisine) / (total restaurant-years for that cuisine)\n",
    "\n",
    "Return a list of 75 tuples of the form\n",
    "\n",
    "    (cuisine name, reports per restaurant-year)\n",
    "    \n",
    "for cuisines with at least 100 violations total, ordered by increasing number of reports per restaurant-year\n",
    "    \n",
    "**Note:** This isn't the only way to normalize things.  How would other ways affect the computation?  If you similarly wanted to compute an average score by cuisine, how might you go about doing that?\n",
    "    \n",
    "**Checkpoint:**\n",
    "- Total entries from valid cuisines: 522,410"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_cuisine = [(\"French\", 20.3550686378036)] * 75\n",
    "\n",
    "grader.score('sql__score_by_cuisine', score_by_cuisine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Specific violations by cuisine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which cuisines tend to have a disproportionate number of what which violations? Answering this question isn't easy because you have to think carefully about normalizations.\n",
    "\n",
    "1. More popular cuisine categories will tend to have more violations just because they represent more restaurants.\n",
    "2. Similarly, some violations are more common.  For example, knowing that \"Equipment not easily movable or sealed to floor\" is a common violation for Chinese restaurants is not particularly helpful when it is a common violation for all restaurants.\n",
    "\n",
    "The right quantity is to look at is the conditional probability of a specific type of violation given a specific cuisine type and divide it by the unconditional probability of the violation for the entire population. Taking this ratio gives the right answer.  Return the 20 highest ratios of the form:\n",
    "\n",
    "    ((cuisine, violation), ratio, count)\n",
    "\n",
    "**Hint:**\n",
    "1. You might want to check out this [Stack Overflow post](http://stackoverflow.com/questions/972877/calculate-frequency-using-sql).\n",
    "2. The definition of a violation changes with time.  For example, 10A can mean two different things \"Toilet facility not maintained ...\" or \"Vermin or other live animal present ...\" when things were prior to 2003. To deal with this, you should limit your analysis to violation codes with end date after Jan 1, 2014. (This end date refers to the validity time ranges in `Violation.txt`).\n",
    "3. The ratios don't mean much when the number of violations of a given type and for a specific category are not large (why not?).  Be sure to filter these out.  We chose 100 as our cutoff.\n",
    "\n",
    "**Checkpoint:**\n",
    "- Top 20 ratios mean: 2.360652529900757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation_by_cuisine = [\n",
    "    ((\"Caf√©/Coffee/Tea\",\n",
    "      \"Toilet facility not maintained and provided with toilet paper; \"\n",
    "      \"waste receptacle and self-closing door.\"),\n",
    "     1.87684775827172, 315)] * 20\n",
    "\n",
    "grader.score('sql__violation_by_cuisine', violation_by_cuisine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
