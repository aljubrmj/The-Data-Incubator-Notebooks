{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Distributed Computing\n",
    "<!-- requirement: small_data/gutenberg -->\n",
    "<!-- requirement: scripts/mapper.py -->\n",
    "<!-- requirement: scripts/reducer.py -->\n",
    "<!-- requirement: images/MapReduce_Work_Structure.png -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big data consists of large, growing files on the order of terabytes ($10^{12}$) and petabytes ($10^{15}$). Big data includes (among other things) transactional, sensor, and social data, which are typically unstructured and can exist in a variety of formats like text, audio, and visual files.  Such data sets present challenges in capturing, storing, searching, visualizing, and querying information. Distributed computing is often the solution to these big data sets.\n",
    "\n",
    "Such large data sets require a qualitative change in how the data is analyzed.  When your data fit in memory, you don't have to worry (much) about the order of operations.  It will take ~100ns to access any random memory location.\n",
    "\n",
    "When your data do not fit in memory, you will have to hit the disk to load the portion you need at any time.  Random access from the disk will take from ~100Î¼s (for SSDs) to ~10ms (for spinning platters).  This is a factor of 1,000 to 100,000 slower than memory!\n",
    "\n",
    "However, these numbers are for random access.  Any consumer hard drive can easily give a sequential read speed of 200MB/s.  This corresponds to a byte every 5ns, twenty times faster than reading random bytes from memory.  In order to be performant, algorithms for large data sets must be able to consider only a subset of data (ideally sequential) at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once an algorithm has been written to work on only a subset of data at a time, it is relatively easily to divide the data amongst several machines and run the task in parallel.  This is the basic idea behind **distributed computing**.  Several computers will each compute part of an answer by themselves, and then those pieces will be combined into a final result.  Within a data center, network latencies (~1ms) are roughly equivalent to hard drive latencies, so distributing data to different nodes isn't too much worse than reading it off of a local hard drive.\n",
    "\n",
    "Not all tasks can be easily completed in a distributed computing framework. Tasks need to be divisible such that many operations can take place independent from each other.  Problems that are obviously well-suited to this framework are called **embarrassingly parallel**, as they can be parallelized to many machines without reworking the algorithm.  Examples include 3D video rendering (each frame is independent) and growing random forests (each tree is independent).  However, algorithms that require many steps which each depend on the result of the previous do not parallelize as easily.  For instance, each step of a gradient descent algorithm needs to know the parameters from the previous step, so it is not embarrassingly parallel.  (It can be, and often is, parallelized, but more work must be done to harmonize the gradient calculations from the disparate calculations.)\n",
    "\n",
    "While it is \"relatively easy\" to make an embarrassingly parallel algorithm work in a distributed manner, there is a lot of bookkeeping to be done in keeping track of data, distributing jobs, collecting results, and dealing with failures.  Fortunately, tools like Spark provide all of this functionality, letting us worry about the algorithm itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce: A simple distributed-computing framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping state synchronized across multiple nodes is difficult.  Many distributed systems adopt a **functional programming** approach, with\n",
    "- Pure functions (with no side effects)\n",
    "- Immutable data\n",
    "\n",
    "In December, 2004, Jeffery Dean and Sanjay Ghemawat of Google introduced [MapReduce](https://research.google.com/archive/mapreduce.html), a scheme for distributed computing based on two higher-order functions: **map** and **reduce**.  Both exist in Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Map**: Map operations are trivially parallelizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "#The list command is to make a readable output\n",
    "#in python3 map and filter create generators\n",
    "list(map(square, range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduce**: Reduce operations should be commutative [$a \\star b = b \\star a$] and associative [$(a \\star b) \\star c = a \\star (b \\star c)$], so they can be evaluated in arbitrary order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "reduce(add, map(square, range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are often specified with `lambda`, but that is only an issue of syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce(lambda x, y: x + y, map(lambda x: x * x, range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Find the sum of squares of 1 million numbers\n",
    "\n",
    "On 1 machine:\n",
    "1. $10^6$ steps to square each number\n",
    "1. $10^6$ steps to add all the numbers\n",
    "1. $\\mathcal{O}(n)$\n",
    "\n",
    "On 1 million machines:\n",
    "1. Map: 1 step to square all of the numbers\n",
    "1. Reduce: $\\log_2 (10^6) \\approx 20$ steps to add all the numbers\n",
    "1. $\\mathcal{O}(\\log n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count: The \"Hello World\" of distributed computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"word count\" problem is a good example of an embarassingly parallel problem, so we use it to demonstrate the thinking around distributed computing.  The problem is to get a count of how many times each word appears in some text.  We can imagine doing this in real life, with two different approaches.\n",
    "\n",
    "**Serial approach:** Read through a book from cover to cover, keeping a tally.\n",
    "\n",
    "**Parallel approach:** Give pages to a bunch of people, have them tally each page, then sum the tallies.\n",
    "\n",
    "We will show how to execute this parallel approach in several frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce with Unix pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write mapper and reducer programs.  We can use Unix pipes to connect the output of the mapper to the reducer.\n",
    "\n",
    "The mapper program takes in text a line at a time and outputs a count of one for each word seen.  (Technically, this is an example of a *flatmap*, since each input gives multiple outputs, but these are flattened into a single output list.)  The output is in the form of key-value pairs, where the key is the word and the value the count.\n",
    "\n",
    "#### `mapper.py`\n",
    "```python\n",
    "import sys\n",
    "\n",
    "def read_input(file, encoding='utf-8'):\n",
    "    for line in file:\n",
    "        yield line.decode(encoding)\n",
    "\n",
    "def write_line(line, encoding='utf-8'):\n",
    "    sys.stdout.buffer.write((line + '\\n').encode(encoding))\n",
    "\n",
    "def main(separator='\\t'):\n",
    "    data = read_input(sys.stdin.buffer)\n",
    "    for line in data:\n",
    "        for word in line.split():\n",
    "            write_line('%s%s%d' % (word, separator, 1))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "The reducer combines groups with the same word to count up the sum.  Note that `itertools.groupby` only creates groups out of sequential inputs.  This is beneficial, since it means that we don't have to read all the input into memory.  It does require that the input to the reducer be sorted by key.\n",
    "\n",
    "#### `reducer.py`\n",
    "```python\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "def read_input(file, encoding='utf-8'):\n",
    "    for line in file:\n",
    "        yield line.decode(encoding)\n",
    "\n",
    "def write_line(line, encoding='utf-8'):\n",
    "    sys.stdout.buffer.write((line + '\\n').encode(encoding))\n",
    "\n",
    "def main(separator='\\t'):\n",
    "    data = map(lambda line: line.split(separator, 1),\n",
    "               read_input(sys.stdin.buffer))\n",
    "    for current_word, group in groupby(data, itemgetter(0)):\n",
    "        total_count = sum(int(count) for _, count in group)\n",
    "        write_line(\"%s%s%d\" % (current_word, separator, total_count))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "One the command line, we can create a pipeline to process the data.  Note the use of `sort` prior to the reducer.\n",
    "```bash\n",
    "cat small_data/gutenberg/* \\\n",
    "    | python scripts/mapper.py \\\n",
    "    | sort -k1,1 \\\n",
    "    | python scripts/reducer.py \\\n",
    "    | grep \"^[a-z]\" \\\n",
    "    | less\n",
    "```\n",
    "\n",
    "We can run the map and reduce phases in parallel with the `parallel` command.\n",
    "```bash\n",
    "cat small_data/gutenberg/* \\\n",
    "    | parallel --pipe python scripts/mapper.py \\\n",
    "    | sort -k1,1 \\\n",
    "    | parallel --pipe python scripts/reducer.py \\\n",
    "    | grep \"^[a-z]\" \\\n",
    "    | less\n",
    "```\n",
    "\n",
    "(Note that the reducer doesn't quite work in parallel, since we haven't ensured that all copies of a key will go to same reducer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is a distributed-computing framework, written in Java.  Its MapReduce example is nearly as simple as the above.\n",
    "\n",
    "```java\n",
    "package com.thedataincubator.hadoopexamples;\n",
    "\n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "import org.apache.hadoop.util.GenericOptionsParser;\n",
    "\n",
    "public class WordCount {\n",
    "\n",
    "  public static class WordTokenizerMapper\n",
    "  extends Mapper<Object, Text, Text, IntWritable>\n",
    "  {\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "    private Text word = new Text();\n",
    "\n",
    "    public void map(Object key,\n",
    "                    Text value,\n",
    "                    Context context)\n",
    "    throws IOException, InterruptedException\n",
    "    {\n",
    "      //System.err.println(String.format(\"[map] key: (%s), value: (%s)\", key, value));\n",
    "      // break each sentence into words, using the punctuation characters shown\n",
    "      StringTokenizer tokenizer = new StringTokenizer(value.toString(), \" \\t\\n\\r\\f,.:;?![]'\\\"\");\n",
    "      while (tokenizer.hasMoreTokens())\n",
    "      {\n",
    "        // make the words lowercase so words like \"an\" and \"An\" are counted as one word\n",
    "        String s = tokenizer.nextToken().toLowerCase().trim();\n",
    "        word.set(s);\n",
    "        context.write(word, one); // this writes the word and the number one, like this: (\"Africa\", 1)\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static class WordOccurrenceReducer\n",
    "  extends Reducer<Text, IntWritable, Text, IntWritable>\n",
    "  {\n",
    "    private IntWritable occurrencesOfWord = new IntWritable();\n",
    "\n",
    "    public void reduce(Text key,\n",
    "                       Iterable<IntWritable> values,\n",
    "                       Context context)\n",
    "    throws IOException, InterruptedException\n",
    "    {\n",
    "      int sum = 0;\n",
    "      for (IntWritable val : values) {\n",
    "        sum += val.get();\n",
    "      }\n",
    "      occurrencesOfWord.set(sum);\n",
    "      context.write(key, occurrencesOfWord); // this writes the word and the count, like this: (\"Africa\", 2)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * the \"driver\" class. it sets everything up, then gets it started.\n",
    "   */\n",
    "  public static void main(String[] args)\n",
    "  throws Exception\n",
    "  {\n",
    "    Configuration conf = new Configuration();\n",
    "    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n",
    "    if (otherArgs.length != 2)\n",
    "    {\n",
    "      System.err.println(\"Usage: wordcount <inputFile> <outputDir>\");\n",
    "      System.exit(2);\n",
    "    }\n",
    "    Job job = new Job(conf, \"word count\");\n",
    "    job.setJarByClass(WordCount.class);\n",
    "    job.setMapperClass(WordTokenizerMapper.class);\n",
    "    job.setCombinerClass(WordOccurrenceReducer.class);\n",
    "    job.setReducerClass(WordOccurrenceReducer.class);\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "    FileInputFormat.addInputPath(job, new Path(otherArgs[0]));\n",
    "    FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));\n",
    "    System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "If Hadoop is installed, you can run this with\n",
    "```bash\n",
    "hadoop jar target/hadoop-examples-1.0.jar \\\n",
    "    com.thedataincubator.hadoopexamples.WordCount \\\n",
    "    gutenberg gutenberg-output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General MapReduce Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MapReduce Process Structure](images/MapReduce_Work_Structure.png)\n",
    "<!-- From Xiaocheng Zhang's blog: http://xiaochongzhang.me/blog/?p=338 -->\n",
    "\n",
    "1. **Input**: the input is a large file on a distributed file system stored as an input system and read according to a user-specified `InputFormat`.  For word count, it is a large text file.\n",
    "1. **Splitting**: an Input Reader splits the file into chunks of 64 MB or 128 MB.  We think of each chunk as consisting of a collection of records which can be represented as key-value pairs `(k0, v0)`.  We think of the records as being unordered but keyed.  The splitting is done so that each chunk is also on a single machine.  While there can be multiple chunks on a single machine, it is useful to abstract things and assume that each chunk resides on its own machine.\n",
    "1. **Mapping**: the user specifies a `map` function, which iterates through the records within a chunk.  It outputs key-value pairs `(k1, v1)` (which are written to the local disk).  It can output multiple key-value pairs per input record or none at all and the key and value need not have the same type as the input.  In our case, each input record is a line in a file (e.g. `\"Deer Bear River\"`) and there is an output record consisting of each word with the number one (e.g. `(\"Deer\", 1)`), indicating that we saw the word `\"Deer\"` once.\n",
    "1. **Combiner**: This runs a reduce right after mapping but before shuffling.  The **local aggregation** can reduce the amount of I/O in the shuffle phase, which is the most important bottleneck.\n",
    "1. **Shuffle**: up until this point, all the computations have happened locally and no data has been exchanged between the servers.  During Shuffle, the key value pairs `(k1, v1)` are sent to a computer based on `k1` so that all pairs with the same key end up at the same computer (and are written to that computer's disk).  Think of `k1` as giving the *address* to which to send the data and `v1` as giving the data payload to deliver.  Again, which we can have multiple keys mapping to the same physical machine, it is useful to abstract this and assume each key residing on its own machine.\n",
    "1. **Reduce**: the user specifies a `reduce` function, which is given pairs `(k1, iter(v1))` where `iter(v1)` is an iterator over the values.  An iterator is like a list except that there is no random access to elements and one can only iterate through its elements once in order.  Underneath the hood, all the data resides on disk and only the current record being iterated over is kept in memory.  The `reduce` function outputs key-value pairs `(k2, v2)` (which are written to the local disk).  Again, the number of values it outputs is independent of the number of inputs and they can be of different types.  In our case, it maps each `(k1, iter(v1))` to `(k1, sum(v1))`.\n",
    "1. **Final Result**: The results at this point are written out to disk according to a user-specified `OutputFormat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is a distributed-computing frame work that can run on top of Hadoop or on its own.  It provides a somewhat more expressive way of describing distributed computations.  We will be covering it in much more detail in the rest of the notebooks, but here is a quick taste.\n",
    "\n",
    "We start by importing modules and doing some set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"temp\")\n",
    "\n",
    "def localpath(path):\n",
    "    return 'file://' + str(os.path.abspath(os.path.curdir)) + '/' + path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains the entire word count process, which proceeds in much the same way as the MapReduce examples above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(localpath('small_data/gutenberg/'))\n",
    "lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "     .map(lambda word: (word.lower(), 1)) \\\n",
    "     .reduceByKey(lambda x, y: x + y) \\\n",
    "     .sortByKey() \\\n",
    "     .saveAsTextFile(localpath(\"gutenberg_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat gutenberg_count/part-00000 | grep \"^('[a-z]\" | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `lines` is a **Resilient Distributed Dataset (RDD)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is like a (smartly) distributed list, which is partitioned across the nodes of the cluster and can be operated on in parallel.  The operations take the form of the usual functional list operations.  There are two types of operations: Transformations and Actions.\n",
    "\n",
    "- **Transformations:** These creates a new RDD from an old one: for instance `map` or `flatMap`.  Transformations in Spark are __lazy__, which means that they do not compute the answer right away -- only when it is needed by something else.  Instead they represent the \"idea\" of the transformation applied to the base data set (e.g. for each chaining).  In particular, this means that intermediate results to computations do not necessarily get created and stored -- if the output of your map is going straight into a reduce, Spark should be clever enough to realize this and not actually store the output of the map.  Another consequence of this design is that the results of transformations are, by default, recomputed each time they are needed -- to store them one must explicitly call `cache`.\n",
    "    \n",
    "    [Transformations](https://spark.apache.org/docs/latest/programming-guide.html#transformations) typically *transform* the data and return another RDD.  Common examples include `map`, `filter`, `flatMap`, and also `join`, `cogroup`, `groupByKey`, `reduceByKey`, `countByKey`, and `sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Actions:** These actually return a value as a result of a computation.  For instance, `reduce` is an action that combines all elements of an RDD using some function and then returns the final result.  (Note that `reduceByKey` actually returns an RDD.)\n",
    "    \n",
    "    [Actions](https://spark.apache.org/docs/latest/programming-guide.html#actions) typically either generate *small* outputs (e.g. `reduce`, `count`, `first`, `take`, `takeSample`, `foreach`, `collect`) or persist to disk (e.g. `saveAsTextFile`, `saveAsSequenceFile`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.flatMap(lambda line: line.split(\" \")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Spark Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While RDDs must be of consistent type, there is little structure on each row.  It is up to the user to remember the order of the data in each row.\n",
    "\n",
    "Inspired by R, Spark **DataFrames** provide a structured view of data in an RDD, without compromising their distributed nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "          .map(lambda word: (word.lower(), 1)) \\\n",
    "          .reduceByKey(lambda x, y: x + y) \\\n",
    "          .toDF(['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations on the DataFrames are automatically carried out in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df['count'] > 5000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were initially using Spark for **batch processing**: We defined a pipeline through which all of the data travel, producing an answer.\n",
    "\n",
    "DataFrames are part of Spark SQL, which allow you to write **queries** to run over distributed data.  You can do that with the syntax show above, or with SQL syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('counts')\n",
    "sqlContext.sql('select * from counts where count > 5000').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark also supports **iterative processing**.  This includes machine learning algorithms like stochastic gradient descent.\n",
    "\n",
    "Here, we use a linear model to work out the correlation between the length of words and their popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.functions import UserDefinedFunction as udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_length = udf(lambda x: Vectors.dense(len(x)), VectorUDT())\n",
    "feat_df = df.withColumn(\"features\", word_length(\"word\")) \\\n",
    "            .withColumnRenamed(\"count\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "model = linreg.fit(feat_df)\n",
    "model.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can also be used to perform **stream processing**.  Incoming data are grouped into \"mini-batches\", and the RDD transformations are applied to those batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
