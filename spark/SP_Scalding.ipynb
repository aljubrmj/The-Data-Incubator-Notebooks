{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalding (Scala on Cascading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to https://github.com/twitter/scalding for the latest and greatest in Scalding documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Background](#background)\n",
    "1. [The Scalding API](#api)\n",
    "1. [The canonical example](#countingwords)\n",
    "1. [Tools set-up](#setup)\n",
    "1. [A big data exercise](#exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id='background'></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frameworks on top of MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you've been exposed to two frameworks of distributed computing: Hadoop MapReduce and Apache Spark. Spark showed us how \"nice\" programming around big data can be in terms of expressing your algorithms in terms of primitives and functions that allow you to think more abstractly than \"key, value, shuffle, map-and-reduce-only.\" However there are still many situations in which we would still like our cluster manager to run MapReduce under the hood without sacrificing convenient utilities like joins.\n",
    "\n",
    "Thankfully, several APIs exist on top of MapReduce that allow exactly this: [Pig](https://pig.apache.org/docs/latest/basic.html), [Crunch](https://crunch.apache.org), and [Cascading](http://www.cascading.org) are the most popular; they each offer benefits and tradeoffs over one another. \n",
    "\n",
    "We'll introduce Scalding, the Scala DSL of Cascading, primarily because:  \n",
    "1. we've already been introduced to Scala via Spark,  \n",
    "1. Scalding has wide usage in sophisticated production settings among a number of recognizable companies,  \n",
    "1. it has an advantage over the UDFs and SQL-like operations of Pig because Pig requires one to maintain two codebases for it to be used in production (one for Pig operations, the other in Python/Java to call the Pig operations)\n",
    "1. for a comparison to Crunch, refer [here](#crunch_difference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id='api'></span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalding API primer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalding's **functions**, i.e. the \"verbs\" of the DSL, can be divided into four types:\n",
    "- Map-like functions\n",
    "- Grouping functions\n",
    "- Group/Reduce functions\n",
    "- Join operations\n",
    "And other misc. functions\n",
    "\n",
    "Scalding's key **objects**, i.e. the \"nouns\":\n",
    "- `TypedPipe[T]` : distributed list of objects of type T\n",
    "- `KeyedList[K,V]` : represents some sharding of objects of key K and value V\n",
    "    - `Grouped[K,V]` : usual groupings\n",
    "    - `CoGrouped[K,V]` : co-groupings / joins\n",
    "\n",
    "**APIs**  \n",
    "- Fields-based API (legacy)\n",
    "- Type-safe API (use this!)\n",
    "- Matrix API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id='countingwords'></span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce paradigm in Scalding  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The \"Scalding\" way to approach the word count algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Have a distributed list of objects of type `String`, i.e. a `TypedPipe[String]` resulting from reading in a text file  \n",
    "- From the `TypedPipe`, generate a flattened list of words. Split tokens on one or more whitespace characters, i.e. with regex: `\\\\s+`  \n",
    "- Group this list of words by word  \n",
    "- Count the number of occurrences of each word\n",
    "- Write tab-separated word (String), count (Long) to file  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script that does the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "import com.twitter.scalding._\n",
    "\n",
    "val lines : TypedPipe[String] = TypedPipe.from(TextLine(\"hello.txt\"))\n",
    "\n",
    "// Write a word count to file\n",
    "lines.flatMap(_.split(\"\\\\s+\"))\n",
    "    .group\n",
    "    .sum\n",
    "    .write(TypedTsv[(String, Long)](\"output\"))\n",
    "```    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scala class (that is also stylistically nicer) that does the above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "import com.twitter.scalding._\n",
    "\n",
    "class WordCountJob(args: Args) extends Job(args) {\n",
    "  TypedPipe.from(TextLine(args(\"input\")))\n",
    "    .flatMap { line => tokenize(line) }\n",
    "    .groupBy { word => word } // use each word for a key\n",
    "    .size // in each group, get the size\n",
    "    .write(TypedTsv[(String, Long)](args(\"output\")))\n",
    "\n",
    "  // Split a piece of text into individual words.\n",
    "  def tokenize(text : String) : Array[String] = {\n",
    "    // Lowercase each word and remove punctuation.\n",
    "    text.toLowerCase.replaceAll(\"[^a-zA-Z0-9\\\\s]\", \"\").split(\"\\\\s+\")\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id='setup'></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and using Scalding locally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ git clone https://github.com/twitter/scalding.git\n",
    "$ ./sbt update\n",
    "$ ./sbt test     # runs the tests; if you do 'sbt assembly' below, these tests, which are long, are repeated\n",
    "$ ./sbt assembly # creates a fat jar with all dependencies, which is useful when using the scald.rb script\n",
    "```\n",
    "\n",
    "Also add a SCALDING_HOME variable to your ~/.bash_profile pointing to the Scalding directory. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalding REPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open the REPL:\n",
    "`$ $SCALDING_HOME/scripts/scald.rb --repl --local`\n",
    "\n",
    "Walk through this REPL example at https://gist.github.com/johnynek/a47699caa62f4f38a3e2 to get a feel for the Scalding programming paradigms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running your Scalding code as an app\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`$ scald.rb --local {YourApp.scala}`\n",
    "\n",
    "Your `build.sbt` should look something like this:  \n",
    "```scala\n",
    "name := {Your App Name}\n",
    "version := \"1.0\"\n",
    "scalaVersion := \"2.10.4\"\n",
    "\n",
    "resolvers += \"Concurrent Maven Repo\" at \"http://conjars.org/repo\"\n",
    "\n",
    "libraryDependencies += Seq(\n",
    "    \"cascading\" % \"cascading-core\" % \"2.0.2\",\n",
    "    \"cascading\" % \"cascading-local\" % \"2.0.2\",\n",
    "    \"cascading\" % \"cascading-hadoop\" % \"2.0.2\",\n",
    "    \"cascading.kryo\" % \"cascading.kryo\" % \"0.4.4\",\n",
    "    \"com.twitter\" % \"meat-locker\" % \"0.3.0\",\n",
    "    \"com.twitter\" % \"maple\" % \"0.2.2\",\n",
    "    \"commons-lang\" % \"commons-lang\" % \"2.4\",\n",
    "    \"com.twitter\" % \"scalding_2.9.2\" % \"0.7.3\",\n",
    "    \"org.specs2\" % \"specs2_2.9.2\" % \"1.12.1\"\n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scalding on Amazon EMR \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Scalding is Hadoop MapReduce under the hood, unlike in the case of Spark where we had to follow special submission instructions, submitting a `jar` of Scalding code works in the same way as submitting a `jar` of straight-up Java MapReduce code. To build your project, run `sbt assembly` in the root directory of your project.\n",
    "\n",
    "You'll fire up an EMR cluster as per usual (use `./scripts/launch_emr.sh` as a guide) and submit a step either during the creation or after the cluster is running through the EMR Console or the AWS CLI.\n",
    "\n",
    "Official EMR documentation is here: http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/CreateCascading.html\n",
    "\n",
    "TL;DR the command to create a cluster and add a step should be something like:\n",
    "```bash\n",
    "aws emr create-cluster --name \"Scalding job cluster\" --ami-version 3.6 \\\n",
    "--use-default-roles --ec2-attributes KeyName=myKey \\\n",
    "--instance-type m3.xlarge --instance-count 3 \\\n",
    "--bootstrap-actions Path=pathtobootstrapscript,Name=\"CascadingSDK\" \\\n",
    "--steps Type=\"CUSTOM_JAR\",Name=\"Scalding Step\",ActionOnFailure=CONTINUE,Jar=s3://pathtojarfile,\\\n",
    "Args=[\"-input\",\"s3://pathtoinputdata\",\"-output\",\"s3://pathtooutputbucket\",\"arg1\",\"arg2\"]\n",
    "```\n",
    "\n",
    "To add to a running cluster:\n",
    "```bash\n",
    "aws emr add-steps --cluster-id j-XXXXX --steps Type=\"CUSTOM_JAR\",Name=\"Scalding Step\",ActionOnFailure=CONTINUE,Jar=s3://pathtojarfile,\\\n",
    "Args=[\"-input\",\"s3://pathtoinputdata\",\"-output\",\"s3://pathtooutputbucket\",\"arg1\",\"arg2\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id='exercise'></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Analyzing Wikipedia Traffic in 2008\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle this exercise, you can use your choice of big-/small-data tools since the data is on `s3` and you can therefore fire up an EC2/EMR cluster installed with the tools of your choice. We suggest using either Spark or Scalding. In both cases, you should be able to complete all the questions using just that one tool (i.e. both the questions that require big-data-munging and those that can be answered in-memory); however you may also choose to use a mix of Python Pandas for the in-memory-size data (e.g. for small time series calculations) after ETLing with one of the big data frameworks. \n",
    "\n",
    "If you answer all the questions in the exercise: congratulations! You'll have completed a full data scientist workflow using \"production-like\" data. You'll get a feel for how the workflow for big data differs from small data. You'll also gain an interesting talking point to potential employers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The English language data set is located at `s3://dataincubator-course/wikidata/wikistats/en_pagecounts` and derived from the encyclopedic [publicly available data](https://aws.amazon.com/datasets/) on AWS. It's about 20 GB compressed and covers the period 10/1/08 through 12/8/08.\n",
    "\n",
    "(If you want to look at the entire data set i.e. not just English-language, the full `wikistats` `pagecount` data set is at `s3://dataincubator-course/wikidata/wikistats/pp_pagecounts/` is ~75 GB and covers the period 10/1/08 through 12/8/08.)\n",
    " \n",
    "Each log has 4 fields: `yearmonthdayhour, projectcode, pagename, pageviews`\n",
    "```\n",
    "2008120701 en Still_Breathing 2\n",
    "2008120701 en Still_Climbing_%28album%29 1\n",
    "2008120701 en Still_Creepin_On_Ah_Come_Up 1\n",
    "2008120701 en Still_Life_with_Spherical_Mirror 2\n",
    "2008120701 en Stillwater_Mining_Company 2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions (for the English language data set only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Give 100 most popular websites by `pageviews` from 10/1/08-10/15/08 (inclusive), ordered by `pageviews` descending.\n",
    "\n",
    "1. For (start date) to (end date): Visualize the distribution of pages' cumulative `pageviews` during this time period. Give the mean, median, standard deviation. What kind of distribution is it?\n",
    "\n",
    "1. For the topic \"Barack Obama\": Look at the time series of daily page views from (start) to (end) date. Calculate the correlation with topic \"Sarah Palin.\"\n",
    "\n",
    "1. Bonus: How does the PageRank of a page predict its mean views over time?\n",
    "\n",
    "1. For a sample of pairs of linked nodes, calculate the `pageview` correlations. How does the the distribution of correlations compare to that of a randomly chosen sample of unlinked node pairs?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the main difference between Crunch and Cascading?**  \n",
    "(Source: https://github.com/cloudera/crunch/wiki/Frequently-Asked-Questions)  \n",
    "\n",
    "The main difference between Crunch and Cascading/Pig/Hive is in their data models. In Pig and Cascading, most operations in a pipeline are performed on collections of Tuples (here are the Javadocs for the Pig Tuple and the Cascading Tuple). In my answer to this question on Quora, I refer to this as the \"single serializable type\" (SST) model. Using the SST data model makes it much easier to implement common operations, and Pig, and Cascading provide big libraries of built-in functions that are designed to operate on their respective Tuple types. They also provide APIs for developers to create their own user-defined functions that interact with their SST data models.\n",
    "\n",
    "Crunch, like the FlumeJava library that is based on, uses a data model that has multiple serializable types (MST). At each stage in a Crunch pipeline, you specify how the data from that stage should be serialized. The benefit of doing this is that it lets you verify at compile-time that each stage of your pipeline will actually receive the type of data it knows how to process. In this sense, the MST model for building pipelines is similar to using a statically typed language, and the SST model is similar to using a dynamically typed language. We feel that the MST model has definite benefits for MapReduce developers:\n",
    "\n",
    "Compile-time type verification lowers the probability that a type error will cause your MapReduce pipeline to fail when it actually runs on a Hadoop cluster.\n",
    "It makes user-defined functions in Crunch extremely easy to write, saving you from writing the boilerplate for type checking your data that you need to do in Pig or Cascading.\n",
    "It makes MapReduce tasks that run over complex data types, like binary data or the results of an HBase scan, much easier to write. There's no work required to map from the complex type onto the Cascading/Pig SST and back again.\n",
    "Crunch's MST serialization model currently has two different implementations, one based on Writables and the other based on Avro records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Describe the differences between HDFS, Cascading, and Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
