{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "\n",
    "// For implicit transformation of RDDs to DataFrames\n",
    "import sqlContext.implicits._\n",
    "\n",
    "// For telling Spark to look in the local file system\n",
    "import java.io._\n",
    "def localpath(path: String): String = {\n",
    "    \"file://\" + new java.io.File(\".\").getCanonicalPath + \"/\" + path\n",
    "}\n",
    "\n",
    "// For timing expression evaluation\n",
    "def time[R](block: => R): R = {\n",
    "    val start: Long = System.nanoTime()\n",
    "    val result = block\n",
    "    val end: Long = System.nanoTime()\n",
    "    val duration: Double = (end - start) / 1000000000.0\n",
    "    println(\"Elapsed time: \" + duration + \"s\")\n",
    "    result\n",
    "}\n",
    "\n",
    "println(\"Using Spark version \" + sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "\n",
    "// For implicit transformation of RDDs to DataFrames\n",
    "import sqlContext.implicits._\n",
    "\n",
    "// For timing expression evaluation\n",
    "def time[R](block: => R): R = {\n",
    "    val start: Long = System.nanoTime()\n",
    "    val result = block\n",
    "    val end: Long = System.nanoTime()\n",
    "    val duration: Double = (end - start) / 1000000000.0\n",
    "    println(\"Elapsed time: \" + duration + \"s\")\n",
    "    result\n",
    "}\n",
    "\n",
    "println(\"Using Spark version \" + sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Intro\n",
    "<!-- requirement: small_data/employee -->\n",
    "<!-- requirement: small_data/gutenberg -->\n",
    "<!-- requirement: small_data/eeg -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like _Hadoop_, __Spark__ is a low-level system for distributed computation on a cluster.  It has two major advantages over MapReduce:\n",
    " \n",
    " \n",
    " - It can do in-memory caching between stages, which improves performance.  It also makes it suitable for classes of algorithms that would otherwise be too slow (*e.g.* iterative algorithms, like the training step in many machine learning algorithms).\n",
    " \n",
    " - It has a more flexible execution model and a more expressive API.  This allows you to easily write things beyond map and reduce.\n",
    " \n",
    "There are also minor advantages: The default API is much nicer than writing raw MR code. The favored interface is via a language called Scala, but there is also good support for using Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The [Spark homepage](http://spark.apache.org) is your best starting point for high level programming guides, API docs, the blog, and in general most things you'll need.\n",
    "* The [GitHub page](https://github.com/apache/spark) can be useful if you want to dig through source code.\n",
    "* The [DataBricks Community Edition](https://community.cloud.databricks.com) is a free Spark notebook sandbox that also includes a bunch of useful tutorials.\n",
    "* There's a GitBook called [Mastering Apache Spark 2.0](https://www.gitbook.com/book/jaceklaskowski/mastering-apache-spark/details).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Spark relate to Hadoop?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer 1__: It's a replacement for it.  You can manage and run your own Spark cluster, independent of Hadoop.  You'll have to figure out the file system layer from scratch though.   (In this context, _Spark SQL_ is the replacement for _Hive_, i.e. for SQL-like operation.)\n",
    "\n",
    "__Answer 2__: It's a complement to it.  You can run Spark on top of a Hadoop cluster, and still leverage HDFS and YARN -- then Spark is just replacing MapReduce.  (In this context, _Spark SQL_ can be used as a drop-in replacement for _Hive_.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Spark API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main abstractions in Spark are:\n",
    "1. Resilient distributed data sets (RDDs):\n",
    "  This is like a (smartly) distributed list, which is partitioned across the nodes of the cluster and can be operated on in parallel.  The operations take the form of the usual functional list operations as shown above.  There are two types of operations: Transformations and Actions.\n",
    "1. Shared variables used in parallel operations: When Spark runs a function in parallel across nodes, it ships a copy of each variable needed by that function to each node.  There are two types of shared variables:\n",
    " - Broadcast variables: These are used to store a value in memory across all nodes (i.e. their values are \"broadcast\" from the driver node)\n",
    " - Accumulator variables: These are variables which are only added to across nodes, for implementing counters and sums.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Transformations:** This creates a new RDD from an old one: for instance `map` or `flatMap`.  Transformations in Spark are __lazy__, which means that they do not compute the answer right away -- only when it is needed by something else.  Instead they represent the \"idea\" of the transformation applied to the base data set (e.g. for each chaining).  In particular, this means that intermediate results to computations do not necessarily get created and stored -- if the output of your map is going straight into a reduce, Spark should be clever enough to realize this and not actually store the output of the map.  Another consequence of this design is that the results of transformations are, by default, recomputed each time they are needed -- to store them one must explicitly call `cache`.\n",
    "    \n",
    "    [Transformations](https://spark.apache.org/docs/latest/programming-guide.html#transformations) typically *transform* the data and return another RDD.  Common examples include `map`, `filter`, `flatMap`, and also `join`, `cogroup`, `groupByKey`, `reduceByKey`, `countByKey`, and `sample`.\n",
    "\n",
    "- **Actions:** These actually return a value as a result of a computation.  For instance, `reduce` is an action that combines all elements of an RDD using some function and then returns the final result.  (Note that `reduceByKey` actually returns an RDD.)\n",
    "    \n",
    "    [Actions](https://spark.apache.org/docs/latest/programming-guide.html#actions) typically either generate *small* outputs (e.g. `reduce`, `count`, `first`, `take`, `takeSample`, `foreach`, `collect`) or persist to disk (e.g. `saveAsTextFile`, `saveAsSequenceFile`, etc.).\n",
    "\n",
    "\n",
    "**Questions:**\n",
    "1. What's the difference between `map` and `foreach` in (non-Spark) Scala?  Why is `map` a transformation but `foreach` an action in Spark?\n",
    "1. Why is `reduceByKey` a transformation but `reduce` an action?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common distributed-computing example is word count: count the number of times a word appears in a large corpus of text.  It is easily parallelizable, since counts can be made on different parts of the text and then added together to get the final count.\n",
    "\n",
    "We start off by making a Spark **context**.  This object manages the resources available to a Spark job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkContext\n",
    "// If we had to we could create a new spark context using this package\n",
    "// The SparkConf package is also important for configuration\n",
    "assert(classOf[SparkContext].getPackage.getImplementationVersion == sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark will work on a distributed file system by default.  To tell it to deal with local files, we introduce the `localpath` convenience function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io._\n",
    "def localpath(path: String): String = {\n",
    "    \"file://\" + new java.io.File(\".\").getCanonicalPath + \"/\" + path\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `textFile` function, we create a RDD that contains all of the lines for all of the files in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// `val sc` is already introduced in the namespace.\n",
    "val lines = sc.textFile(localpath(\"small_data/gutenberg/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `count` action will tell us the total number of lines in these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do useful calculations, we will chain transformations together.  Here, a `flatMap` converts the RDD of lines to a RDD of words.  The `map` makes a tuple of `(word, 1)`.  The word acts as a key for the `reduceByKey` transformation, which sums up the values associated with each key, giving us the total count for each word.  Finally, we sort the RDD and write it out to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val filename = \"tmp/output_gutenberg\" + (System.currentTimeMillis / 1000).toString\n",
    "\n",
    "lines.flatMap(line => line.split(\" \"))\n",
    "     .map(word => (word.toLowerCase(), 1))\n",
    "     .reduceByKey(_ + _)\n",
    "     .sortByKey()\n",
    "     .saveAsTextFile(localpath(filename))\n",
    "\n",
    "println(s\"Output at $filename\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happened in each of these steps.  If we just looks at `lines`, all we see is that it is an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't see any of the lines themselves.  This is a result of Spark's lazy computation.  Only when an action requests some data is a computation actually run.  The `collect()` action will gather all of the elements in the RDD into a Python list.  We don't want all of these, so we'll use `take(5)` to gather five elements from the RDD.  Spark will do the minimal computation necessary to get those five elements; here we see that they come from the beginning of one of the books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements in the `lines` RDD are whole lines.  The `flatMap` transformation will create a new RDD containing the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.flatMap(line => line.split(\" \")).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference from the basic `map` transformation, which would convert each line into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.map(line => line.split(\" \")).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use a `map`, now to create those key-value pairs.  We're keying by the word, with the count as the value.  Each time we see a word, it's just a count of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.flatMap(line => line.split(\" \"))\n",
    "     .map(word => (word.toLowerCase, 1))\n",
    "     .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `reduceByKey` will gather like keys together into a single entry, whose value is the result of running the specified reduction function, in this case summation, on all the values associated with that key.  This give us the counts per word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.flatMap(line => line.split(\" \"))\n",
    "     .map(word => (word.toLowerCase, 1))\n",
    "     .reduceByKey(_ + _)\n",
    "     .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sortByKey` will alphabetize the RDD, for our convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.flatMap(line => line.split(\" \"))\n",
    "     .map(word => (word.toLowerCase, 1))\n",
    "     .reduceByKey(_ + _)\n",
    "     .sortByKey()\n",
    "     .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** There are a lot of special symbols in the output.  How would you do word count only on pure characters?  Hint: in Scala and Java, you can test for regular expressions this way:\n",
    "```scala\n",
    "\"abc\".matches(\"^\\\\w+$\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: a simple simulation for $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val numSamples = 100000\n",
    "\n",
    "val count = sc.parallelize(1 to numSamples).map{_ =>\n",
    "  val x = Math.random()\n",
    "  val y = Math.random()\n",
    "  if (x*x + y*y < 1) 1 else 0\n",
    "}.reduce(_ + _)\n",
    "println(\"Pi is roughly \" + 4.0 * count / numSamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first steps of any data science project usually involve ETL and exploratory analytics on a data set.\n",
    "Here we'll acquaint ourselves both with the Spark shell and some Spark functions that we can use to perform such tasks.\n",
    "\n",
    "#### The data set\n",
    "From UC Irvine's [Machine Learning Archive](http://cml.ics.uci.edu/):\n",
    "> \"The first four lines are header information. Line 1 contains the subject identifier and indicates if the subject was an alcoholic (a) or control (c) subject by the fourth letter. Line 4 identifies the matching conditions: a single object shown (`S1 obj`), object 2 shown in a matching condition (`S2 match`), and object 2 shown in a non matching condition (`S2 nomatch`). Line 5 identifies the start of the data from sensor `FP1`. The four columns of data are: the trial number, sensor position, sample number (0-255), and sensor value (in micro volts).\"\n",
    "\n",
    "There are 16,452 rows in this file including the header. Here's a preview of the first 10 lines and last 10 lines:\n",
    "```\n",
    "# co2a0000364.rd\n",
    "# 120 trials, 64 chans, 416 samples 368 post_stim samples\n",
    "# 3.906000 msecs uV\n",
    "# S1 obj , trial 0\n",
    "# FP1 chan 0\n",
    "0 FP1 0 -8.921\n",
    "0 FP1 1 -8.433\n",
    "0 FP1 2 -2.574\n",
    "0 FP1 3 5.239\n",
    "0 FP1 4 11.587\n",
    "    ...\n",
    "0 Y 246 24.150\n",
    "0 Y 247 20.243\n",
    "0 Y 248 11.454\n",
    "0 Y 249 4.618\n",
    "0 Y 250 3.153\n",
    "0 Y 251 6.571\n",
    "0 Y 252 12.431\n",
    "0 Y 253 15.849\n",
    "0 Y 254 16.337\n",
    "0 Y 255 14.872\n",
    "```\n",
    "\n",
    "We need to filter out the header lines, which begin with a '#'.  We'll also make a simple class to represent each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isHeader(line: String) : Boolean = {\n",
    "    line.contains(\"# \")\n",
    "}\n",
    "\n",
    "case class Record(\n",
    "    trial: Int,\n",
    "    posn: String,\n",
    "    sample: Int,\n",
    "    reading: Double\n",
    ")\n",
    "\n",
    "def parse(line: String) = {\n",
    "    val tokens = line.split(\"\\\\s+\")\n",
    "    val trial = tokens(0).toInt\n",
    "    val posn = tokens(1)\n",
    "    val sample = tokens(2).toInt\n",
    "    val reading = tokens(3).toDouble\n",
    "    Record(trial, posn, sample, reading)\n",
    "}\n",
    "\n",
    "val data = ( sc.textFile(localpath(\"small_data/eeg/*\"))\n",
    "                 .filter(x => !isHeader(x))\n",
    "                 .map(parse)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an action for `RDD[Double]` called `stats` that will provide us with summary statistics about the values in the RDD. How can we get out summary stats of the `reading` column across the entire data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(data.map(_.reading).stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we do the same but for only the position `FP1`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(data.filter(_.posn == \"FP1\")\n",
    "            .map(_.reading).stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure there are 256 samples per `posn` in this data set. How can we do this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val samples = { data.map(x => ((x.trial, x.posn), 1))\n",
    "                    .reduceByKey(_ + _)}\n",
    "val posns = samples.map{case (x, y) => (x._2, y)}.distinct()\n",
    "\n",
    "println(posns.take(1)(0))\n",
    "println(posns.count(), \n",
    "        posns.filter{case (x, y) => y == 256}.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins in Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in MapReduce, a join is accomplished by emitting key-value pairs `(k, v)` and joining on similar keys.  It is accomplished by roughly the same mechanism (sending similar keys to the same *node*), although in Spark those key-value pairs may not (necessarily) be persisted to disk.\n",
    "\n",
    "The important thing to remember about performing joins between RDDs is that you need to massage your data into `pairRDD`, i.e. one with key-value pairs organized as `(k, v)`.\n",
    "\n",
    "Having a `pairRDD` also allows for other methods you might expect such as `groupByKey` or `reduceByKey`.\n",
    "\n",
    "As an example, consider the case where we have two tables, stored here as CSV files. One contains sales information, connecting users to particular transactions.\n",
    "\n",
    "```\n",
    "sales,transaction1,product1,user1,100\n",
    "sales,transaction2,product4,user2,150\n",
    "sales,transaction3,product3,user1,200\n",
    "sales,transaction4,product4,user4,50\n",
    "sales,transaction5,product1,user3,100\n",
    "sales,transaction6,product2,user1,100\n",
    "```\n",
    "\n",
    "We want some structure to represent each row of data in an RDD.  We could use a list or tuple, but in this case we'll make a simple `case class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Transaction(\n",
    "    transactionId: String,\n",
    "    productId: String,\n",
    "    userId: String,\n",
    "    amount: Int\n",
    ")\n",
    "\n",
    "val transactions = (sc.textFile(localpath(\"small_data/employee/sales.csv\"))\n",
    "    .flatMap({ _.split(\",\") match {\n",
    "        case Array(\n",
    "            \"sales\",\n",
    "            transactionId: String,\n",
    "            productId: String,\n",
    "            user: String,\n",
    "            amount: String\n",
    "        ) => Some(new Transaction(transactionId, productId, user, amount.toInt))\n",
    "        case _ => None\n",
    "    }}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other table contains information about the users themselves.\n",
    "\n",
    "```\n",
    "users,user1,bob@exmaple.com,EN,US\n",
    "users,user2,jon@exmaple.com,EN,GB\n",
    "users,user3,sally@exmaple.com,FR,FR\n",
    "users,user4,katie@exmaple.com,FR,FR\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class User(\n",
    "    userId: String,\n",
    "    email: String,\n",
    "    language: String,\n",
    "    country: String\n",
    ")\n",
    "\n",
    "val users = (sc.textFile(localpath(\"small_data/employee/users.csv\"))\n",
    "    .map({line => \n",
    "        val data = line.split(\",\")\n",
    "        User(data(1), data(2), data(3), data(4))\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the individual RDDs, we can make calculations about the total sales or distribution of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val totalSales = transactions.map(_.amount).sum\n",
    "val usersByCountryCount = users.map(u => (u.country, u)).countByKey\n",
    "\n",
    "println(s\"total sales $totalSales\")\n",
    "println(\"Users by country\")\n",
    "usersByCountryCount foreach println"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to know the total sales per country, we need to join these RDDs.  To do this, we need a shared key, which in this case is the user ID.  We use map to transform both RDDs into key-value pairs, and then join to combine them into a single RDD.  This RDD has elements of the form `(userId, (transactionAmount, userCountry))`.  Another map and reduce by key is all we need to get the calculation we desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This is (userId, (transaction, user))\n",
    "val salesByCountry = (transactions.map(t => (t.userId, t.amount))\n",
    "                .join(users.map(u => (u.userId, u.country)))\n",
    "                .map({ \n",
    "                  case (userId: String, (amount: Int, country: String)) \n",
    "                      => (country, amount)\n",
    "                }) \n",
    "                .reduceByKey(_ + _)\n",
    "                .collect)\n",
    "\n",
    "println(\"Transactions by country\")\n",
    "salesByCountry foreach println"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A warning about traversables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformations `mapPartitions`, `groupByKey`, `mapPartitionsWithIndex`, and `cogroup` return iterators.  Much like Python iterators, you can only traverse them once.  Here's a common anti-pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Anti-example\n",
    "\n",
    "def mean(iter: TraversableOnce[Int]) = {\n",
    "    val size = iter.size\n",
    "    val sum = iter.sum.toDouble\n",
    "    sum / size\n",
    "}\n",
    "\n",
    "val iterMean = mean(Iterator(1, 2, 3, 4, 5))\n",
    "val listMean = mean(List(1, 2, 3, 4, 5))\n",
    "\n",
    "println(s\"iterator mean: $iterMean\\nlist mean: $listMean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that data set (or data sets derived from it). This allows future actions to be much faster.\n",
    "\n",
    "The argument passed to the persist method determines how the RDD will be stored. In general, you may choose\n",
    "* Deserialized or serialized formats (serialized Java objects are more space-efficient but more CPU-intensive to read)\n",
    "* In-memory only or allow spilling to disk\n",
    "* *Fast* fault tolerance via redundancy\n",
    "* [Full documentation](http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence)\n",
    "\n",
    "\n",
    "*Serialization*: the process of translating data structures or object states into a format (i.e. series of bits) that can be stored and reconstructed later.\n",
    "\n",
    "\n",
    "1. The cache method is a shortcut for the default behavior: in memory and deserialized:\n",
    "```scala\n",
    "myRdd.cache()\n",
    "```\n",
    "2. Otherwise, specify the desired behavior:\n",
    "```scala\n",
    "myRdd.persist(MEMORY_ONLY_SER)\n",
    "```\n",
    "3. Cleaning up cached data is important for memory management. During shuffles, some intermediate data is automatically persisted, so it can be worthwhile to manually unpersist RDDs instead of waiting for garbage collection:\n",
    "```scala\n",
    "myRDD.unpersist()\n",
    "```\n",
    "\n",
    "**Question:** Caching is a key tool for iterative algorithms and fast interactive use.  Why might you not always do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomStream = sc.parallelize(1 to 1000000).map{_ => Math.random()}\n",
    "\n",
    "val uncachedLines = randomStream\n",
    "val cachedLines = randomStream.cache()\n",
    "\n",
    "time{uncachedLines.count()}\n",
    "time{uncachedLines.count()}\n",
    "time{cachedLines.count()}\n",
    "time{cachedLines.count()} // this runs faster the 2nd time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to implement counters (like we had in `mrjob` and Hadoop in general).  As usual, here is a first anti-example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Anti-example\n",
    "\n",
    "var counter: Int = 0\n",
    "\n",
    "sc.parallelize((1 to 10)).foreach(x => counter += 1)\n",
    "\n",
    "println(counter)  // what's happening here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the correct way to implement this using [Accumulators](https://spark.apache.org/docs/1.3.0/api/scala/index.html#org.apache.spark.Accumulator).  We are just using them for integer addition but you can use them for any [monoid](https://en.wikipedia.org/wiki/Monoid).  You'll hear about these a lot in Scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Example\n",
    "\n",
    "var accumCounter = sc.accumulator(0)\n",
    "\n",
    "sc.parallelize((1 to 10)).foreach(x => accumCounter += 1)\n",
    "\n",
    "println(accumCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Lazy example\n",
    "\n",
    "var accumCounter = sc.accumulator(0)\n",
    "\n",
    "val myRdd = sc.parallelize((1 to 10)).\n",
    "    map{x => accumCounter += 1\n",
    "        x}\n",
    "\n",
    "println(\"counter: \" + accumCounter)\n",
    "\n",
    "println(myRdd.reduce(_ + _))\n",
    "\n",
    "println(\"counter: \" + accumCounter)\n",
    "\n",
    "myRdd.take(3).foreach(println)\n",
    "\n",
    "println(\"counter: \" + accumCounter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broad variables are the Spark equivalent of Hadoop's distributed cache: they deposit read-only data cached on each machine rather than shipping a copy of it with tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Basic usage:\n",
    "\n",
    "val iBV = sc.broadcast(10)\n",
    "assert(iBV.value == 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a more realistic example to show how you might implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val states = Map(\n",
    "    0 -> \"AL\",\n",
    "    1 -> \"KY\",\n",
    "    2 -> \"VT\" // etc.\n",
    ")\n",
    "\n",
    "case class Sale(state: Int, sales: Double)\n",
    "\n",
    "val data = sc.parallelize(Array(\n",
    "    Sale(0, 30.0),\n",
    "    Sale(1, 40.0),\n",
    "    Sale(2, 20.0),\n",
    "    Sale(2, 10.0)\n",
    "))\n",
    "\n",
    "// This works but it will send `states` to each node again if run twice\n",
    "time{val results1 = data.map(s => (states(s.state), s.sales)).collect()}\n",
    "\n",
    "// // broadcast to each node once, and can be used freely inside map\n",
    "val statesBV = sc.broadcast(states)\n",
    "time{val results2 = data.map(s => (statesBV.value(s.state), s.sales)).collect()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val results1 = data.map(s => (states(s.state), s.sales)).collect()\n",
    "val results2 = data.map(s => (statesBV.value(s.state), s.sales)).collect()\n",
    "assert(results1.toList == results2.toList) // why do we need .toList?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some caveats:**\n",
    "1. After the broadcast variable is created, it should be used instead of the value so that it is not shipped to the nodes more than once.\n",
    "2. The broadcast variable should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable.\n",
    "3. How does the memory footprint of broadcasting scale with the number of tasks? With the number of executors?\n",
    "\n",
    "**Question:** The above example is called a map-side join.  \n",
    "1. Why is it called this?\n",
    "1. What are the strengths and weaknesses of this versus a full join (i.e. emitting key-value pairs to common keys)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exit Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the main distinguishing factors of Spark versus Hadoop?\n",
    "1. Go over the pros and cons of caching/broadcasting information for MapReduce jobs.\n",
    "1. What are the inputs and outputs for actions and transformations, respectively?\n",
    "1. `union` is a function that \"merges\" two RDDs \"vertically\".  How do you have a union between an `RDD` of type `S` and one of type `T` in a statically typed language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
