{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext(\"local[*]\", \"pyspark_df\")\n",
    "print(sc.version)\n",
    "\n",
    "# Alternatively...\n",
    "# conf = SparkConf().setAppName(\"pyspark_df\").setMaster(\"local[*]\")\n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to convert RDDs into DataFrames\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resets the HDFS environment.  (It's only necessary if you've already run\n",
    "# this notebook.)\n",
    "# Also clears the local file version, in case you wrote to local FS instead\n",
    "!hadoop fs -rm -r parquet_demo_pyspark\n",
    "!rm -r parquet_demo_pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark DataFrames\n",
    "<!-- requirement: small_data/nycp.csv -->\n",
    "<!-- requirement: projects/citi-subset -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation and Spark SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL is the current effort to provide support for writing SQL queries in Spark. Newer versions support Hive, Parquet, and other data sources. [Docs](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "\n",
    "The key feature of Spark SQL is the use of DataFrames instead of RDDs. A DataFrame is a distributed collection of data organized into named columns, and operations on DataFrames are first parsed through an optimized execution engine which streamlines and may even reorder the request to optimize execution. The keyword to search here is Catalyst.\n",
    "\n",
    "Under the hood, operations on DataFrames are boiled down to operations on RDDs, but the RDDs are created by the execution engine, and not directly by the user. It is also possible to convert RDDs to DataFrames and vice versa.\n",
    "\n",
    "The Spark ML package, unlike MLlib, uses DataFrames as inputs and outputs.\n",
    "\n",
    "**Question:** What is an example of a \"bad\" sequence of operations which should be reordered for optimal performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames are...\n",
    "\n",
    "* Immutable, like RDDs\n",
    "* Lineage is remembered, like RDDs (resiliency)\n",
    "* Lazy execution, like RDDs\n",
    "* So why do we care?\n",
    "\n",
    "\n",
    "DataFrames are an abstraction that lets us think of data in a familiar form (Panda, data.frame, SQL table, etc.).\n",
    "\n",
    "We can use a similar API to RDDs!\n",
    "\n",
    "Access to SQL-like optimizations and cost analysis due to it being in a columnar format.\n",
    "\n",
    "What about type safety?\n",
    "\n",
    "What are these UDF things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (sc.parallelize(range(1,10001))\n",
    "         .map(lambda x: (random.random(), random.random())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.toDF()\n",
    "# Note: this isn't always so easy, you may need to explicitly specify a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"_1\", \"x\").withColumnRenamed(\"_2\", \"y\")\n",
    "df.write.save(\"parquet_demo_pyspark\", format=\"parquet\")\n",
    "# Another (older) syntax\n",
    "# df.write.parquet(\"file:///home/jovyan/datacourse/module5/demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try rerunning the above cell.\n",
    "\n",
    "Save modes:\n",
    "* error\n",
    "* append\n",
    "* overwrite\n",
    "* ignore (i.e. CREATE TABLE IF NOT EXISTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"ignore\").parquet(\"parquet_demo_pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = sqlContext.read.parquet(\"parquet_demo_pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.describe(\"x\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dfp = dfp.filter(dfp[\"x\"] < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dfp.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Catalyst Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Catalyst optimizer is a tool which exploits the structure and type knowledge offered by DataFrames to enable faster execution.    At the end of the day its still using `RDD`'s under the hood, but it goes through an optimization step to turn the high level \"query\" written with DataFrames (or SQL) into an optimized query and then code generation into Spark Core (RDD).  This is really much like a compiler and it has the same tree sort of optimization structure.\n",
    "\n",
    "We explore this by making use of the `explain` method of a `DataFrame` which shows\n",
    "\n",
    "- *Parsed Logical Plan* (the input of the user)\n",
    "- *Analyzed Logical Plan* (type information)\n",
    "- *Optimized Logical Plan* (after optimization)\n",
    "- *Physical Plan* (execution plan)\n",
    "\n",
    "\n",
    "Lets take our `filtered_dfp` from before and look at its Logical Plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dfp.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see a more interesting example.  Lets take `df` and apply a filter on the \"x\" column and look at the resulting plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.filter(df[\"x\"] < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at a different plan where we filter both \"x\" and \"y\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.filter(df[\"x\"] < 0.5).filter(df[\"y\"] < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dfp = dfp.filter(dfp[\"x\"] < 0.5).filter(dfp[\"y\"] < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dfp.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, it's just manipulating trees based on rules.\n",
    "The introductory [blog post](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html) has good pictures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Tungsten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Memory management and GC (better than the JVM)\n",
    "* Cache-aware computation\n",
    "* Codegen (compile queries into Java bytecode)\n",
    "\n",
    "Cache-aware computation example:\n",
    "* Case 1: pointer -> key, value\n",
    "* Case 2: key, pointer -> key, value\n",
    "\n",
    "The CPU has to find keys for sort purposes. This helps it find them faster.\n",
    "\n",
    "[More](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL and DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires Hive to permanently store tables\n",
    "df.registerTempTable('nums')  # This is NOT the same as a temp table in SQL proper\n",
    "sql_df = sqlContext.sql(\"select x, y from nums where y > 0.9 limit 3\")\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reminder:* Check the UI (port 4040 by default) for tables in memory.\n",
    "\n",
    "*Reminder:* A number of interactive tutorials are available on the DataBricks [community cloud](https://community.cloud.databricks.com). I highly recommend making an account and checking out the guide.\n",
    "\n",
    "This is also a good place to learn about connecting to databases like Cassandra or using JDBC protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding columns and functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because DataFrames are immutable, adding new information means appending columns to an existing DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(threshold):\n",
    "    def pred(val):\n",
    "        if val > threshold:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labelizer = udf(predictor(0.5), DoubleType())\n",
    "y_labelizer = udf(predictor(0.9), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = dfp.withColumn(\"x_label\", x_labelizer(\"x\")).withColumn(\"y_label\", y_labelizer(\"y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type safety and DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = new_df.rdd\n",
    "row = rdd.take(1)\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that take always returns a list of results\n",
    "print(type(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = row[0]\n",
    "print(type(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, we're not too worried about type safety. But it's important to note that in Scala/Java, these Row objects do not contain the type information of the objects inside them and therefore type safety can be lost converting from RDDs to DataFrames. [DataSets](http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes) (fleshed out in Spark 2.0) are a newer incarnation of DataFrames that add encoding information to preserve that type safety.We can, however, drill into Row objects to extract the information we want.\n",
    "\n",
    "We can, however, drill into Row objects to extract the information we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common ways to slow down your code is to call into Python a bit too much.  Lets look at an example where we just want to split a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def localpath(path):\n",
    "    return 'file://' + os.path.join(os.path.abspath(os.path.curdir), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(localpath('./small_data/nycp.csv'), \n",
    "                         header=True)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType\n",
    "split_str = udf(lambda x : x.split(':'), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df.withColumn(\"split\", split_str(\"STATION\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"split\", split_str(\"STATION\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df.withColumn(\"split\", F.split(df.STATION, \"-\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"split\", F.split(df['STATION'], \"-\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great, but lets do a real computation, lets say we want to find the average temperature in each month and then plot it.  One thing we can take advantage of here is the built in conversion to Pandas DataFrames.  If we can reduce our data to manageable size, we can make plots quite easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_temp = (df.withColumn(\"avg\", (df[\"TMIN\"] + df[\"TMAX\"]) / 2)\n",
    "              .withColumn(\"month\",df['DATE'].substr(5, 2).cast(IntegerType()))\n",
    "              .groupBy(\"month\")\n",
    "              .avg('avg')\n",
    "              .withColumnRenamed(\"avg(avg)\", \"temp\")\n",
    "           )\n",
    "avg_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(avg_temp.toPandas()\n",
    "         .set_index(\"month\")\n",
    "         .sort_index()\n",
    "         .plot.bar())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculation takes advantage of both the scalability of Spark and the nice plotting features of Pandas.  It can be useful to think of Spark as the tool to handle scaling data while something like Pandas can be used on the aggregate data for visualization and summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we want to join data sets together.  Lets work with a slightly larger data set that we can join with the data set we just downloaded!\n",
    "\n",
    "The data set we will work with keep track of individual `citibike` rides over the course of a few years.  We expect that the weather might play a role in how many rides there are in a given day so we will use the `citibike` data along with the weather data we have already obtained to confirm this suspicion.\n",
    "\n",
    "First we will have to download the data which is a few `Gb` in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd projects\n",
    "# Uncomment the following line to trigger the download of the data set.\n",
    "#DO_DOWNLOAD=true\n",
    "if [ ! -d \"cititrip\" ] && [ \"$DO_DOWNLOAD\" ] ; then\n",
    "    wget -q \"https://s3.amazonaws.com/dataincubator-course/cititrip/2013-07 - Citi Bike trip data.csv.gz\" - -nc  -P cititrip/.\n",
    "    wget -q \"https://s3.amazonaws.com/dataincubator-course/cititrip/2013-08 - Citi Bike trip data.csv.gz\" -nc  -P cititrip/.\n",
    "    wget -q \"https://s3.amazonaws.com/dataincubator-course/cititrip/2013-09 - Citi Bike trip data.csv.gz\" -nc  -P cititrip/.\n",
    "    wget -q \"https://s3.amazonaws.com/dataincubator-course/cititrip/2013-10 - Citi Bike trip data.csv.gz\" -nc  -P cititrip/.\n",
    "    wget -q \"https://s3.amazonaws.com/dataincubator-course/cititrip/2013-11 - Citi Bike trip data.csv.gz\" -nc  -P cititrip/.\n",
    "    wget -q \"https://s3.amazonaws.com/dataincubator-course/cititrip/2013-12 - Citi Bike trip data.csv.gz\" -nc  -P cititrip/.\n",
    "    wget -q \"https://s3.amazonaws.com/dataincubator-course/cititrip/2014-01 - Citi Bike trip data.csv.gz\" -nc  -P cititrip/.\n",
    "    wget -q \"https://s3.amazonaws.com/dataincubator-course/cititrip/2014-02 - Citi Bike trip data.csv.gz\" -nc  -P cititrip/.\n",
    "    wget -q \"https://s3.amazonaws.com/dataincubator-course/cititrip/2014-03 - Citi Bike trip data.csv.gz\" -nc  -P cititrip/.\n",
    "    wget -q \"https://s3.amazonaws.com/dataincubator-course/cititrip/2014-04 - Citi Bike trip data.csv.gz\" -nc  -P cititrip/.\n",
    "    wget -q \"https://s3.amazonaws.com/dataincubator-course/cititrip/2014-05 - Citi Bike trip data.csv.gz\" -nc  -P cititrip/.\n",
    "    wget -q \"https://s3.amazonaws.com/dataincubator-course/cititrip/2014-06 - Citi Bike trip data.csv.gz\" -nc  -P cititrip/.\n",
    "    wget -q \"https://s3.amazonaws.com/dataincubator-course/cititrip/2014-07 - Citi Bike trip data.csv.gz\" -nc  -P cititrip/.\n",
    "    wget -q \"https://s3.amazonaws.com/dataincubator-course/cititrip/2014-08 - Citi Bike trip data.csv.gz\" -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201409-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201410-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201411-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201412-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201501-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201502-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201503-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201504-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201505-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201506-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201507-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201508-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201509-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201510-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201511-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201512-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201601-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201602-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201603-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201604-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201605-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "    wget -q https://s3.amazonaws.com/dataincubator-course/cititrip/201606-citibike-tripdata.csv.gz -nc  -P cititrip/.\n",
    "fi\n",
    "echo \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = 'projects/cititrip' if os.path.isdir('projects/cititrip') else 'projects/citi-subset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read in the data and show just the first few rows, this is always smart just to get a sense of what the data contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(localpath(datadir),\n",
    "                         header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can join this data, we need to do a little clean up to parse out the dates, lets look at the beginning of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.withColumn(\"day\", F.split(\"starttime\", \" \").getItem(0))\n",
    "   .select(\"day\")\n",
    "   .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets look at another part of the DataFrame, notice that the dates are not always in a unified format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.withColumn(\"day\", F.split(\"starttime\", \" \").getItem(0))\n",
    "   .select(\"day\")\n",
    "   .filter(F.col(\"day\").contains('-'))\n",
    "   .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to handle these two cases when converting to dates, to see some information about date strings, check out [simple date format](https://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = (df.withColumn(\"st\", F.split(\"starttime\", \" \").getItem(0))\n",
    "               .withColumn(\"day\",\n",
    "                           F.when(F.col(\"st\").contains(\"/\"), \n",
    "                                  F.to_date(\"st\", \"MM/dd/yyyy\"))\n",
    "                            .otherwise(F.to_date(\"st\", \"yyyy-MM-dd\")))\n",
    "               .select(\"day\")\n",
    ")\n",
    "days.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can aggregate by \"day\" and count the number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_counts = (days.select(\"day\")\n",
    "                  .groupBy(\"day\")\n",
    "                  .count()\n",
    "                  .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(day_counts.toPandas()\n",
    "           .set_index(\"day\")\n",
    "           .sort_index()\n",
    "           .plot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it might be very correlated with weather, to examine this, lets take our original DataFrame and join them together to get some sense of the correlation.  Note that we will do the join after we have already computed the aggregate summation over each day, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = (sqlContext.read.csv(localpath('./small_data/nycp.csv'), \n",
    "                         header=True)\n",
    "                     .withColumn(\"avg\", \n",
    "                                 (F.col(\"TMIN\") + F.col(\"TMAX\")) / 2)\n",
    "                     .withColumn(\"dt\", F.to_date(F.col(\"DATE\"),\n",
    "                                                \"yyyyMMdd\"))\n",
    "          )\n",
    "df_temp.select(['avg', 'dt']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = df_temp.select(['avg', 'dt']).join(day_counts,\n",
    "                                            df_temp.dt == day_counts.day,\n",
    "                                            'inner')\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the correlation of these two columns and see if it looks fairly large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.corr(\"avg\", \"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests a strong correlation, not too surprising given people probably don't want to ride when its cold.  Lets plot these just to see this visually.  To make it look pretty, lets scale the columns as well.  Here we will use a bit of poor way to do this, in the Machine Learning notebook, we will learn how to use some built in functionality to do this faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_t = joined.agg({\"avg\":\"min\"}).collect()[0][0]\n",
    "max_t = joined.agg({\"avg\":\"max\"}).collect()[0][0]\n",
    "min_c = joined.agg({\"count\":\"min\"}).collect()[0][0]\n",
    "max_c = joined.agg({\"count\":\"max\"}).collect()[0][0]\n",
    "print(min_t, max_t, min_c, max_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(joined.withColumn(\"scale_temp\", \n",
    "                   (F.col(\"avg\") - min_t)/(max_t - min_t))\n",
    "       .withColumn(\"scale_count\",\n",
    "                  (F.col(\"count\") - min_c)/(max_c - min_c))\n",
    "       .select([\"day\", \"scale_temp\", \"scale_count\"])\n",
    "       .toPandas()\n",
    "       .set_index(\"day\")\n",
    "       .plot(alpha=.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly weather is a strong predictor of bike ridership, but it isn't the only thing apparent, do you notice anything else? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
