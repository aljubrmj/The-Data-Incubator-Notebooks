{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "\n",
    "// For implicit transformation of RDDs to DataFrames\n",
    "import sqlContext.implicits._\n",
    "\n",
    "// For telling Spark to look in the local file system\n",
    "import java.io._\n",
    "def localpath(path: String): String = {\n",
    "    \"file://\" + new java.io.File(\".\").getCanonicalPath + \"/\" + path\n",
    "}\n",
    "\n",
    "// For timing expression evaluation\n",
    "def time[R](block: => R): R = {\n",
    "    val start: Long = System.nanoTime()\n",
    "    val result = block\n",
    "    val end: Long = System.nanoTime()\n",
    "    val duration: Double = (end - start) / 1000000000.0\n",
    "    println(\"Elapsed time: \" + duration + \"s\")\n",
    "    result\n",
    "}\n",
    "\n",
    "println(\"Using Spark version \" + sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    "// If you have a Hive install, you can connect it to Spark:\n",
    "val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)\n",
    "\n",
    "hiveContext.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING)\")\n",
    "hiveContext.sql(\"LOAD DATA LOCAL INPATH 'small_data/employer/hashmap.csv' INTO TABLE src\")\n",
    "\n",
    "// Queries are expressed in HiveQL\n",
    "sqlContext.sql(\"FROM src SELECT key, value\").collect().foreach(println)\n",
    "*/\n",
    "\n",
    "import math._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation and Spark SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL is the current effort to provide support for writing SQL queries in Spark. Newer versions support Hive, Parquet, and other data sources. [Docs](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "\n",
    "The key feature of Spark SQL is the use of DataFrames instead of RDDs. A DataFrame is a distributed collection of data organized into named columns, and operations on DataFrames are first parsed through an optimized execution engine which streamlines and may even reorder the request to optimize execution. The keyword to search here is Catalyst.\n",
    "\n",
    "Under the hood, operations on DataFrames are boiled down to operations on RDDs, but the RDDs are created by the execution engine, and not directly by the user. It is also possible to convert RDDs to DataFrames and vice versa.\n",
    "\n",
    "The Spark ML package, unlike MLlib, uses DataFrames as inputs and outputs.\n",
    "\n",
    "**Question:** What is an example of a \"bad\" sequence of operations which should be reordered for optimal performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames are...\n",
    "\n",
    "* Immutable, like RDDs\n",
    "* Lineage is remembered, like RDDs (resiliency)\n",
    "* Lazy execution, like RDDs\n",
    "* So why do we care?\n",
    "\n",
    "\n",
    "DataFrames are an abstraction that lets us think of data in a familiar form (Panda, data.frame, SQL table, etc.).\n",
    "\n",
    "We can use a similar API to RDDs!\n",
    "\n",
    "Access to SQL-like optimizations and cost analysis due to it being in a columnar format.\n",
    "\n",
    "What about type safety?\n",
    "\n",
    "What are these UDF things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = sc.parallelize((1 until 10001)).\n",
    "    map(x => (random, random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This isn't always so easy. You may need to explicity define a schema.\n",
    "val df = data.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Parquet format will be used by default\n",
    "df\n",
    "    .withColumnRenamed(\"_1\", \"x\")\n",
    "    .withColumnRenamed(\"_2\", \"y\")\n",
    "    .write\n",
    "    .save(\"spark_parquet_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try rerunning the above cell.\n",
    "\n",
    "Save modes:\n",
    "* error\n",
    "* append\n",
    "* overwrite\n",
    "* ignore (i.e. CREATE TABLE IF NOT EXISTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"ignore\").format(\"parquet\").save(\"spark_parquet_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfp = sqlContext.read.load(\"spark_parquet_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.describe(\"x\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val filteredDF = dfp.filter(dfp(\"x\") < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Catalyst Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDF.explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val filteredDF = df.filter(df(\"_1\") < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDF.explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val filteredDF = df.filter(df(\"_1\") < 0.5).filter(df(\"_2\") < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDF.explain(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val filteredDFP = dfp.filter(dfp(\"x\") < 0.5).filter(dfp(\"y\") < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDFP.explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, it's just manipulating trees based on rules.\n",
    "The introductory [blog post](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html) has good pictures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Tungsten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Memory management and GC (better than the JVM)\n",
    "* Cache-aware computation\n",
    "* Codegen (compile queries into Java bytecode)\n",
    "\n",
    "Cache-aware computation example:\n",
    "* Case 1: pointer -> key, value\n",
    "* Case 2: key, pointer -> key, value\n",
    "\n",
    "The CPU has to find keys for sort purposes. This helps it find them faster.\n",
    "\n",
    "[More](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame performance and tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](http://spark.apache.org/docs/latest/sql-programming-guide.html#performance-tuning) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL and DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Requires Hive to permanently store tables\n",
    "dfp.registerTempTable(\"nums\")  // This is NOT the same as a temp table in SQL proper\n",
    "val sqlDF = sqlContext.sql(\"select x, y from nums where y > 0.9 limit 3\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF.explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reminder:* Check the UI (port 4040 by default) for tables in memory.\n",
    "\n",
    "*Reminder:* A number of interactive tutorials are available on the DataBricks [community cloud](https://community.cloud.databricks.com). I highly recommend making an account and checking out the guide.\n",
    "\n",
    "This is also a good place to learn about connecting to databases like Cassandra or using JDBC protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding columns and functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because DataFrames are immutable, adding new information means appending columns to an existing DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Currying lets us specify some of a function's arguments and delay specifying the rest until later.\n",
    "// Remember how we can pass an argument into a function as a value or as a function? `f: => Int`\n",
    "\n",
    "def prediction(threshold: Double)(num: Double): Int = {\n",
    "    if (num >= threshold) 1 else 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "val xLabelizer = udf(prediction(0.5) _)\n",
    "val yLabelizer = udf(prediction(0.9) _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val newDF = dfp.withColumn(\"xLabel\", xLabelizer(dfp(\"x\"))).withColumn(\"yLabel\", yLabelizer(dfp(\"y\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type safety and DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd = newDF.rdd\n",
    "val row = rdd.take(1)\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.getClass.getName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Remember that `take` always returns a list of results\n",
    "val r = row(0)\n",
    "println(r.schema)\n",
    "\n",
    "// The fields are the column names by default\n",
    "r.fieldIndex(\"yLabel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, we're not too worried about type safety. But it's important to note that in Scala/Java, these Row objects do not contain the type information of the objects inside them and therefore type safety can be lost converting from RDDs to DataFrames. [DataSets](http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes) (fleshed out in Spark 2.0) are a newer incarnation of DataFrames that add encoding information to preserve that type safety.\n",
    "\n",
    "By default in Spark 2.0+, DataFrames are just an alias for DataSet[Row] and Rows are a light wrapper around Array[Any] which can cause some problems.\n",
    "\n",
    "We can redefine the encoding with `.as` and specifying types or a case class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The \"old way\" of doing it...\n",
    "// We can always drill into Row objects to extract the information we want.\n",
    "println(r(0).getClass.getName + \": \" + r(0))\n",
    "println(r(2).getClass.getName + \": \" + r(2))\n",
    "\n",
    "// Or by field name\n",
    "println(\"Trying to get the field 'x' as Double... \" + r.getAs[Double](\"x\"))\n",
    "println(\"Trying to get the field 'xLabel' as Int... \" + r.getAs[Int](\"xLabel\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// DataSets are often easier\n",
    "// Note that going to a lower-precision type won't be allowed\n",
    "val newDS = newDF.as[(Double, Double, Float, Long)]\n",
    "newDS.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val newRDD: org.apache.spark.rdd.RDD[(Double, Double, Float, Long)] = newDS.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dsrow = newRDD.take(1)(0)\n",
    "dsrow.getClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// No longer a Row\n",
    "println(dsrow._2.getClass.getName)\n",
    "println(dsrow._2)\n",
    "println(dsrow._4.getClass.getName)\n",
    "println(dsrow._4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using case classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Matching is done by column name\n",
    "// Note that again, you are responsible for making this type matching make sense.\n",
    "case class Observation(\n",
    "    x: Double,\n",
    "    y: Double,\n",
    "    xLabel: Float,\n",
    "    yLabel: Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ccDS = newDF.as[Observation]\n",
    "val ccRDD = ccDS.rdd\n",
    "ccRDD.getClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccRDD.map(obs => (obs.x, obs.xLabel)).take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2018 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
