{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Functional Programming Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms for distributed computing must work well when data and computation are divided among separate nodes. Since different nodes will process different data, this history of their computation will differ. Many algorithms make use of computation history in some way; in computer science jargon we call such algorithms **stateful**. In distributed computing, we will try to avoid algorithms with output that depends on the **state** of the node executing the code. Distributed computing should be redundant and failure-tolerant, so reproducibility is important.\n",
    "\n",
    "Functional programming takes the perspective that programs are compositions of stateless functions that operate on immutable data.  This means the output of a function is completely determined by its input.  We will see this can often provide the following benefits:\n",
    "\n",
    "1. Logic easier to **understand**\n",
    "1. Code easier to **test**\n",
    "1. Code easier to **parallelize** (the big one for us)\n",
    "1. Code can be easier to **prove** correct in a mathematical sense (we won't get much into this, but there is a host of cool math concepts like $\\lambda$ calculus related to this idea).\n",
    "\n",
    "Now of course nothing comes for free and functional programs can sometimes be a bit trickier to write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful vs. Stateless Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a very simple example: an object that counts the number of characters in a collection of strings. We implement it in two ways:\n",
    "\n",
    "1. A counter that maintains an internal count as we pass subsequent containers to its `count` method.\n",
    "1. A counter that requires a running count to be passed in as an argument to its `count` method.\n",
    "\n",
    "The `StatefulCounter` will be a full-fledged object, while in contrast we'll see the `StatelessCounter` is essentially just a namespace for a `count` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulCounter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "\n",
    "    def count(self, string):\n",
    "        for _ in string:\n",
    "            self.counter += 1\n",
    "\n",
    "        return self.counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatelessCounter(object):\n",
    "\n",
    "    def count(self, counter, string):\n",
    "        for _ in string:\n",
    "            counter += 1\n",
    "\n",
    "        return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ['a' * 12, 'a' * 5, 'a' * 33]\n",
    "stateful = StatefulCounter()\n",
    "stateless = StatelessCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in strings:\n",
    "    stateful.count(string)\n",
    "\n",
    "print(stateful.counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for string in strings:\n",
    "    count = stateless.count(count, string)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this example is so simple it may seem trivial, the fact that the stateless counter passes information out of the object to a variable represents the moment that different nodes in a distributed context can perform IO between each other or to nodes that will handle later stages of computation.\n",
    "\n",
    "Let's consider a slightly more complex example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulLinearRegression(object):\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.coef_ = X.dot(y) / X.dot(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [self.coef_ * x for x in X]\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        sse = np.sum((self.predict(X) - y)**2)\n",
    "        return np.sqrt(sse / len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatelessLinearRegression(object):\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        coef = X.dot(y) / X.dot(X)\n",
    "\n",
    "        def predict(X):\n",
    "            return [coef * x for x in X]\n",
    "\n",
    "        def score(X, y):\n",
    "            sse = np.sum((predict(X) - y)**2)\n",
    "            return np.sqrt(sse / len(y))\n",
    "\n",
    "        return predict, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate some random data that we can use our `StateFulLinearRegression` and `StatelessLinearRegression` to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.uniform(size=100)\n",
    "y1 = 2 * X1 + np.random.uniform(-0.1, 0.1, 100)\n",
    "\n",
    "X2 = np.random.uniform(size=100)\n",
    "y2 = 2 * X2 + np.random.uniform(-0.1, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_linreg = StatefulLinearRegression()\n",
    "stateful_linreg.fit(X1, y1)\n",
    "stateful_linreg.score(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict, score = StatelessLinearRegression().fit(X1, y1)\n",
    "score(X2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our stateless linear regression object's `fit` method returns a `predict` and `score` method. In functional programming, functions are first class constructs and are often passed into functions are arguments or are returned by functions. The `fit` method is an example of a **closure**, a technique for binding functions with an environment. The `coef` variable and `predict` and `score` are all defined within the scope of the `fit` method, which effectively means `coef` is distributed as a constant with the returned `predict` and `score` methods. \n",
    "\n",
    "**Question**: *What are the advantages and disadvantages of each approach?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ubiquitous Python Decorator is a nice example of some of the paradigms of functional programming, particular a function a first class object. A `Decorator` is a function which operates on other functions generally to add some extra functionality.  For example, lets write a decorator which times operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def timeit(func):\n",
    "    def f_(*args, **kwargs):\n",
    "        ts = time.time()\n",
    "        return_ = func(*args, **kwargs)\n",
    "        print(\"Elapsed time for {} : {:.2e}\".format(func.__name__, time.time() - ts))\n",
    "        return return_\n",
    "    return f_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to decorate another function, or in other words create a new function which is the output of the decorator applied to the base function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def multiply_random_matrices(shape):\n",
    "    return np.random.rand(shape, shape) @ np.random.rand(shape, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply_random_matrices(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how Python implements the normal functional operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map, Filter, and Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Python is usually considered an object-oriented programming language, it also contains features of a functional language, including the commonplace functions `map`, `filter`, and `reduce` (as of Python 3, `reduce` is now part of `functools` instead of a built-in function). We will often structure distributed jobs as a composition of data transformations and aggregations.\n",
    "\n",
    "For a transformation, we define a function which transform individual data elements, and `map` that function to the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    'Lorem ipsum dolor sit amet, consectetur adipiscing elit.',\n",
    "    'Morbi iaculis egestas leo, in consectetur diam ornare in. Nulla eleifend cursus turpis in luctus.',\n",
    "    'Nullam accumsan congue hendrerit.'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "list(map(tokenize, text)) # list to visualize output; map returns generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark we will we will also encounter `flatMap`, which additionally unpacks the output of the mapped function into a single data structure. Below is an example implementation to illustrate how Spark's `flatMap` works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatMap(f, data):\n",
    "    return [element for nested in map(f, data) for element in nested]\n",
    "\n",
    "flatMap(tokenize, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering is very similar, except the transformation function should return a Boolean for each data element, which will then be used to filter the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vowel_start(word):\n",
    "    return word[0] in 'aeiou'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize(text[0]))\n",
    "print(list(filter(vowel_start, tokenize(text[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When aggregating a data structure, we will use `reduce` to apply our aggregating function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_length(accumulator, word):\n",
    "    if isinstance(accumulator, int):\n",
    "        return accumulator + len(word)\n",
    "    else:\n",
    "        return len(accumulator) + len(word)\n",
    "\n",
    "reduce(total_length, tokenize(text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used with `reduce` are always functions of two arguments: an argument that acts as an \"accumulator\" and an argument that iterates through the data structure being reduced. In the language of imperative programming, the above would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_structure = tokenize(text[0])\n",
    "\n",
    "accumulator = data_structure[0] # accumulator is initialized to first value in data structure\n",
    "for word in data_structure[1:]:\n",
    "    if isinstance(accumulator, int):\n",
    "        accumulator += len(word)\n",
    "    else:\n",
    "        accumulator = len(accumulator) + len(word)\n",
    "\n",
    "accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `functools` [documentation](https://docs.python.org/3/library/functools.html#functools.reduce) includes an example implementation of `reduce` for general functions.\n",
    "\n",
    "In some functional languages, `reduce` is called `foldLeft`, highlighting the \"folding\" of each subsequent element of the data structure, `word`, into `accumulator`. By default in Python, `reduce` initializes the \"accumulator\" to the first element of the iterable, but in languages (like Scala) with `foldLeft`, we choose the initialization of the accumulator. This can simplify the function being used for reduction, since it avoids any special cases associated with initializing the accumulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_length(accumulator, word):\n",
    "    return accumulator + len(word)\n",
    "\n",
    "reduce(total_length, tokenize(text[0]), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see these patterns repeated as we learn Spark's API.  \n",
    "\n",
    "**Questions**:\n",
    "- *How are map and reduce useful in distributed computing?*\n",
    "- *What other operations do we need for functional computing?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anonymous Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been using named functions, but often the overhead of writing an entire function definition is non-optimal both in terms of number of lines of code, but also because we may not want to deal with things such as namespace conflicts.\n",
    "\n",
    "The general solution is to use **anonymous functions**, sometimes called **lambda functions**, however, Python does not have great support for these tools.  That said they can still be useful for simple things.  Lets first see an example, where we tokenize the text without defining a tokenize function.  We are calling `list(map(...))` so often, we can just make it a function `map_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_(*args, **kwargs):\n",
    "    return list(map(*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_(lambda x : x.split(), text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keyword here is the `lambda` which then takes arguments, followed by a colon, followed by the return value. We pass around the anonymous function object just like any other function object, even bind it to a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenize = lambda x : x.split()\n",
    "new_tokenize(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially for little bits of code, anonymous functions can be useful and are used widely in Spark code.  That said, general best practices do suggest using a named function for anything non-trivial and it seems like the Python community is moving further in that direction with the removal of tuple unpacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2019 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
